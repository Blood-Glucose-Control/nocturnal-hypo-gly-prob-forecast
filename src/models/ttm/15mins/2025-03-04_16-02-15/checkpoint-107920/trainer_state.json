{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 891.900826446281,
  "eval_steps": 500,
  "global_step": 107920,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.132231404958677,
      "grad_norm": 13.91285514831543,
      "learning_rate": 4.979338842975207e-05,
      "loss": 13.411,
      "step": 500
    },
    {
      "epoch": 8.264462809917354,
      "grad_norm": 12.833491325378418,
      "learning_rate": 4.958677685950414e-05,
      "loss": 12.0785,
      "step": 1000
    },
    {
      "epoch": 12.396694214876034,
      "grad_norm": 11.675077438354492,
      "learning_rate": 4.9380165289256205e-05,
      "loss": 11.7532,
      "step": 1500
    },
    {
      "epoch": 16.52892561983471,
      "grad_norm": 12.436896324157715,
      "learning_rate": 4.917355371900827e-05,
      "loss": 11.568,
      "step": 2000
    },
    {
      "epoch": 20.66115702479339,
      "grad_norm": 16.822996139526367,
      "learning_rate": 4.896694214876033e-05,
      "loss": 11.4515,
      "step": 2500
    },
    {
      "epoch": 24.793388429752067,
      "grad_norm": 11.852038383483887,
      "learning_rate": 4.87603305785124e-05,
      "loss": 11.3561,
      "step": 3000
    },
    {
      "epoch": 28.925619834710744,
      "grad_norm": 19.147008895874023,
      "learning_rate": 4.855371900826447e-05,
      "loss": 11.2754,
      "step": 3500
    },
    {
      "epoch": 33.05785123966942,
      "grad_norm": 19.651607513427734,
      "learning_rate": 4.834710743801653e-05,
      "loss": 11.2077,
      "step": 4000
    },
    {
      "epoch": 37.1900826446281,
      "grad_norm": 185.37216186523438,
      "learning_rate": 4.8140495867768596e-05,
      "loss": 11.1767,
      "step": 4500
    },
    {
      "epoch": 41.32231404958678,
      "grad_norm": 24.474308013916016,
      "learning_rate": 4.793388429752066e-05,
      "loss": 11.1081,
      "step": 5000
    },
    {
      "epoch": 45.45454545454545,
      "grad_norm": 17.121244430541992,
      "learning_rate": 4.772727272727273e-05,
      "loss": 11.0908,
      "step": 5500
    },
    {
      "epoch": 49.586776859504134,
      "grad_norm": 23.19003677368164,
      "learning_rate": 4.75206611570248e-05,
      "loss": 11.0255,
      "step": 6000
    },
    {
      "epoch": 53.71900826446281,
      "grad_norm": 37.14229965209961,
      "learning_rate": 4.731404958677686e-05,
      "loss": 11.042,
      "step": 6500
    },
    {
      "epoch": 57.85123966942149,
      "grad_norm": 15.462435722351074,
      "learning_rate": 4.7107438016528926e-05,
      "loss": 10.9435,
      "step": 7000
    },
    {
      "epoch": 61.98347107438016,
      "grad_norm": 24.41610336303711,
      "learning_rate": 4.6900826446280993e-05,
      "loss": 10.9256,
      "step": 7500
    },
    {
      "epoch": 66.11570247933884,
      "grad_norm": 19.84303855895996,
      "learning_rate": 4.669421487603306e-05,
      "loss": 10.9168,
      "step": 8000
    },
    {
      "epoch": 70.24793388429752,
      "grad_norm": 26.09157943725586,
      "learning_rate": 4.648760330578513e-05,
      "loss": 10.869,
      "step": 8500
    },
    {
      "epoch": 74.3801652892562,
      "grad_norm": 24.564319610595703,
      "learning_rate": 4.6280991735537196e-05,
      "loss": 10.9153,
      "step": 9000
    },
    {
      "epoch": 78.51239669421487,
      "grad_norm": 27.021060943603516,
      "learning_rate": 4.607438016528926e-05,
      "loss": 10.8492,
      "step": 9500
    },
    {
      "epoch": 82.64462809917356,
      "grad_norm": 18.20688247680664,
      "learning_rate": 4.586776859504133e-05,
      "loss": 10.8571,
      "step": 10000
    },
    {
      "epoch": 86.77685950413223,
      "grad_norm": 29.072172164916992,
      "learning_rate": 4.566115702479339e-05,
      "loss": 10.8244,
      "step": 10500
    },
    {
      "epoch": 90.9090909090909,
      "grad_norm": 29.7242374420166,
      "learning_rate": 4.545454545454546e-05,
      "loss": 10.8127,
      "step": 11000
    },
    {
      "epoch": 95.04132231404958,
      "grad_norm": 20.182735443115234,
      "learning_rate": 4.524793388429752e-05,
      "loss": 10.79,
      "step": 11500
    },
    {
      "epoch": 99.17355371900827,
      "grad_norm": 29.710208892822266,
      "learning_rate": 4.504132231404959e-05,
      "loss": 10.7965,
      "step": 12000
    },
    {
      "epoch": 103.30578512396694,
      "grad_norm": 20.019262313842773,
      "learning_rate": 4.4834710743801654e-05,
      "loss": 10.7489,
      "step": 12500
    },
    {
      "epoch": 107.43801652892562,
      "grad_norm": 18.957792282104492,
      "learning_rate": 4.462809917355372e-05,
      "loss": 10.7739,
      "step": 13000
    },
    {
      "epoch": 111.5702479338843,
      "grad_norm": 31.60511589050293,
      "learning_rate": 4.442148760330579e-05,
      "loss": 10.7444,
      "step": 13500
    },
    {
      "epoch": 115.70247933884298,
      "grad_norm": 37.755374908447266,
      "learning_rate": 4.4214876033057856e-05,
      "loss": 10.7239,
      "step": 14000
    },
    {
      "epoch": 119.83471074380165,
      "grad_norm": 25.873018264770508,
      "learning_rate": 4.400826446280992e-05,
      "loss": 10.732,
      "step": 14500
    },
    {
      "epoch": 123.96694214876032,
      "grad_norm": 26.764684677124023,
      "learning_rate": 4.3801652892561984e-05,
      "loss": 10.7169,
      "step": 15000
    },
    {
      "epoch": 128.099173553719,
      "grad_norm": 21.62598991394043,
      "learning_rate": 4.359504132231405e-05,
      "loss": 10.7057,
      "step": 15500
    },
    {
      "epoch": 132.23140495867767,
      "grad_norm": 19.434314727783203,
      "learning_rate": 4.338842975206612e-05,
      "loss": 10.7032,
      "step": 16000
    },
    {
      "epoch": 136.36363636363637,
      "grad_norm": 24.640254974365234,
      "learning_rate": 4.318181818181819e-05,
      "loss": 10.6935,
      "step": 16500
    },
    {
      "epoch": 140.49586776859505,
      "grad_norm": 42.58231735229492,
      "learning_rate": 4.2975206611570254e-05,
      "loss": 10.6893,
      "step": 17000
    },
    {
      "epoch": 144.62809917355372,
      "grad_norm": 19.202863693237305,
      "learning_rate": 4.2768595041322315e-05,
      "loss": 10.6867,
      "step": 17500
    },
    {
      "epoch": 148.7603305785124,
      "grad_norm": 19.529769897460938,
      "learning_rate": 4.256198347107438e-05,
      "loss": 10.6848,
      "step": 18000
    },
    {
      "epoch": 152.89256198347107,
      "grad_norm": 23.358797073364258,
      "learning_rate": 4.235537190082644e-05,
      "loss": 10.6554,
      "step": 18500
    },
    {
      "epoch": 157.02479338842974,
      "grad_norm": 38.632896423339844,
      "learning_rate": 4.214876033057851e-05,
      "loss": 10.6819,
      "step": 19000
    },
    {
      "epoch": 161.15702479338842,
      "grad_norm": 27.951366424560547,
      "learning_rate": 4.194214876033058e-05,
      "loss": 10.6641,
      "step": 19500
    },
    {
      "epoch": 165.28925619834712,
      "grad_norm": 27.371828079223633,
      "learning_rate": 4.1735537190082645e-05,
      "loss": 10.6483,
      "step": 20000
    },
    {
      "epoch": 169.4214876033058,
      "grad_norm": 49.54058837890625,
      "learning_rate": 4.152892561983471e-05,
      "loss": 10.6889,
      "step": 20500
    },
    {
      "epoch": 173.55371900826447,
      "grad_norm": 34.460174560546875,
      "learning_rate": 4.132231404958678e-05,
      "loss": 10.6217,
      "step": 21000
    },
    {
      "epoch": 177.68595041322314,
      "grad_norm": 17.4661922454834,
      "learning_rate": 4.111570247933885e-05,
      "loss": 10.6333,
      "step": 21500
    },
    {
      "epoch": 181.8181818181818,
      "grad_norm": 23.878849029541016,
      "learning_rate": 4.0909090909090915e-05,
      "loss": 10.6391,
      "step": 22000
    },
    {
      "epoch": 185.9504132231405,
      "grad_norm": 26.258155822753906,
      "learning_rate": 4.0702479338842975e-05,
      "loss": 10.6221,
      "step": 22500
    },
    {
      "epoch": 190.08264462809916,
      "grad_norm": 15.623177528381348,
      "learning_rate": 4.049586776859504e-05,
      "loss": 10.6138,
      "step": 23000
    },
    {
      "epoch": 194.21487603305786,
      "grad_norm": 14.162776947021484,
      "learning_rate": 4.028925619834711e-05,
      "loss": 10.5997,
      "step": 23500
    },
    {
      "epoch": 198.34710743801654,
      "grad_norm": 23.155902862548828,
      "learning_rate": 4.008264462809918e-05,
      "loss": 10.6177,
      "step": 24000
    },
    {
      "epoch": 202.4793388429752,
      "grad_norm": 22.079450607299805,
      "learning_rate": 3.9876033057851245e-05,
      "loss": 10.5929,
      "step": 24500
    },
    {
      "epoch": 206.61157024793388,
      "grad_norm": 41.40623092651367,
      "learning_rate": 3.9669421487603306e-05,
      "loss": 10.6407,
      "step": 25000
    },
    {
      "epoch": 210.74380165289256,
      "grad_norm": 18.391023635864258,
      "learning_rate": 3.946280991735537e-05,
      "loss": 10.5741,
      "step": 25500
    },
    {
      "epoch": 214.87603305785123,
      "grad_norm": 32.71812438964844,
      "learning_rate": 3.925619834710744e-05,
      "loss": 10.5971,
      "step": 26000
    },
    {
      "epoch": 219.0082644628099,
      "grad_norm": 38.93989944458008,
      "learning_rate": 3.90495867768595e-05,
      "loss": 10.582,
      "step": 26500
    },
    {
      "epoch": 223.1404958677686,
      "grad_norm": 20.56291389465332,
      "learning_rate": 3.884297520661157e-05,
      "loss": 10.5573,
      "step": 27000
    },
    {
      "epoch": 227.27272727272728,
      "grad_norm": 23.214847564697266,
      "learning_rate": 3.8636363636363636e-05,
      "loss": 10.5804,
      "step": 27500
    },
    {
      "epoch": 231.40495867768595,
      "grad_norm": 30.090850830078125,
      "learning_rate": 3.8429752066115703e-05,
      "loss": 10.5763,
      "step": 28000
    },
    {
      "epoch": 235.53719008264463,
      "grad_norm": 15.331023216247559,
      "learning_rate": 3.822314049586777e-05,
      "loss": 10.5621,
      "step": 28500
    },
    {
      "epoch": 239.6694214876033,
      "grad_norm": 26.133493423461914,
      "learning_rate": 3.801652892561984e-05,
      "loss": 10.5584,
      "step": 29000
    },
    {
      "epoch": 243.80165289256198,
      "grad_norm": 16.60858917236328,
      "learning_rate": 3.7809917355371906e-05,
      "loss": 10.5485,
      "step": 29500
    },
    {
      "epoch": 247.93388429752065,
      "grad_norm": 13.893257141113281,
      "learning_rate": 3.760330578512397e-05,
      "loss": 10.5355,
      "step": 30000
    },
    {
      "epoch": 252.06611570247935,
      "grad_norm": 18.37576675415039,
      "learning_rate": 3.7396694214876034e-05,
      "loss": 10.5795,
      "step": 30500
    },
    {
      "epoch": 256.198347107438,
      "grad_norm": 18.98477554321289,
      "learning_rate": 3.71900826446281e-05,
      "loss": 10.5329,
      "step": 31000
    },
    {
      "epoch": 260.3305785123967,
      "grad_norm": 22.544965744018555,
      "learning_rate": 3.698347107438017e-05,
      "loss": 10.514,
      "step": 31500
    },
    {
      "epoch": 264.46280991735534,
      "grad_norm": 25.763395309448242,
      "learning_rate": 3.6776859504132236e-05,
      "loss": 10.5477,
      "step": 32000
    },
    {
      "epoch": 268.59504132231405,
      "grad_norm": 15.119967460632324,
      "learning_rate": 3.65702479338843e-05,
      "loss": 10.5124,
      "step": 32500
    },
    {
      "epoch": 272.72727272727275,
      "grad_norm": 18.715129852294922,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 10.549,
      "step": 33000
    },
    {
      "epoch": 276.8595041322314,
      "grad_norm": 40.30424499511719,
      "learning_rate": 3.615702479338843e-05,
      "loss": 10.5262,
      "step": 33500
    },
    {
      "epoch": 280.9917355371901,
      "grad_norm": 24.734586715698242,
      "learning_rate": 3.59504132231405e-05,
      "loss": 10.5423,
      "step": 34000
    },
    {
      "epoch": 285.12396694214874,
      "grad_norm": 15.061891555786133,
      "learning_rate": 3.574380165289256e-05,
      "loss": 10.5142,
      "step": 34500
    },
    {
      "epoch": 289.25619834710744,
      "grad_norm": 23.368040084838867,
      "learning_rate": 3.553719008264463e-05,
      "loss": 10.513,
      "step": 35000
    },
    {
      "epoch": 293.3884297520661,
      "grad_norm": 15.50491714477539,
      "learning_rate": 3.5330578512396694e-05,
      "loss": 10.5171,
      "step": 35500
    },
    {
      "epoch": 297.5206611570248,
      "grad_norm": 25.616619110107422,
      "learning_rate": 3.512396694214876e-05,
      "loss": 10.5216,
      "step": 36000
    },
    {
      "epoch": 301.6528925619835,
      "grad_norm": 16.161117553710938,
      "learning_rate": 3.491735537190083e-05,
      "loss": 10.4707,
      "step": 36500
    },
    {
      "epoch": 305.78512396694214,
      "grad_norm": 16.24015235900879,
      "learning_rate": 3.47107438016529e-05,
      "loss": 10.5187,
      "step": 37000
    },
    {
      "epoch": 309.91735537190084,
      "grad_norm": 16.083189010620117,
      "learning_rate": 3.4504132231404964e-05,
      "loss": 10.5227,
      "step": 37500
    },
    {
      "epoch": 314.0495867768595,
      "grad_norm": 25.154489517211914,
      "learning_rate": 3.429752066115703e-05,
      "loss": 10.5181,
      "step": 38000
    },
    {
      "epoch": 318.1818181818182,
      "grad_norm": 20.284324645996094,
      "learning_rate": 3.409090909090909e-05,
      "loss": 10.465,
      "step": 38500
    },
    {
      "epoch": 322.31404958677683,
      "grad_norm": 16.84857749938965,
      "learning_rate": 3.388429752066116e-05,
      "loss": 10.4965,
      "step": 39000
    },
    {
      "epoch": 326.44628099173553,
      "grad_norm": 19.53676414489746,
      "learning_rate": 3.367768595041322e-05,
      "loss": 10.5057,
      "step": 39500
    },
    {
      "epoch": 330.57851239669424,
      "grad_norm": 42.251312255859375,
      "learning_rate": 3.347107438016529e-05,
      "loss": 10.489,
      "step": 40000
    },
    {
      "epoch": 334.7107438016529,
      "grad_norm": 21.786718368530273,
      "learning_rate": 3.3264462809917355e-05,
      "loss": 10.4838,
      "step": 40500
    },
    {
      "epoch": 338.8429752066116,
      "grad_norm": 41.29119873046875,
      "learning_rate": 3.305785123966942e-05,
      "loss": 10.4908,
      "step": 41000
    },
    {
      "epoch": 342.97520661157023,
      "grad_norm": 13.133056640625,
      "learning_rate": 3.285123966942149e-05,
      "loss": 10.4947,
      "step": 41500
    },
    {
      "epoch": 347.10743801652893,
      "grad_norm": 22.474401473999023,
      "learning_rate": 3.264462809917356e-05,
      "loss": 10.4972,
      "step": 42000
    },
    {
      "epoch": 351.2396694214876,
      "grad_norm": 23.623048782348633,
      "learning_rate": 3.243801652892562e-05,
      "loss": 10.4559,
      "step": 42500
    },
    {
      "epoch": 355.3719008264463,
      "grad_norm": 21.291711807250977,
      "learning_rate": 3.2231404958677685e-05,
      "loss": 10.4948,
      "step": 43000
    },
    {
      "epoch": 359.504132231405,
      "grad_norm": 35.49480056762695,
      "learning_rate": 3.202479338842975e-05,
      "loss": 10.4928,
      "step": 43500
    },
    {
      "epoch": 363.6363636363636,
      "grad_norm": 15.213943481445312,
      "learning_rate": 3.181818181818182e-05,
      "loss": 10.4437,
      "step": 44000
    },
    {
      "epoch": 367.7685950413223,
      "grad_norm": 13.780783653259277,
      "learning_rate": 3.161157024793389e-05,
      "loss": 10.4727,
      "step": 44500
    },
    {
      "epoch": 371.900826446281,
      "grad_norm": 14.811641693115234,
      "learning_rate": 3.1404958677685955e-05,
      "loss": 10.4732,
      "step": 45000
    },
    {
      "epoch": 376.0330578512397,
      "grad_norm": 18.18851089477539,
      "learning_rate": 3.119834710743802e-05,
      "loss": 10.4623,
      "step": 45500
    },
    {
      "epoch": 380.1652892561983,
      "grad_norm": 23.1408748626709,
      "learning_rate": 3.099173553719008e-05,
      "loss": 10.4691,
      "step": 46000
    },
    {
      "epoch": 384.297520661157,
      "grad_norm": 17.17743492126465,
      "learning_rate": 3.078512396694215e-05,
      "loss": 10.4394,
      "step": 46500
    },
    {
      "epoch": 388.4297520661157,
      "grad_norm": 24.894590377807617,
      "learning_rate": 3.057851239669421e-05,
      "loss": 10.4628,
      "step": 47000
    },
    {
      "epoch": 392.56198347107437,
      "grad_norm": 17.209802627563477,
      "learning_rate": 3.0371900826446282e-05,
      "loss": 10.4627,
      "step": 47500
    },
    {
      "epoch": 396.6942148760331,
      "grad_norm": 15.587900161743164,
      "learning_rate": 3.016528925619835e-05,
      "loss": 10.4439,
      "step": 48000
    },
    {
      "epoch": 400.8264462809917,
      "grad_norm": 16.58292579650879,
      "learning_rate": 2.9958677685950414e-05,
      "loss": 10.475,
      "step": 48500
    },
    {
      "epoch": 404.9586776859504,
      "grad_norm": 29.34184455871582,
      "learning_rate": 2.975206611570248e-05,
      "loss": 10.4755,
      "step": 49000
    },
    {
      "epoch": 409.09090909090907,
      "grad_norm": 23.580305099487305,
      "learning_rate": 2.954545454545455e-05,
      "loss": 10.4487,
      "step": 49500
    },
    {
      "epoch": 413.22314049586777,
      "grad_norm": 15.6605863571167,
      "learning_rate": 2.9338842975206616e-05,
      "loss": 10.453,
      "step": 50000
    },
    {
      "epoch": 417.35537190082647,
      "grad_norm": 23.03619384765625,
      "learning_rate": 2.9132231404958676e-05,
      "loss": 10.4295,
      "step": 50500
    },
    {
      "epoch": 421.4876033057851,
      "grad_norm": 42.0230712890625,
      "learning_rate": 2.8925619834710744e-05,
      "loss": 10.4156,
      "step": 51000
    },
    {
      "epoch": 425.6198347107438,
      "grad_norm": 25.20656967163086,
      "learning_rate": 2.871900826446281e-05,
      "loss": 10.4661,
      "step": 51500
    },
    {
      "epoch": 429.75206611570246,
      "grad_norm": 19.46259117126465,
      "learning_rate": 2.8512396694214875e-05,
      "loss": 10.454,
      "step": 52000
    },
    {
      "epoch": 433.88429752066116,
      "grad_norm": 14.9389009475708,
      "learning_rate": 2.8305785123966943e-05,
      "loss": 10.4352,
      "step": 52500
    },
    {
      "epoch": 438.0165289256198,
      "grad_norm": 26.286691665649414,
      "learning_rate": 2.809917355371901e-05,
      "loss": 10.4194,
      "step": 53000
    },
    {
      "epoch": 442.1487603305785,
      "grad_norm": 37.34561538696289,
      "learning_rate": 2.7892561983471078e-05,
      "loss": 10.4283,
      "step": 53500
    },
    {
      "epoch": 446.2809917355372,
      "grad_norm": 21.129793167114258,
      "learning_rate": 2.7685950413223145e-05,
      "loss": 10.4101,
      "step": 54000
    },
    {
      "epoch": 450.41322314049586,
      "grad_norm": 16.755111694335938,
      "learning_rate": 2.7479338842975206e-05,
      "loss": 10.4235,
      "step": 54500
    },
    {
      "epoch": 454.54545454545456,
      "grad_norm": 16.53533172607422,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 10.4403,
      "step": 55000
    },
    {
      "epoch": 458.6776859504132,
      "grad_norm": 18.913936614990234,
      "learning_rate": 2.7066115702479337e-05,
      "loss": 10.4318,
      "step": 55500
    },
    {
      "epoch": 462.8099173553719,
      "grad_norm": 20.53507423400879,
      "learning_rate": 2.6859504132231405e-05,
      "loss": 10.4522,
      "step": 56000
    },
    {
      "epoch": 466.94214876033055,
      "grad_norm": 17.370563507080078,
      "learning_rate": 2.6652892561983472e-05,
      "loss": 10.4192,
      "step": 56500
    },
    {
      "epoch": 471.07438016528926,
      "grad_norm": 14.934922218322754,
      "learning_rate": 2.644628099173554e-05,
      "loss": 10.4503,
      "step": 57000
    },
    {
      "epoch": 475.20661157024796,
      "grad_norm": 24.584218978881836,
      "learning_rate": 2.6239669421487607e-05,
      "loss": 10.4302,
      "step": 57500
    },
    {
      "epoch": 479.3388429752066,
      "grad_norm": 21.141544342041016,
      "learning_rate": 2.6033057851239674e-05,
      "loss": 10.4207,
      "step": 58000
    },
    {
      "epoch": 483.4710743801653,
      "grad_norm": 20.670419692993164,
      "learning_rate": 2.5826446280991735e-05,
      "loss": 10.419,
      "step": 58500
    },
    {
      "epoch": 487.60330578512395,
      "grad_norm": 35.93917465209961,
      "learning_rate": 2.5619834710743802e-05,
      "loss": 10.4204,
      "step": 59000
    },
    {
      "epoch": 491.73553719008265,
      "grad_norm": 16.37688636779785,
      "learning_rate": 2.5413223140495866e-05,
      "loss": 10.4373,
      "step": 59500
    },
    {
      "epoch": 495.8677685950413,
      "grad_norm": 15.52349853515625,
      "learning_rate": 2.5206611570247934e-05,
      "loss": 10.4042,
      "step": 60000
    },
    {
      "epoch": 500.0,
      "grad_norm": 21.109739303588867,
      "learning_rate": 2.5e-05,
      "loss": 10.4113,
      "step": 60500
    },
    {
      "epoch": 504.1322314049587,
      "grad_norm": 42.413002014160156,
      "learning_rate": 2.479338842975207e-05,
      "loss": 10.4135,
      "step": 61000
    },
    {
      "epoch": 508.26446280991735,
      "grad_norm": 26.892684936523438,
      "learning_rate": 2.4586776859504136e-05,
      "loss": 10.4256,
      "step": 61500
    },
    {
      "epoch": 512.396694214876,
      "grad_norm": 32.732017517089844,
      "learning_rate": 2.43801652892562e-05,
      "loss": 10.4213,
      "step": 62000
    },
    {
      "epoch": 516.5289256198347,
      "grad_norm": 40.151119232177734,
      "learning_rate": 2.4173553719008264e-05,
      "loss": 10.4152,
      "step": 62500
    },
    {
      "epoch": 520.6611570247934,
      "grad_norm": 17.71255874633789,
      "learning_rate": 2.396694214876033e-05,
      "loss": 10.4666,
      "step": 63000
    },
    {
      "epoch": 524.7933884297521,
      "grad_norm": 19.88428497314453,
      "learning_rate": 2.37603305785124e-05,
      "loss": 10.387,
      "step": 63500
    },
    {
      "epoch": 528.9256198347107,
      "grad_norm": 25.993572235107422,
      "learning_rate": 2.3553719008264463e-05,
      "loss": 10.3778,
      "step": 64000
    },
    {
      "epoch": 533.0578512396694,
      "grad_norm": 21.61750602722168,
      "learning_rate": 2.334710743801653e-05,
      "loss": 10.3971,
      "step": 64500
    },
    {
      "epoch": 537.1900826446281,
      "grad_norm": 36.22737121582031,
      "learning_rate": 2.3140495867768598e-05,
      "loss": 10.4036,
      "step": 65000
    },
    {
      "epoch": 541.3223140495868,
      "grad_norm": 30.953380584716797,
      "learning_rate": 2.2933884297520665e-05,
      "loss": 10.4215,
      "step": 65500
    },
    {
      "epoch": 545.4545454545455,
      "grad_norm": 24.48067855834961,
      "learning_rate": 2.272727272727273e-05,
      "loss": 10.4306,
      "step": 66000
    },
    {
      "epoch": 549.5867768595041,
      "grad_norm": 27.16787338256836,
      "learning_rate": 2.2520661157024793e-05,
      "loss": 10.3919,
      "step": 66500
    },
    {
      "epoch": 553.7190082644628,
      "grad_norm": 37.90617752075195,
      "learning_rate": 2.231404958677686e-05,
      "loss": 10.408,
      "step": 67000
    },
    {
      "epoch": 557.8512396694215,
      "grad_norm": 33.43796920776367,
      "learning_rate": 2.2107438016528928e-05,
      "loss": 10.4084,
      "step": 67500
    },
    {
      "epoch": 561.9834710743802,
      "grad_norm": 25.32017707824707,
      "learning_rate": 2.1900826446280992e-05,
      "loss": 10.3824,
      "step": 68000
    },
    {
      "epoch": 566.1157024793389,
      "grad_norm": 15.407135963439941,
      "learning_rate": 2.169421487603306e-05,
      "loss": 10.3952,
      "step": 68500
    },
    {
      "epoch": 570.2479338842975,
      "grad_norm": 16.045412063598633,
      "learning_rate": 2.1487603305785127e-05,
      "loss": 10.4072,
      "step": 69000
    },
    {
      "epoch": 574.3801652892562,
      "grad_norm": 13.940682411193848,
      "learning_rate": 2.128099173553719e-05,
      "loss": 10.4024,
      "step": 69500
    },
    {
      "epoch": 578.5123966942149,
      "grad_norm": 15.73843002319336,
      "learning_rate": 2.1074380165289255e-05,
      "loss": 10.3903,
      "step": 70000
    },
    {
      "epoch": 582.6446280991736,
      "grad_norm": 21.444684982299805,
      "learning_rate": 2.0867768595041323e-05,
      "loss": 10.4002,
      "step": 70500
    },
    {
      "epoch": 586.7768595041322,
      "grad_norm": 23.246524810791016,
      "learning_rate": 2.066115702479339e-05,
      "loss": 10.4055,
      "step": 71000
    },
    {
      "epoch": 590.9090909090909,
      "grad_norm": 13.319497108459473,
      "learning_rate": 2.0454545454545457e-05,
      "loss": 10.4011,
      "step": 71500
    },
    {
      "epoch": 595.0413223140496,
      "grad_norm": 18.06191062927246,
      "learning_rate": 2.024793388429752e-05,
      "loss": 10.3717,
      "step": 72000
    },
    {
      "epoch": 599.1735537190083,
      "grad_norm": 38.206695556640625,
      "learning_rate": 2.004132231404959e-05,
      "loss": 10.4038,
      "step": 72500
    },
    {
      "epoch": 603.305785123967,
      "grad_norm": 15.68794059753418,
      "learning_rate": 1.9834710743801653e-05,
      "loss": 10.3979,
      "step": 73000
    },
    {
      "epoch": 607.4380165289256,
      "grad_norm": 29.438152313232422,
      "learning_rate": 1.962809917355372e-05,
      "loss": 10.4077,
      "step": 73500
    },
    {
      "epoch": 611.5702479338843,
      "grad_norm": 15.465295791625977,
      "learning_rate": 1.9421487603305784e-05,
      "loss": 10.3944,
      "step": 74000
    },
    {
      "epoch": 615.702479338843,
      "grad_norm": 11.565686225891113,
      "learning_rate": 1.9214876033057852e-05,
      "loss": 10.3982,
      "step": 74500
    },
    {
      "epoch": 619.8347107438017,
      "grad_norm": 18.41701889038086,
      "learning_rate": 1.900826446280992e-05,
      "loss": 10.3853,
      "step": 75000
    },
    {
      "epoch": 623.9669421487604,
      "grad_norm": 32.35441589355469,
      "learning_rate": 1.8801652892561987e-05,
      "loss": 10.4022,
      "step": 75500
    },
    {
      "epoch": 628.099173553719,
      "grad_norm": 24.87134552001953,
      "learning_rate": 1.859504132231405e-05,
      "loss": 10.3972,
      "step": 76000
    },
    {
      "epoch": 632.2314049586777,
      "grad_norm": 26.786407470703125,
      "learning_rate": 1.8388429752066118e-05,
      "loss": 10.3846,
      "step": 76500
    },
    {
      "epoch": 636.3636363636364,
      "grad_norm": 22.627893447875977,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 10.4176,
      "step": 77000
    },
    {
      "epoch": 640.4958677685951,
      "grad_norm": 19.662076950073242,
      "learning_rate": 1.797520661157025e-05,
      "loss": 10.3683,
      "step": 77500
    },
    {
      "epoch": 644.6280991735537,
      "grad_norm": 15.260043144226074,
      "learning_rate": 1.7768595041322314e-05,
      "loss": 10.4008,
      "step": 78000
    },
    {
      "epoch": 648.7603305785124,
      "grad_norm": 37.618465423583984,
      "learning_rate": 1.756198347107438e-05,
      "loss": 10.3917,
      "step": 78500
    },
    {
      "epoch": 652.8925619834711,
      "grad_norm": 12.265318870544434,
      "learning_rate": 1.735537190082645e-05,
      "loss": 10.3503,
      "step": 79000
    },
    {
      "epoch": 657.0247933884298,
      "grad_norm": 23.83986473083496,
      "learning_rate": 1.7148760330578516e-05,
      "loss": 10.3909,
      "step": 79500
    },
    {
      "epoch": 661.1570247933885,
      "grad_norm": 21.276540756225586,
      "learning_rate": 1.694214876033058e-05,
      "loss": 10.3755,
      "step": 80000
    },
    {
      "epoch": 665.2892561983471,
      "grad_norm": 17.48516845703125,
      "learning_rate": 1.6735537190082644e-05,
      "loss": 10.398,
      "step": 80500
    },
    {
      "epoch": 669.4214876033058,
      "grad_norm": 23.484121322631836,
      "learning_rate": 1.652892561983471e-05,
      "loss": 10.3864,
      "step": 81000
    },
    {
      "epoch": 673.5537190082645,
      "grad_norm": 36.88665771484375,
      "learning_rate": 1.632231404958678e-05,
      "loss": 10.3833,
      "step": 81500
    },
    {
      "epoch": 677.6859504132232,
      "grad_norm": 26.39670753479004,
      "learning_rate": 1.6115702479338843e-05,
      "loss": 10.3905,
      "step": 82000
    },
    {
      "epoch": 681.8181818181819,
      "grad_norm": 15.977500915527344,
      "learning_rate": 1.590909090909091e-05,
      "loss": 10.3698,
      "step": 82500
    },
    {
      "epoch": 685.9504132231405,
      "grad_norm": 16.48129653930664,
      "learning_rate": 1.5702479338842978e-05,
      "loss": 10.3847,
      "step": 83000
    },
    {
      "epoch": 690.0826446280992,
      "grad_norm": 15.562871932983398,
      "learning_rate": 1.549586776859504e-05,
      "loss": 10.3772,
      "step": 83500
    },
    {
      "epoch": 694.2148760330579,
      "grad_norm": 27.04522705078125,
      "learning_rate": 1.5289256198347106e-05,
      "loss": 10.3591,
      "step": 84000
    },
    {
      "epoch": 698.3471074380166,
      "grad_norm": 25.958248138427734,
      "learning_rate": 1.5082644628099175e-05,
      "loss": 10.355,
      "step": 84500
    },
    {
      "epoch": 702.4793388429752,
      "grad_norm": 24.652620315551758,
      "learning_rate": 1.487603305785124e-05,
      "loss": 10.3891,
      "step": 85000
    },
    {
      "epoch": 706.6115702479339,
      "grad_norm": 38.010196685791016,
      "learning_rate": 1.4669421487603308e-05,
      "loss": 10.3909,
      "step": 85500
    },
    {
      "epoch": 710.7438016528926,
      "grad_norm": 30.62753677368164,
      "learning_rate": 1.4462809917355372e-05,
      "loss": 10.3842,
      "step": 86000
    },
    {
      "epoch": 714.8760330578513,
      "grad_norm": 21.642606735229492,
      "learning_rate": 1.4256198347107438e-05,
      "loss": 10.3892,
      "step": 86500
    },
    {
      "epoch": 719.00826446281,
      "grad_norm": 17.81034278869629,
      "learning_rate": 1.4049586776859505e-05,
      "loss": 10.3793,
      "step": 87000
    },
    {
      "epoch": 723.1404958677685,
      "grad_norm": 23.505760192871094,
      "learning_rate": 1.3842975206611573e-05,
      "loss": 10.3469,
      "step": 87500
    },
    {
      "epoch": 727.2727272727273,
      "grad_norm": 20.125350952148438,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 10.3736,
      "step": 88000
    },
    {
      "epoch": 731.404958677686,
      "grad_norm": 14.955270767211914,
      "learning_rate": 1.3429752066115702e-05,
      "loss": 10.3682,
      "step": 88500
    },
    {
      "epoch": 735.5371900826447,
      "grad_norm": 21.13946533203125,
      "learning_rate": 1.322314049586777e-05,
      "loss": 10.3785,
      "step": 89000
    },
    {
      "epoch": 739.6694214876034,
      "grad_norm": 24.742250442504883,
      "learning_rate": 1.3016528925619837e-05,
      "loss": 10.3611,
      "step": 89500
    },
    {
      "epoch": 743.801652892562,
      "grad_norm": 13.813817977905273,
      "learning_rate": 1.2809917355371901e-05,
      "loss": 10.3847,
      "step": 90000
    },
    {
      "epoch": 747.9338842975206,
      "grad_norm": 24.8188533782959,
      "learning_rate": 1.2603305785123967e-05,
      "loss": 10.3846,
      "step": 90500
    },
    {
      "epoch": 752.0661157024794,
      "grad_norm": 31.883729934692383,
      "learning_rate": 1.2396694214876034e-05,
      "loss": 10.3579,
      "step": 91000
    },
    {
      "epoch": 756.198347107438,
      "grad_norm": 28.218740463256836,
      "learning_rate": 1.21900826446281e-05,
      "loss": 10.375,
      "step": 91500
    },
    {
      "epoch": 760.3305785123966,
      "grad_norm": 28.656112670898438,
      "learning_rate": 1.1983471074380166e-05,
      "loss": 10.3816,
      "step": 92000
    },
    {
      "epoch": 764.4628099173553,
      "grad_norm": 24.244525909423828,
      "learning_rate": 1.1776859504132231e-05,
      "loss": 10.3553,
      "step": 92500
    },
    {
      "epoch": 768.595041322314,
      "grad_norm": 28.43316078186035,
      "learning_rate": 1.1570247933884299e-05,
      "loss": 10.3987,
      "step": 93000
    },
    {
      "epoch": 772.7272727272727,
      "grad_norm": 23.353897094726562,
      "learning_rate": 1.1363636363636365e-05,
      "loss": 10.3836,
      "step": 93500
    },
    {
      "epoch": 776.8595041322315,
      "grad_norm": 28.791494369506836,
      "learning_rate": 1.115702479338843e-05,
      "loss": 10.3711,
      "step": 94000
    },
    {
      "epoch": 780.99173553719,
      "grad_norm": 17.233352661132812,
      "learning_rate": 1.0950413223140496e-05,
      "loss": 10.3699,
      "step": 94500
    },
    {
      "epoch": 785.1239669421487,
      "grad_norm": 19.160654067993164,
      "learning_rate": 1.0743801652892564e-05,
      "loss": 10.3869,
      "step": 95000
    },
    {
      "epoch": 789.2561983471074,
      "grad_norm": 21.219791412353516,
      "learning_rate": 1.0537190082644628e-05,
      "loss": 10.3496,
      "step": 95500
    },
    {
      "epoch": 793.3884297520661,
      "grad_norm": 44.199337005615234,
      "learning_rate": 1.0330578512396695e-05,
      "loss": 10.3767,
      "step": 96000
    },
    {
      "epoch": 797.5206611570248,
      "grad_norm": 10.741833686828613,
      "learning_rate": 1.012396694214876e-05,
      "loss": 10.3458,
      "step": 96500
    },
    {
      "epoch": 801.6528925619834,
      "grad_norm": 24.632930755615234,
      "learning_rate": 9.917355371900826e-06,
      "loss": 10.3935,
      "step": 97000
    },
    {
      "epoch": 805.7851239669421,
      "grad_norm": 20.80477523803711,
      "learning_rate": 9.710743801652892e-06,
      "loss": 10.3249,
      "step": 97500
    },
    {
      "epoch": 809.9173553719008,
      "grad_norm": 25.936927795410156,
      "learning_rate": 9.50413223140496e-06,
      "loss": 10.382,
      "step": 98000
    },
    {
      "epoch": 814.0495867768595,
      "grad_norm": 16.2012882232666,
      "learning_rate": 9.297520661157025e-06,
      "loss": 10.3634,
      "step": 98500
    },
    {
      "epoch": 818.1818181818181,
      "grad_norm": 30.373950958251953,
      "learning_rate": 9.090909090909091e-06,
      "loss": 10.3552,
      "step": 99000
    },
    {
      "epoch": 822.3140495867768,
      "grad_norm": 20.216716766357422,
      "learning_rate": 8.884297520661157e-06,
      "loss": 10.3822,
      "step": 99500
    },
    {
      "epoch": 826.4462809917355,
      "grad_norm": 16.574857711791992,
      "learning_rate": 8.677685950413224e-06,
      "loss": 10.3776,
      "step": 100000
    },
    {
      "epoch": 830.5785123966942,
      "grad_norm": 18.172687530517578,
      "learning_rate": 8.47107438016529e-06,
      "loss": 10.3572,
      "step": 100500
    },
    {
      "epoch": 834.7107438016529,
      "grad_norm": 19.651159286499023,
      "learning_rate": 8.264462809917356e-06,
      "loss": 10.3627,
      "step": 101000
    },
    {
      "epoch": 838.8429752066115,
      "grad_norm": 15.982803344726562,
      "learning_rate": 8.057851239669421e-06,
      "loss": 10.3756,
      "step": 101500
    },
    {
      "epoch": 842.9752066115702,
      "grad_norm": 17.409181594848633,
      "learning_rate": 7.851239669421489e-06,
      "loss": 10.3694,
      "step": 102000
    },
    {
      "epoch": 847.1074380165289,
      "grad_norm": 29.928300857543945,
      "learning_rate": 7.644628099173553e-06,
      "loss": 10.353,
      "step": 102500
    },
    {
      "epoch": 851.2396694214876,
      "grad_norm": 20.375551223754883,
      "learning_rate": 7.43801652892562e-06,
      "loss": 10.3455,
      "step": 103000
    },
    {
      "epoch": 855.3719008264463,
      "grad_norm": 31.908374786376953,
      "learning_rate": 7.231404958677686e-06,
      "loss": 10.3833,
      "step": 103500
    },
    {
      "epoch": 859.5041322314049,
      "grad_norm": 14.289555549621582,
      "learning_rate": 7.0247933884297525e-06,
      "loss": 10.3574,
      "step": 104000
    },
    {
      "epoch": 863.6363636363636,
      "grad_norm": 26.519712448120117,
      "learning_rate": 6.818181818181818e-06,
      "loss": 10.342,
      "step": 104500
    },
    {
      "epoch": 867.7685950413223,
      "grad_norm": 20.919801712036133,
      "learning_rate": 6.611570247933885e-06,
      "loss": 10.3517,
      "step": 105000
    },
    {
      "epoch": 871.900826446281,
      "grad_norm": 17.376174926757812,
      "learning_rate": 6.404958677685951e-06,
      "loss": 10.3427,
      "step": 105500
    },
    {
      "epoch": 876.0330578512396,
      "grad_norm": 19.7518367767334,
      "learning_rate": 6.198347107438017e-06,
      "loss": 10.3716,
      "step": 106000
    },
    {
      "epoch": 880.1652892561983,
      "grad_norm": 42.60895538330078,
      "learning_rate": 5.991735537190083e-06,
      "loss": 10.3667,
      "step": 106500
    },
    {
      "epoch": 884.297520661157,
      "grad_norm": 14.255569458007812,
      "learning_rate": 5.7851239669421495e-06,
      "loss": 10.3388,
      "step": 107000
    },
    {
      "epoch": 888.4297520661157,
      "grad_norm": 15.747669219970703,
      "learning_rate": 5.578512396694215e-06,
      "loss": 10.3633,
      "step": 107500
    }
  ],
  "logging_steps": 500,
  "max_steps": 121000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 1520,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4300179615861120.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
