{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 376.8595041322314,
  "eval_steps": 500,
  "global_step": 45600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.132231404958677,
      "grad_norm": 13.91285514831543,
      "learning_rate": 4.979338842975207e-05,
      "loss": 13.411,
      "step": 500
    },
    {
      "epoch": 8.264462809917354,
      "grad_norm": 12.833491325378418,
      "learning_rate": 4.958677685950414e-05,
      "loss": 12.0785,
      "step": 1000
    },
    {
      "epoch": 12.396694214876034,
      "grad_norm": 11.675077438354492,
      "learning_rate": 4.9380165289256205e-05,
      "loss": 11.7532,
      "step": 1500
    },
    {
      "epoch": 16.52892561983471,
      "grad_norm": 12.436896324157715,
      "learning_rate": 4.917355371900827e-05,
      "loss": 11.568,
      "step": 2000
    },
    {
      "epoch": 20.66115702479339,
      "grad_norm": 16.822996139526367,
      "learning_rate": 4.896694214876033e-05,
      "loss": 11.4515,
      "step": 2500
    },
    {
      "epoch": 24.793388429752067,
      "grad_norm": 11.852038383483887,
      "learning_rate": 4.87603305785124e-05,
      "loss": 11.3561,
      "step": 3000
    },
    {
      "epoch": 28.925619834710744,
      "grad_norm": 19.147008895874023,
      "learning_rate": 4.855371900826447e-05,
      "loss": 11.2754,
      "step": 3500
    },
    {
      "epoch": 33.05785123966942,
      "grad_norm": 19.651607513427734,
      "learning_rate": 4.834710743801653e-05,
      "loss": 11.2077,
      "step": 4000
    },
    {
      "epoch": 37.1900826446281,
      "grad_norm": 185.37216186523438,
      "learning_rate": 4.8140495867768596e-05,
      "loss": 11.1767,
      "step": 4500
    },
    {
      "epoch": 41.32231404958678,
      "grad_norm": 24.474308013916016,
      "learning_rate": 4.793388429752066e-05,
      "loss": 11.1081,
      "step": 5000
    },
    {
      "epoch": 45.45454545454545,
      "grad_norm": 17.121244430541992,
      "learning_rate": 4.772727272727273e-05,
      "loss": 11.0908,
      "step": 5500
    },
    {
      "epoch": 49.586776859504134,
      "grad_norm": 23.19003677368164,
      "learning_rate": 4.75206611570248e-05,
      "loss": 11.0255,
      "step": 6000
    },
    {
      "epoch": 53.71900826446281,
      "grad_norm": 37.14229965209961,
      "learning_rate": 4.731404958677686e-05,
      "loss": 11.042,
      "step": 6500
    },
    {
      "epoch": 57.85123966942149,
      "grad_norm": 15.462435722351074,
      "learning_rate": 4.7107438016528926e-05,
      "loss": 10.9435,
      "step": 7000
    },
    {
      "epoch": 61.98347107438016,
      "grad_norm": 24.41610336303711,
      "learning_rate": 4.6900826446280993e-05,
      "loss": 10.9256,
      "step": 7500
    },
    {
      "epoch": 66.11570247933884,
      "grad_norm": 19.84303855895996,
      "learning_rate": 4.669421487603306e-05,
      "loss": 10.9168,
      "step": 8000
    },
    {
      "epoch": 70.24793388429752,
      "grad_norm": 26.09157943725586,
      "learning_rate": 4.648760330578513e-05,
      "loss": 10.869,
      "step": 8500
    },
    {
      "epoch": 74.3801652892562,
      "grad_norm": 24.564319610595703,
      "learning_rate": 4.6280991735537196e-05,
      "loss": 10.9153,
      "step": 9000
    },
    {
      "epoch": 78.51239669421487,
      "grad_norm": 27.021060943603516,
      "learning_rate": 4.607438016528926e-05,
      "loss": 10.8492,
      "step": 9500
    },
    {
      "epoch": 82.64462809917356,
      "grad_norm": 18.20688247680664,
      "learning_rate": 4.586776859504133e-05,
      "loss": 10.8571,
      "step": 10000
    },
    {
      "epoch": 86.77685950413223,
      "grad_norm": 29.072172164916992,
      "learning_rate": 4.566115702479339e-05,
      "loss": 10.8244,
      "step": 10500
    },
    {
      "epoch": 90.9090909090909,
      "grad_norm": 29.7242374420166,
      "learning_rate": 4.545454545454546e-05,
      "loss": 10.8127,
      "step": 11000
    },
    {
      "epoch": 95.04132231404958,
      "grad_norm": 20.182735443115234,
      "learning_rate": 4.524793388429752e-05,
      "loss": 10.79,
      "step": 11500
    },
    {
      "epoch": 99.17355371900827,
      "grad_norm": 29.710208892822266,
      "learning_rate": 4.504132231404959e-05,
      "loss": 10.7965,
      "step": 12000
    },
    {
      "epoch": 103.30578512396694,
      "grad_norm": 20.019262313842773,
      "learning_rate": 4.4834710743801654e-05,
      "loss": 10.7489,
      "step": 12500
    },
    {
      "epoch": 107.43801652892562,
      "grad_norm": 18.957792282104492,
      "learning_rate": 4.462809917355372e-05,
      "loss": 10.7739,
      "step": 13000
    },
    {
      "epoch": 111.5702479338843,
      "grad_norm": 31.60511589050293,
      "learning_rate": 4.442148760330579e-05,
      "loss": 10.7444,
      "step": 13500
    },
    {
      "epoch": 115.70247933884298,
      "grad_norm": 37.755374908447266,
      "learning_rate": 4.4214876033057856e-05,
      "loss": 10.7239,
      "step": 14000
    },
    {
      "epoch": 119.83471074380165,
      "grad_norm": 25.873018264770508,
      "learning_rate": 4.400826446280992e-05,
      "loss": 10.732,
      "step": 14500
    },
    {
      "epoch": 123.96694214876032,
      "grad_norm": 26.764684677124023,
      "learning_rate": 4.3801652892561984e-05,
      "loss": 10.7169,
      "step": 15000
    },
    {
      "epoch": 128.099173553719,
      "grad_norm": 21.62598991394043,
      "learning_rate": 4.359504132231405e-05,
      "loss": 10.7057,
      "step": 15500
    },
    {
      "epoch": 132.23140495867767,
      "grad_norm": 19.434314727783203,
      "learning_rate": 4.338842975206612e-05,
      "loss": 10.7032,
      "step": 16000
    },
    {
      "epoch": 136.36363636363637,
      "grad_norm": 24.640254974365234,
      "learning_rate": 4.318181818181819e-05,
      "loss": 10.6935,
      "step": 16500
    },
    {
      "epoch": 140.49586776859505,
      "grad_norm": 42.58231735229492,
      "learning_rate": 4.2975206611570254e-05,
      "loss": 10.6893,
      "step": 17000
    },
    {
      "epoch": 144.62809917355372,
      "grad_norm": 19.202863693237305,
      "learning_rate": 4.2768595041322315e-05,
      "loss": 10.6867,
      "step": 17500
    },
    {
      "epoch": 148.7603305785124,
      "grad_norm": 19.529769897460938,
      "learning_rate": 4.256198347107438e-05,
      "loss": 10.6848,
      "step": 18000
    },
    {
      "epoch": 152.89256198347107,
      "grad_norm": 23.358797073364258,
      "learning_rate": 4.235537190082644e-05,
      "loss": 10.6554,
      "step": 18500
    },
    {
      "epoch": 157.02479338842974,
      "grad_norm": 38.632896423339844,
      "learning_rate": 4.214876033057851e-05,
      "loss": 10.6819,
      "step": 19000
    },
    {
      "epoch": 161.15702479338842,
      "grad_norm": 27.951366424560547,
      "learning_rate": 4.194214876033058e-05,
      "loss": 10.6641,
      "step": 19500
    },
    {
      "epoch": 165.28925619834712,
      "grad_norm": 27.371828079223633,
      "learning_rate": 4.1735537190082645e-05,
      "loss": 10.6483,
      "step": 20000
    },
    {
      "epoch": 169.4214876033058,
      "grad_norm": 49.54058837890625,
      "learning_rate": 4.152892561983471e-05,
      "loss": 10.6889,
      "step": 20500
    },
    {
      "epoch": 173.55371900826447,
      "grad_norm": 34.460174560546875,
      "learning_rate": 4.132231404958678e-05,
      "loss": 10.6217,
      "step": 21000
    },
    {
      "epoch": 177.68595041322314,
      "grad_norm": 17.4661922454834,
      "learning_rate": 4.111570247933885e-05,
      "loss": 10.6333,
      "step": 21500
    },
    {
      "epoch": 181.8181818181818,
      "grad_norm": 23.878849029541016,
      "learning_rate": 4.0909090909090915e-05,
      "loss": 10.6391,
      "step": 22000
    },
    {
      "epoch": 185.9504132231405,
      "grad_norm": 26.258155822753906,
      "learning_rate": 4.0702479338842975e-05,
      "loss": 10.6221,
      "step": 22500
    },
    {
      "epoch": 190.08264462809916,
      "grad_norm": 15.623177528381348,
      "learning_rate": 4.049586776859504e-05,
      "loss": 10.6138,
      "step": 23000
    },
    {
      "epoch": 194.21487603305786,
      "grad_norm": 14.162776947021484,
      "learning_rate": 4.028925619834711e-05,
      "loss": 10.5997,
      "step": 23500
    },
    {
      "epoch": 198.34710743801654,
      "grad_norm": 23.155902862548828,
      "learning_rate": 4.008264462809918e-05,
      "loss": 10.6177,
      "step": 24000
    },
    {
      "epoch": 202.4793388429752,
      "grad_norm": 22.079450607299805,
      "learning_rate": 3.9876033057851245e-05,
      "loss": 10.5929,
      "step": 24500
    },
    {
      "epoch": 206.61157024793388,
      "grad_norm": 41.40623092651367,
      "learning_rate": 3.9669421487603306e-05,
      "loss": 10.6407,
      "step": 25000
    },
    {
      "epoch": 210.74380165289256,
      "grad_norm": 18.391023635864258,
      "learning_rate": 3.946280991735537e-05,
      "loss": 10.5741,
      "step": 25500
    },
    {
      "epoch": 214.87603305785123,
      "grad_norm": 32.71812438964844,
      "learning_rate": 3.925619834710744e-05,
      "loss": 10.5971,
      "step": 26000
    },
    {
      "epoch": 219.0082644628099,
      "grad_norm": 38.93989944458008,
      "learning_rate": 3.90495867768595e-05,
      "loss": 10.582,
      "step": 26500
    },
    {
      "epoch": 223.1404958677686,
      "grad_norm": 20.56291389465332,
      "learning_rate": 3.884297520661157e-05,
      "loss": 10.5573,
      "step": 27000
    },
    {
      "epoch": 227.27272727272728,
      "grad_norm": 23.214847564697266,
      "learning_rate": 3.8636363636363636e-05,
      "loss": 10.5804,
      "step": 27500
    },
    {
      "epoch": 231.40495867768595,
      "grad_norm": 30.090850830078125,
      "learning_rate": 3.8429752066115703e-05,
      "loss": 10.5763,
      "step": 28000
    },
    {
      "epoch": 235.53719008264463,
      "grad_norm": 15.331023216247559,
      "learning_rate": 3.822314049586777e-05,
      "loss": 10.5621,
      "step": 28500
    },
    {
      "epoch": 239.6694214876033,
      "grad_norm": 26.133493423461914,
      "learning_rate": 3.801652892561984e-05,
      "loss": 10.5584,
      "step": 29000
    },
    {
      "epoch": 243.80165289256198,
      "grad_norm": 16.60858917236328,
      "learning_rate": 3.7809917355371906e-05,
      "loss": 10.5485,
      "step": 29500
    },
    {
      "epoch": 247.93388429752065,
      "grad_norm": 13.893257141113281,
      "learning_rate": 3.760330578512397e-05,
      "loss": 10.5355,
      "step": 30000
    },
    {
      "epoch": 252.06611570247935,
      "grad_norm": 18.37576675415039,
      "learning_rate": 3.7396694214876034e-05,
      "loss": 10.5795,
      "step": 30500
    },
    {
      "epoch": 256.198347107438,
      "grad_norm": 18.98477554321289,
      "learning_rate": 3.71900826446281e-05,
      "loss": 10.5329,
      "step": 31000
    },
    {
      "epoch": 260.3305785123967,
      "grad_norm": 22.544965744018555,
      "learning_rate": 3.698347107438017e-05,
      "loss": 10.514,
      "step": 31500
    },
    {
      "epoch": 264.46280991735534,
      "grad_norm": 25.763395309448242,
      "learning_rate": 3.6776859504132236e-05,
      "loss": 10.5477,
      "step": 32000
    },
    {
      "epoch": 268.59504132231405,
      "grad_norm": 15.119967460632324,
      "learning_rate": 3.65702479338843e-05,
      "loss": 10.5124,
      "step": 32500
    },
    {
      "epoch": 272.72727272727275,
      "grad_norm": 18.715129852294922,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 10.549,
      "step": 33000
    },
    {
      "epoch": 276.8595041322314,
      "grad_norm": 40.30424499511719,
      "learning_rate": 3.615702479338843e-05,
      "loss": 10.5262,
      "step": 33500
    },
    {
      "epoch": 280.9917355371901,
      "grad_norm": 24.734586715698242,
      "learning_rate": 3.59504132231405e-05,
      "loss": 10.5423,
      "step": 34000
    },
    {
      "epoch": 285.12396694214874,
      "grad_norm": 15.061891555786133,
      "learning_rate": 3.574380165289256e-05,
      "loss": 10.5142,
      "step": 34500
    },
    {
      "epoch": 289.25619834710744,
      "grad_norm": 23.368040084838867,
      "learning_rate": 3.553719008264463e-05,
      "loss": 10.513,
      "step": 35000
    },
    {
      "epoch": 293.3884297520661,
      "grad_norm": 15.50491714477539,
      "learning_rate": 3.5330578512396694e-05,
      "loss": 10.5171,
      "step": 35500
    },
    {
      "epoch": 297.5206611570248,
      "grad_norm": 25.616619110107422,
      "learning_rate": 3.512396694214876e-05,
      "loss": 10.5216,
      "step": 36000
    },
    {
      "epoch": 301.6528925619835,
      "grad_norm": 16.161117553710938,
      "learning_rate": 3.491735537190083e-05,
      "loss": 10.4707,
      "step": 36500
    },
    {
      "epoch": 305.78512396694214,
      "grad_norm": 16.24015235900879,
      "learning_rate": 3.47107438016529e-05,
      "loss": 10.5187,
      "step": 37000
    },
    {
      "epoch": 309.91735537190084,
      "grad_norm": 16.083189010620117,
      "learning_rate": 3.4504132231404964e-05,
      "loss": 10.5227,
      "step": 37500
    },
    {
      "epoch": 314.0495867768595,
      "grad_norm": 25.154489517211914,
      "learning_rate": 3.429752066115703e-05,
      "loss": 10.5181,
      "step": 38000
    },
    {
      "epoch": 318.1818181818182,
      "grad_norm": 20.284324645996094,
      "learning_rate": 3.409090909090909e-05,
      "loss": 10.465,
      "step": 38500
    },
    {
      "epoch": 322.31404958677683,
      "grad_norm": 16.84857749938965,
      "learning_rate": 3.388429752066116e-05,
      "loss": 10.4965,
      "step": 39000
    },
    {
      "epoch": 326.44628099173553,
      "grad_norm": 19.53676414489746,
      "learning_rate": 3.367768595041322e-05,
      "loss": 10.5057,
      "step": 39500
    },
    {
      "epoch": 330.57851239669424,
      "grad_norm": 42.251312255859375,
      "learning_rate": 3.347107438016529e-05,
      "loss": 10.489,
      "step": 40000
    },
    {
      "epoch": 334.7107438016529,
      "grad_norm": 21.786718368530273,
      "learning_rate": 3.3264462809917355e-05,
      "loss": 10.4838,
      "step": 40500
    },
    {
      "epoch": 338.8429752066116,
      "grad_norm": 41.29119873046875,
      "learning_rate": 3.305785123966942e-05,
      "loss": 10.4908,
      "step": 41000
    },
    {
      "epoch": 342.97520661157023,
      "grad_norm": 13.133056640625,
      "learning_rate": 3.285123966942149e-05,
      "loss": 10.4947,
      "step": 41500
    },
    {
      "epoch": 347.10743801652893,
      "grad_norm": 22.474401473999023,
      "learning_rate": 3.264462809917356e-05,
      "loss": 10.4972,
      "step": 42000
    },
    {
      "epoch": 351.2396694214876,
      "grad_norm": 23.623048782348633,
      "learning_rate": 3.243801652892562e-05,
      "loss": 10.4559,
      "step": 42500
    },
    {
      "epoch": 355.3719008264463,
      "grad_norm": 21.291711807250977,
      "learning_rate": 3.2231404958677685e-05,
      "loss": 10.4948,
      "step": 43000
    },
    {
      "epoch": 359.504132231405,
      "grad_norm": 35.49480056762695,
      "learning_rate": 3.202479338842975e-05,
      "loss": 10.4928,
      "step": 43500
    },
    {
      "epoch": 363.6363636363636,
      "grad_norm": 15.213943481445312,
      "learning_rate": 3.181818181818182e-05,
      "loss": 10.4437,
      "step": 44000
    },
    {
      "epoch": 367.7685950413223,
      "grad_norm": 13.780783653259277,
      "learning_rate": 3.161157024793389e-05,
      "loss": 10.4727,
      "step": 44500
    },
    {
      "epoch": 371.900826446281,
      "grad_norm": 14.811641693115234,
      "learning_rate": 3.1404958677685955e-05,
      "loss": 10.4732,
      "step": 45000
    },
    {
      "epoch": 376.0330578512397,
      "grad_norm": 18.18851089477539,
      "learning_rate": 3.119834710743802e-05,
      "loss": 10.4623,
      "step": 45500
    }
  ],
  "logging_steps": 500,
  "max_steps": 121000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 1520,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1816978346449920.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
