{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 739.4986449864499,
  "eval_steps": 500,
  "global_step": 545750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 5.091018199920654,
      "learning_rate": 4.996612466124662e-05,
      "loss": 6.803,
      "step": 500
    },
    {
      "epoch": 1.3550135501355014,
      "grad_norm": 3.3020758628845215,
      "learning_rate": 4.993224932249323e-05,
      "loss": 6.1589,
      "step": 1000
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 8.262969017028809,
      "learning_rate": 4.989837398373984e-05,
      "loss": 6.1256,
      "step": 1500
    },
    {
      "epoch": 2.710027100271003,
      "grad_norm": 7.3213653564453125,
      "learning_rate": 4.986449864498645e-05,
      "loss": 6.0501,
      "step": 2000
    },
    {
      "epoch": 3.3875338753387534,
      "grad_norm": 15.888947486877441,
      "learning_rate": 4.9830623306233066e-05,
      "loss": 5.9678,
      "step": 2500
    },
    {
      "epoch": 4.065040650406504,
      "grad_norm": 8.027318954467773,
      "learning_rate": 4.9796747967479676e-05,
      "loss": 5.9816,
      "step": 3000
    },
    {
      "epoch": 4.742547425474255,
      "grad_norm": 9.33156681060791,
      "learning_rate": 4.9762872628726286e-05,
      "loss": 5.9411,
      "step": 3500
    },
    {
      "epoch": 5.420054200542006,
      "grad_norm": 10.087059020996094,
      "learning_rate": 4.97289972899729e-05,
      "loss": 5.8978,
      "step": 4000
    },
    {
      "epoch": 6.097560975609756,
      "grad_norm": 9.301751136779785,
      "learning_rate": 4.969512195121951e-05,
      "loss": 5.9311,
      "step": 4500
    },
    {
      "epoch": 6.775067750677507,
      "grad_norm": 11.422779083251953,
      "learning_rate": 4.966124661246613e-05,
      "loss": 5.8857,
      "step": 5000
    },
    {
      "epoch": 7.452574525745257,
      "grad_norm": 6.880033493041992,
      "learning_rate": 4.962737127371274e-05,
      "loss": 5.8476,
      "step": 5500
    },
    {
      "epoch": 8.130081300813009,
      "grad_norm": 7.027878284454346,
      "learning_rate": 4.959349593495935e-05,
      "loss": 5.8935,
      "step": 6000
    },
    {
      "epoch": 8.807588075880759,
      "grad_norm": 7.722722053527832,
      "learning_rate": 4.955962059620597e-05,
      "loss": 5.8373,
      "step": 6500
    },
    {
      "epoch": 9.48509485094851,
      "grad_norm": 9.376630783081055,
      "learning_rate": 4.952574525745258e-05,
      "loss": 5.8788,
      "step": 7000
    },
    {
      "epoch": 10.16260162601626,
      "grad_norm": 11.910229682922363,
      "learning_rate": 4.9491869918699193e-05,
      "loss": 5.8272,
      "step": 7500
    },
    {
      "epoch": 10.840108401084011,
      "grad_norm": 10.906257629394531,
      "learning_rate": 4.9457994579945803e-05,
      "loss": 5.8068,
      "step": 8000
    },
    {
      "epoch": 11.517615176151761,
      "grad_norm": 14.655856132507324,
      "learning_rate": 4.9424119241192414e-05,
      "loss": 5.8539,
      "step": 8500
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 7.715482234954834,
      "learning_rate": 4.9390243902439024e-05,
      "loss": 5.7983,
      "step": 9000
    },
    {
      "epoch": 12.872628726287262,
      "grad_norm": 11.57014274597168,
      "learning_rate": 4.935636856368564e-05,
      "loss": 5.8107,
      "step": 9500
    },
    {
      "epoch": 13.550135501355014,
      "grad_norm": 8.040351867675781,
      "learning_rate": 4.932249322493225e-05,
      "loss": 5.8246,
      "step": 10000
    },
    {
      "epoch": 14.227642276422765,
      "grad_norm": 13.104251861572266,
      "learning_rate": 4.928861788617886e-05,
      "loss": 5.8106,
      "step": 10500
    },
    {
      "epoch": 14.905149051490515,
      "grad_norm": 9.188155174255371,
      "learning_rate": 4.925474254742548e-05,
      "loss": 5.8132,
      "step": 11000
    },
    {
      "epoch": 15.582655826558266,
      "grad_norm": 11.850305557250977,
      "learning_rate": 4.922086720867209e-05,
      "loss": 5.8105,
      "step": 11500
    },
    {
      "epoch": 16.260162601626018,
      "grad_norm": 11.092033386230469,
      "learning_rate": 4.9186991869918704e-05,
      "loss": 5.784,
      "step": 12000
    },
    {
      "epoch": 16.937669376693766,
      "grad_norm": 8.578161239624023,
      "learning_rate": 4.9153116531165314e-05,
      "loss": 5.7848,
      "step": 12500
    },
    {
      "epoch": 17.615176151761517,
      "grad_norm": 10.56583309173584,
      "learning_rate": 4.9119241192411924e-05,
      "loss": 5.8027,
      "step": 13000
    },
    {
      "epoch": 18.29268292682927,
      "grad_norm": 7.775592803955078,
      "learning_rate": 4.908536585365854e-05,
      "loss": 5.7688,
      "step": 13500
    },
    {
      "epoch": 18.97018970189702,
      "grad_norm": 12.018457412719727,
      "learning_rate": 4.905149051490515e-05,
      "loss": 5.7836,
      "step": 14000
    },
    {
      "epoch": 19.647696476964768,
      "grad_norm": 8.249943733215332,
      "learning_rate": 4.901761517615177e-05,
      "loss": 5.7988,
      "step": 14500
    },
    {
      "epoch": 20.32520325203252,
      "grad_norm": 7.565089702606201,
      "learning_rate": 4.898373983739837e-05,
      "loss": 5.7566,
      "step": 15000
    },
    {
      "epoch": 21.00271002710027,
      "grad_norm": 9.527578353881836,
      "learning_rate": 4.894986449864499e-05,
      "loss": 5.7912,
      "step": 15500
    },
    {
      "epoch": 21.680216802168022,
      "grad_norm": 6.278652667999268,
      "learning_rate": 4.89159891598916e-05,
      "loss": 5.7818,
      "step": 16000
    },
    {
      "epoch": 22.357723577235774,
      "grad_norm": 9.314443588256836,
      "learning_rate": 4.8882113821138215e-05,
      "loss": 5.7763,
      "step": 16500
    },
    {
      "epoch": 23.035230352303522,
      "grad_norm": 14.77385425567627,
      "learning_rate": 4.884823848238483e-05,
      "loss": 5.7497,
      "step": 17000
    },
    {
      "epoch": 23.712737127371273,
      "grad_norm": 9.693492889404297,
      "learning_rate": 4.8814363143631435e-05,
      "loss": 5.775,
      "step": 17500
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 15.656332969665527,
      "learning_rate": 4.878048780487805e-05,
      "loss": 5.7446,
      "step": 18000
    },
    {
      "epoch": 25.067750677506776,
      "grad_norm": 9.059091567993164,
      "learning_rate": 4.874661246612466e-05,
      "loss": 5.797,
      "step": 18500
    },
    {
      "epoch": 25.745257452574524,
      "grad_norm": 9.663070678710938,
      "learning_rate": 4.871273712737128e-05,
      "loss": 5.7604,
      "step": 19000
    },
    {
      "epoch": 26.422764227642276,
      "grad_norm": 14.631203651428223,
      "learning_rate": 4.867886178861789e-05,
      "loss": 5.7375,
      "step": 19500
    },
    {
      "epoch": 27.100271002710027,
      "grad_norm": 7.689953804016113,
      "learning_rate": 4.86449864498645e-05,
      "loss": 5.7543,
      "step": 20000
    },
    {
      "epoch": 27.77777777777778,
      "grad_norm": 12.156087875366211,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 5.7679,
      "step": 20500
    },
    {
      "epoch": 28.45528455284553,
      "grad_norm": 8.73508071899414,
      "learning_rate": 4.8577235772357725e-05,
      "loss": 5.754,
      "step": 21000
    },
    {
      "epoch": 29.132791327913278,
      "grad_norm": 6.654566287994385,
      "learning_rate": 4.854336043360434e-05,
      "loss": 5.7533,
      "step": 21500
    },
    {
      "epoch": 29.81029810298103,
      "grad_norm": 13.123746871948242,
      "learning_rate": 4.8509485094850945e-05,
      "loss": 5.7613,
      "step": 22000
    },
    {
      "epoch": 30.48780487804878,
      "grad_norm": 10.495832443237305,
      "learning_rate": 4.847560975609756e-05,
      "loss": 5.7233,
      "step": 22500
    },
    {
      "epoch": 31.165311653116532,
      "grad_norm": 7.0927605628967285,
      "learning_rate": 4.844173441734418e-05,
      "loss": 5.7641,
      "step": 23000
    },
    {
      "epoch": 31.84281842818428,
      "grad_norm": 8.636788368225098,
      "learning_rate": 4.840785907859079e-05,
      "loss": 5.7313,
      "step": 23500
    },
    {
      "epoch": 32.520325203252035,
      "grad_norm": 9.791983604431152,
      "learning_rate": 4.8373983739837406e-05,
      "loss": 5.7351,
      "step": 24000
    },
    {
      "epoch": 33.19783197831978,
      "grad_norm": 24.067975997924805,
      "learning_rate": 4.834010840108401e-05,
      "loss": 5.7762,
      "step": 24500
    },
    {
      "epoch": 33.87533875338753,
      "grad_norm": 8.568990707397461,
      "learning_rate": 4.8306233062330626e-05,
      "loss": 5.7436,
      "step": 25000
    },
    {
      "epoch": 34.552845528455286,
      "grad_norm": 9.561119079589844,
      "learning_rate": 4.8272357723577236e-05,
      "loss": 5.734,
      "step": 25500
    },
    {
      "epoch": 35.230352303523034,
      "grad_norm": 9.91040325164795,
      "learning_rate": 4.823848238482385e-05,
      "loss": 5.742,
      "step": 26000
    },
    {
      "epoch": 35.90785907859079,
      "grad_norm": 11.39449405670166,
      "learning_rate": 4.820460704607046e-05,
      "loss": 5.7499,
      "step": 26500
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 9.793062210083008,
      "learning_rate": 4.817073170731707e-05,
      "loss": 5.7413,
      "step": 27000
    },
    {
      "epoch": 37.262872628726285,
      "grad_norm": 7.896596431732178,
      "learning_rate": 4.813685636856369e-05,
      "loss": 5.7286,
      "step": 27500
    },
    {
      "epoch": 37.94037940379404,
      "grad_norm": 6.42507791519165,
      "learning_rate": 4.81029810298103e-05,
      "loss": 5.7345,
      "step": 28000
    },
    {
      "epoch": 38.61788617886179,
      "grad_norm": 11.094392776489258,
      "learning_rate": 4.8069105691056916e-05,
      "loss": 5.7163,
      "step": 28500
    },
    {
      "epoch": 39.295392953929536,
      "grad_norm": 9.148066520690918,
      "learning_rate": 4.8035230352303526e-05,
      "loss": 5.7792,
      "step": 29000
    },
    {
      "epoch": 39.97289972899729,
      "grad_norm": 10.703146934509277,
      "learning_rate": 4.8001355013550136e-05,
      "loss": 5.7311,
      "step": 29500
    },
    {
      "epoch": 40.65040650406504,
      "grad_norm": 11.24628734588623,
      "learning_rate": 4.796747967479675e-05,
      "loss": 5.7382,
      "step": 30000
    },
    {
      "epoch": 41.327913279132794,
      "grad_norm": 12.71223258972168,
      "learning_rate": 4.793360433604336e-05,
      "loss": 5.713,
      "step": 30500
    },
    {
      "epoch": 42.00542005420054,
      "grad_norm": 7.349093914031982,
      "learning_rate": 4.789972899728998e-05,
      "loss": 5.7301,
      "step": 31000
    },
    {
      "epoch": 42.68292682926829,
      "grad_norm": 8.91130256652832,
      "learning_rate": 4.786585365853658e-05,
      "loss": 5.7398,
      "step": 31500
    },
    {
      "epoch": 43.360433604336045,
      "grad_norm": 5.983088493347168,
      "learning_rate": 4.78319783197832e-05,
      "loss": 5.7064,
      "step": 32000
    },
    {
      "epoch": 44.03794037940379,
      "grad_norm": 9.94255256652832,
      "learning_rate": 4.779810298102981e-05,
      "loss": 5.7605,
      "step": 32500
    },
    {
      "epoch": 44.71544715447155,
      "grad_norm": 11.870882987976074,
      "learning_rate": 4.776422764227643e-05,
      "loss": 5.7491,
      "step": 33000
    },
    {
      "epoch": 45.392953929539296,
      "grad_norm": 11.326759338378906,
      "learning_rate": 4.773035230352304e-05,
      "loss": 5.696,
      "step": 33500
    },
    {
      "epoch": 46.070460704607044,
      "grad_norm": 11.065422058105469,
      "learning_rate": 4.769647696476965e-05,
      "loss": 5.7135,
      "step": 34000
    },
    {
      "epoch": 46.7479674796748,
      "grad_norm": 11.42674446105957,
      "learning_rate": 4.7662601626016264e-05,
      "loss": 5.7237,
      "step": 34500
    },
    {
      "epoch": 47.42547425474255,
      "grad_norm": 10.969059944152832,
      "learning_rate": 4.7628726287262874e-05,
      "loss": 5.7174,
      "step": 35000
    },
    {
      "epoch": 48.1029810298103,
      "grad_norm": 11.1267671585083,
      "learning_rate": 4.759485094850949e-05,
      "loss": 5.7379,
      "step": 35500
    },
    {
      "epoch": 48.78048780487805,
      "grad_norm": 7.527477741241455,
      "learning_rate": 4.75609756097561e-05,
      "loss": 5.7232,
      "step": 36000
    },
    {
      "epoch": 49.4579945799458,
      "grad_norm": 11.792120933532715,
      "learning_rate": 4.752710027100271e-05,
      "loss": 5.7007,
      "step": 36500
    },
    {
      "epoch": 50.13550135501355,
      "grad_norm": 12.336548805236816,
      "learning_rate": 4.749322493224933e-05,
      "loss": 5.7497,
      "step": 37000
    },
    {
      "epoch": 50.8130081300813,
      "grad_norm": 8.872333526611328,
      "learning_rate": 4.745934959349594e-05,
      "loss": 5.7216,
      "step": 37500
    },
    {
      "epoch": 51.49051490514905,
      "grad_norm": 14.99206256866455,
      "learning_rate": 4.7425474254742554e-05,
      "loss": 5.7078,
      "step": 38000
    },
    {
      "epoch": 52.1680216802168,
      "grad_norm": 14.154269218444824,
      "learning_rate": 4.739159891598916e-05,
      "loss": 5.7151,
      "step": 38500
    },
    {
      "epoch": 52.84552845528455,
      "grad_norm": 16.802804946899414,
      "learning_rate": 4.7357723577235774e-05,
      "loss": 5.7296,
      "step": 39000
    },
    {
      "epoch": 53.523035230352306,
      "grad_norm": 8.198492050170898,
      "learning_rate": 4.732384823848239e-05,
      "loss": 5.6997,
      "step": 39500
    },
    {
      "epoch": 54.200542005420054,
      "grad_norm": 8.03692626953125,
      "learning_rate": 4.7289972899729e-05,
      "loss": 5.7388,
      "step": 40000
    },
    {
      "epoch": 54.8780487804878,
      "grad_norm": 9.592068672180176,
      "learning_rate": 4.725609756097561e-05,
      "loss": 5.703,
      "step": 40500
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 8.117247581481934,
      "learning_rate": 4.722222222222222e-05,
      "loss": 5.7057,
      "step": 41000
    },
    {
      "epoch": 56.233062330623305,
      "grad_norm": 9.578608512878418,
      "learning_rate": 4.718834688346884e-05,
      "loss": 5.7093,
      "step": 41500
    },
    {
      "epoch": 56.91056910569106,
      "grad_norm": 20.542922973632812,
      "learning_rate": 4.715447154471545e-05,
      "loss": 5.7104,
      "step": 42000
    },
    {
      "epoch": 57.58807588075881,
      "grad_norm": 10.634708404541016,
      "learning_rate": 4.7120596205962065e-05,
      "loss": 5.6843,
      "step": 42500
    },
    {
      "epoch": 58.265582655826556,
      "grad_norm": 13.108489036560059,
      "learning_rate": 4.7086720867208675e-05,
      "loss": 5.7408,
      "step": 43000
    },
    {
      "epoch": 58.94308943089431,
      "grad_norm": 7.956690311431885,
      "learning_rate": 4.7052845528455285e-05,
      "loss": 5.7162,
      "step": 43500
    },
    {
      "epoch": 59.62059620596206,
      "grad_norm": 10.43460750579834,
      "learning_rate": 4.70189701897019e-05,
      "loss": 5.7095,
      "step": 44000
    },
    {
      "epoch": 60.29810298102981,
      "grad_norm": 8.457387924194336,
      "learning_rate": 4.698509485094851e-05,
      "loss": 5.7415,
      "step": 44500
    },
    {
      "epoch": 60.97560975609756,
      "grad_norm": 8.040995597839355,
      "learning_rate": 4.695121951219512e-05,
      "loss": 5.6758,
      "step": 45000
    },
    {
      "epoch": 61.65311653116531,
      "grad_norm": 8.13553524017334,
      "learning_rate": 4.691734417344174e-05,
      "loss": 5.6884,
      "step": 45500
    },
    {
      "epoch": 62.330623306233065,
      "grad_norm": 10.497289657592773,
      "learning_rate": 4.688346883468835e-05,
      "loss": 5.7133,
      "step": 46000
    },
    {
      "epoch": 63.00813008130081,
      "grad_norm": 15.844855308532715,
      "learning_rate": 4.6849593495934965e-05,
      "loss": 5.7167,
      "step": 46500
    },
    {
      "epoch": 63.68563685636856,
      "grad_norm": 9.074761390686035,
      "learning_rate": 4.6815718157181575e-05,
      "loss": 5.69,
      "step": 47000
    },
    {
      "epoch": 64.36314363143632,
      "grad_norm": 9.115644454956055,
      "learning_rate": 4.6781842818428185e-05,
      "loss": 5.7144,
      "step": 47500
    },
    {
      "epoch": 65.04065040650407,
      "grad_norm": 7.438839912414551,
      "learning_rate": 4.6747967479674795e-05,
      "loss": 5.7096,
      "step": 48000
    },
    {
      "epoch": 65.71815718157181,
      "grad_norm": 13.010701179504395,
      "learning_rate": 4.671409214092141e-05,
      "loss": 5.7045,
      "step": 48500
    },
    {
      "epoch": 66.39566395663957,
      "grad_norm": 8.549729347229004,
      "learning_rate": 4.668021680216802e-05,
      "loss": 5.7061,
      "step": 49000
    },
    {
      "epoch": 67.07317073170732,
      "grad_norm": 7.454829216003418,
      "learning_rate": 4.664634146341464e-05,
      "loss": 5.6976,
      "step": 49500
    },
    {
      "epoch": 67.75067750677506,
      "grad_norm": 11.433731079101562,
      "learning_rate": 4.661246612466125e-05,
      "loss": 5.7339,
      "step": 50000
    },
    {
      "epoch": 68.42818428184282,
      "grad_norm": 9.491683959960938,
      "learning_rate": 4.657859078590786e-05,
      "loss": 5.6896,
      "step": 50500
    },
    {
      "epoch": 69.10569105691057,
      "grad_norm": 18.037641525268555,
      "learning_rate": 4.6544715447154476e-05,
      "loss": 5.7133,
      "step": 51000
    },
    {
      "epoch": 69.78319783197831,
      "grad_norm": 10.448026657104492,
      "learning_rate": 4.6510840108401086e-05,
      "loss": 5.6725,
      "step": 51500
    },
    {
      "epoch": 70.46070460704607,
      "grad_norm": 8.89246940612793,
      "learning_rate": 4.6476964769647696e-05,
      "loss": 5.701,
      "step": 52000
    },
    {
      "epoch": 71.13821138211382,
      "grad_norm": 9.766633987426758,
      "learning_rate": 4.644308943089431e-05,
      "loss": 5.7397,
      "step": 52500
    },
    {
      "epoch": 71.81571815718158,
      "grad_norm": 7.610794544219971,
      "learning_rate": 4.640921409214092e-05,
      "loss": 5.6946,
      "step": 53000
    },
    {
      "epoch": 72.49322493224932,
      "grad_norm": 11.947199821472168,
      "learning_rate": 4.637533875338754e-05,
      "loss": 5.6909,
      "step": 53500
    },
    {
      "epoch": 73.17073170731707,
      "grad_norm": 9.854961395263672,
      "learning_rate": 4.634146341463415e-05,
      "loss": 5.6739,
      "step": 54000
    },
    {
      "epoch": 73.84823848238483,
      "grad_norm": 6.153323173522949,
      "learning_rate": 4.630758807588076e-05,
      "loss": 5.6826,
      "step": 54500
    },
    {
      "epoch": 74.52574525745257,
      "grad_norm": 9.705251693725586,
      "learning_rate": 4.627371273712737e-05,
      "loss": 5.7062,
      "step": 55000
    },
    {
      "epoch": 75.20325203252033,
      "grad_norm": 8.191561698913574,
      "learning_rate": 4.6239837398373986e-05,
      "loss": 5.6917,
      "step": 55500
    },
    {
      "epoch": 75.88075880758808,
      "grad_norm": 8.579245567321777,
      "learning_rate": 4.62059620596206e-05,
      "loss": 5.6945,
      "step": 56000
    },
    {
      "epoch": 76.55826558265582,
      "grad_norm": 13.465226173400879,
      "learning_rate": 4.617208672086721e-05,
      "loss": 5.7235,
      "step": 56500
    },
    {
      "epoch": 77.23577235772358,
      "grad_norm": 7.795341968536377,
      "learning_rate": 4.613821138211382e-05,
      "loss": 5.6659,
      "step": 57000
    },
    {
      "epoch": 77.91327913279133,
      "grad_norm": 8.340959548950195,
      "learning_rate": 4.610433604336043e-05,
      "loss": 5.7015,
      "step": 57500
    },
    {
      "epoch": 78.59078590785907,
      "grad_norm": 9.505107879638672,
      "learning_rate": 4.607046070460705e-05,
      "loss": 5.7274,
      "step": 58000
    },
    {
      "epoch": 79.26829268292683,
      "grad_norm": 17.29327392578125,
      "learning_rate": 4.603658536585366e-05,
      "loss": 5.6947,
      "step": 58500
    },
    {
      "epoch": 79.94579945799458,
      "grad_norm": 12.023770332336426,
      "learning_rate": 4.600271002710027e-05,
      "loss": 5.6738,
      "step": 59000
    },
    {
      "epoch": 80.62330623306234,
      "grad_norm": 10.241198539733887,
      "learning_rate": 4.596883468834689e-05,
      "loss": 5.7083,
      "step": 59500
    },
    {
      "epoch": 81.30081300813008,
      "grad_norm": 18.282018661499023,
      "learning_rate": 4.59349593495935e-05,
      "loss": 5.6599,
      "step": 60000
    },
    {
      "epoch": 81.97831978319783,
      "grad_norm": 10.7692289352417,
      "learning_rate": 4.5901084010840114e-05,
      "loss": 5.7053,
      "step": 60500
    },
    {
      "epoch": 82.65582655826559,
      "grad_norm": 11.36131763458252,
      "learning_rate": 4.5867208672086724e-05,
      "loss": 5.7152,
      "step": 61000
    },
    {
      "epoch": 83.33333333333333,
      "grad_norm": 12.090702056884766,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 5.6963,
      "step": 61500
    },
    {
      "epoch": 84.01084010840108,
      "grad_norm": 11.715343475341797,
      "learning_rate": 4.579945799457995e-05,
      "loss": 5.6697,
      "step": 62000
    },
    {
      "epoch": 84.68834688346884,
      "grad_norm": 10.861991882324219,
      "learning_rate": 4.576558265582656e-05,
      "loss": 5.7116,
      "step": 62500
    },
    {
      "epoch": 85.36585365853658,
      "grad_norm": 7.252991199493408,
      "learning_rate": 4.573170731707318e-05,
      "loss": 5.686,
      "step": 63000
    },
    {
      "epoch": 86.04336043360433,
      "grad_norm": 7.696922779083252,
      "learning_rate": 4.569783197831978e-05,
      "loss": 5.669,
      "step": 63500
    },
    {
      "epoch": 86.72086720867209,
      "grad_norm": 8.799280166625977,
      "learning_rate": 4.56639566395664e-05,
      "loss": 5.684,
      "step": 64000
    },
    {
      "epoch": 87.39837398373983,
      "grad_norm": 7.576893329620361,
      "learning_rate": 4.563008130081301e-05,
      "loss": 5.7047,
      "step": 64500
    },
    {
      "epoch": 88.07588075880759,
      "grad_norm": 9.006096839904785,
      "learning_rate": 4.5596205962059624e-05,
      "loss": 5.6738,
      "step": 65000
    },
    {
      "epoch": 88.75338753387534,
      "grad_norm": 9.088390350341797,
      "learning_rate": 4.5562330623306234e-05,
      "loss": 5.6929,
      "step": 65500
    },
    {
      "epoch": 89.4308943089431,
      "grad_norm": 10.151029586791992,
      "learning_rate": 4.5528455284552844e-05,
      "loss": 5.7001,
      "step": 66000
    },
    {
      "epoch": 90.10840108401084,
      "grad_norm": 7.036646366119385,
      "learning_rate": 4.549457994579946e-05,
      "loss": 5.7139,
      "step": 66500
    },
    {
      "epoch": 90.78590785907859,
      "grad_norm": 20.453771591186523,
      "learning_rate": 4.546070460704607e-05,
      "loss": 5.6628,
      "step": 67000
    },
    {
      "epoch": 91.46341463414635,
      "grad_norm": 7.753979682922363,
      "learning_rate": 4.542682926829269e-05,
      "loss": 5.7063,
      "step": 67500
    },
    {
      "epoch": 92.14092140921409,
      "grad_norm": 8.421381950378418,
      "learning_rate": 4.53929539295393e-05,
      "loss": 5.675,
      "step": 68000
    },
    {
      "epoch": 92.81842818428184,
      "grad_norm": 10.344338417053223,
      "learning_rate": 4.535907859078591e-05,
      "loss": 5.693,
      "step": 68500
    },
    {
      "epoch": 93.4959349593496,
      "grad_norm": 12.800844192504883,
      "learning_rate": 4.5325203252032525e-05,
      "loss": 5.671,
      "step": 69000
    },
    {
      "epoch": 94.17344173441734,
      "grad_norm": 8.62265396118164,
      "learning_rate": 4.5291327913279135e-05,
      "loss": 5.7068,
      "step": 69500
    },
    {
      "epoch": 94.8509485094851,
      "grad_norm": 6.375396728515625,
      "learning_rate": 4.525745257452575e-05,
      "loss": 5.6704,
      "step": 70000
    },
    {
      "epoch": 95.52845528455285,
      "grad_norm": 6.98372220993042,
      "learning_rate": 4.5223577235772355e-05,
      "loss": 5.6863,
      "step": 70500
    },
    {
      "epoch": 96.2059620596206,
      "grad_norm": 6.998977184295654,
      "learning_rate": 4.518970189701897e-05,
      "loss": 5.6901,
      "step": 71000
    },
    {
      "epoch": 96.88346883468834,
      "grad_norm": 10.754554748535156,
      "learning_rate": 4.515582655826558e-05,
      "loss": 5.6476,
      "step": 71500
    },
    {
      "epoch": 97.5609756097561,
      "grad_norm": 6.208372592926025,
      "learning_rate": 4.51219512195122e-05,
      "loss": 5.7224,
      "step": 72000
    },
    {
      "epoch": 98.23848238482385,
      "grad_norm": 7.487302780151367,
      "learning_rate": 4.5088075880758815e-05,
      "loss": 5.6849,
      "step": 72500
    },
    {
      "epoch": 98.9159891598916,
      "grad_norm": 12.949740409851074,
      "learning_rate": 4.505420054200542e-05,
      "loss": 5.6571,
      "step": 73000
    },
    {
      "epoch": 99.59349593495935,
      "grad_norm": 9.150491714477539,
      "learning_rate": 4.5020325203252035e-05,
      "loss": 5.6813,
      "step": 73500
    },
    {
      "epoch": 100.2710027100271,
      "grad_norm": 7.2192254066467285,
      "learning_rate": 4.4986449864498645e-05,
      "loss": 5.6746,
      "step": 74000
    },
    {
      "epoch": 100.94850948509485,
      "grad_norm": 8.052684783935547,
      "learning_rate": 4.495257452574526e-05,
      "loss": 5.6955,
      "step": 74500
    },
    {
      "epoch": 101.6260162601626,
      "grad_norm": 12.560942649841309,
      "learning_rate": 4.491869918699187e-05,
      "loss": 5.7026,
      "step": 75000
    },
    {
      "epoch": 102.30352303523036,
      "grad_norm": 16.167072296142578,
      "learning_rate": 4.488482384823848e-05,
      "loss": 5.6309,
      "step": 75500
    },
    {
      "epoch": 102.9810298102981,
      "grad_norm": 10.525370597839355,
      "learning_rate": 4.48509485094851e-05,
      "loss": 5.7072,
      "step": 76000
    },
    {
      "epoch": 103.65853658536585,
      "grad_norm": 9.487058639526367,
      "learning_rate": 4.481707317073171e-05,
      "loss": 5.6746,
      "step": 76500
    },
    {
      "epoch": 104.3360433604336,
      "grad_norm": 7.807596683502197,
      "learning_rate": 4.4783197831978326e-05,
      "loss": 5.7144,
      "step": 77000
    },
    {
      "epoch": 105.01355013550136,
      "grad_norm": 9.954623222351074,
      "learning_rate": 4.474932249322493e-05,
      "loss": 5.6621,
      "step": 77500
    },
    {
      "epoch": 105.6910569105691,
      "grad_norm": 10.59250259399414,
      "learning_rate": 4.4715447154471546e-05,
      "loss": 5.6918,
      "step": 78000
    },
    {
      "epoch": 106.36856368563686,
      "grad_norm": 10.967586517333984,
      "learning_rate": 4.468157181571816e-05,
      "loss": 5.6809,
      "step": 78500
    },
    {
      "epoch": 107.04607046070461,
      "grad_norm": 11.346611022949219,
      "learning_rate": 4.464769647696477e-05,
      "loss": 5.6848,
      "step": 79000
    },
    {
      "epoch": 107.72357723577235,
      "grad_norm": 9.244811058044434,
      "learning_rate": 4.461382113821139e-05,
      "loss": 5.6826,
      "step": 79500
    },
    {
      "epoch": 108.40108401084011,
      "grad_norm": 8.84953784942627,
      "learning_rate": 4.457994579945799e-05,
      "loss": 5.6783,
      "step": 80000
    },
    {
      "epoch": 109.07859078590786,
      "grad_norm": 11.30982494354248,
      "learning_rate": 4.454607046070461e-05,
      "loss": 5.679,
      "step": 80500
    },
    {
      "epoch": 109.7560975609756,
      "grad_norm": 9.001773834228516,
      "learning_rate": 4.451219512195122e-05,
      "loss": 5.6881,
      "step": 81000
    },
    {
      "epoch": 110.43360433604336,
      "grad_norm": 10.962065696716309,
      "learning_rate": 4.4478319783197837e-05,
      "loss": 5.6598,
      "step": 81500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 9.60132122039795,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 5.6873,
      "step": 82000
    },
    {
      "epoch": 111.78861788617886,
      "grad_norm": 9.14824104309082,
      "learning_rate": 4.4410569105691057e-05,
      "loss": 5.6482,
      "step": 82500
    },
    {
      "epoch": 112.46612466124661,
      "grad_norm": 7.440793514251709,
      "learning_rate": 4.4376693766937673e-05,
      "loss": 5.7015,
      "step": 83000
    },
    {
      "epoch": 113.14363143631437,
      "grad_norm": 8.676054000854492,
      "learning_rate": 4.4342818428184283e-05,
      "loss": 5.6775,
      "step": 83500
    },
    {
      "epoch": 113.82113821138212,
      "grad_norm": 10.077384948730469,
      "learning_rate": 4.43089430894309e-05,
      "loss": 5.6775,
      "step": 84000
    },
    {
      "epoch": 114.49864498644986,
      "grad_norm": 10.0122652053833,
      "learning_rate": 4.427506775067751e-05,
      "loss": 5.6795,
      "step": 84500
    },
    {
      "epoch": 115.17615176151762,
      "grad_norm": 8.472055435180664,
      "learning_rate": 4.424119241192412e-05,
      "loss": 5.6649,
      "step": 85000
    },
    {
      "epoch": 115.85365853658537,
      "grad_norm": 7.391278266906738,
      "learning_rate": 4.420731707317074e-05,
      "loss": 5.6827,
      "step": 85500
    },
    {
      "epoch": 116.53116531165311,
      "grad_norm": 9.232333183288574,
      "learning_rate": 4.417344173441735e-05,
      "loss": 5.6822,
      "step": 86000
    },
    {
      "epoch": 117.20867208672087,
      "grad_norm": 7.815216541290283,
      "learning_rate": 4.413956639566396e-05,
      "loss": 5.6691,
      "step": 86500
    },
    {
      "epoch": 117.88617886178862,
      "grad_norm": 8.958131790161133,
      "learning_rate": 4.410569105691057e-05,
      "loss": 5.6709,
      "step": 87000
    },
    {
      "epoch": 118.56368563685636,
      "grad_norm": 10.196486473083496,
      "learning_rate": 4.4071815718157184e-05,
      "loss": 5.6763,
      "step": 87500
    },
    {
      "epoch": 119.24119241192412,
      "grad_norm": 7.570178985595703,
      "learning_rate": 4.4037940379403794e-05,
      "loss": 5.6697,
      "step": 88000
    },
    {
      "epoch": 119.91869918699187,
      "grad_norm": 11.12653636932373,
      "learning_rate": 4.400406504065041e-05,
      "loss": 5.6838,
      "step": 88500
    },
    {
      "epoch": 120.59620596205961,
      "grad_norm": 7.945408821105957,
      "learning_rate": 4.397018970189702e-05,
      "loss": 5.6789,
      "step": 89000
    },
    {
      "epoch": 121.27371273712737,
      "grad_norm": 10.780491828918457,
      "learning_rate": 4.393631436314363e-05,
      "loss": 5.6862,
      "step": 89500
    },
    {
      "epoch": 121.95121951219512,
      "grad_norm": 12.257450103759766,
      "learning_rate": 4.390243902439025e-05,
      "loss": 5.683,
      "step": 90000
    },
    {
      "epoch": 122.62872628726288,
      "grad_norm": 10.77872085571289,
      "learning_rate": 4.386856368563686e-05,
      "loss": 5.6704,
      "step": 90500
    },
    {
      "epoch": 123.30623306233062,
      "grad_norm": 8.932414054870605,
      "learning_rate": 4.3834688346883474e-05,
      "loss": 5.6586,
      "step": 91000
    },
    {
      "epoch": 123.98373983739837,
      "grad_norm": 8.264118194580078,
      "learning_rate": 4.3800813008130085e-05,
      "loss": 5.6734,
      "step": 91500
    },
    {
      "epoch": 124.66124661246613,
      "grad_norm": 8.977347373962402,
      "learning_rate": 4.3766937669376695e-05,
      "loss": 5.6708,
      "step": 92000
    },
    {
      "epoch": 125.33875338753387,
      "grad_norm": 7.906527519226074,
      "learning_rate": 4.373306233062331e-05,
      "loss": 5.6883,
      "step": 92500
    },
    {
      "epoch": 126.01626016260163,
      "grad_norm": 11.390049934387207,
      "learning_rate": 4.369918699186992e-05,
      "loss": 5.6622,
      "step": 93000
    },
    {
      "epoch": 126.69376693766938,
      "grad_norm": 8.2907133102417,
      "learning_rate": 4.366531165311653e-05,
      "loss": 5.6695,
      "step": 93500
    },
    {
      "epoch": 127.37127371273712,
      "grad_norm": 15.299290657043457,
      "learning_rate": 4.363143631436314e-05,
      "loss": 5.6923,
      "step": 94000
    },
    {
      "epoch": 128.0487804878049,
      "grad_norm": 11.34947681427002,
      "learning_rate": 4.359756097560976e-05,
      "loss": 5.6475,
      "step": 94500
    },
    {
      "epoch": 128.72628726287263,
      "grad_norm": 8.687628746032715,
      "learning_rate": 4.3563685636856375e-05,
      "loss": 5.6752,
      "step": 95000
    },
    {
      "epoch": 129.40379403794037,
      "grad_norm": 7.222461223602295,
      "learning_rate": 4.3529810298102985e-05,
      "loss": 5.6421,
      "step": 95500
    },
    {
      "epoch": 130.08130081300814,
      "grad_norm": 16.749404907226562,
      "learning_rate": 4.3495934959349595e-05,
      "loss": 5.6976,
      "step": 96000
    },
    {
      "epoch": 130.75880758807588,
      "grad_norm": 7.7512898445129395,
      "learning_rate": 4.3462059620596205e-05,
      "loss": 5.6698,
      "step": 96500
    },
    {
      "epoch": 131.43631436314362,
      "grad_norm": 7.948211669921875,
      "learning_rate": 4.342818428184282e-05,
      "loss": 5.684,
      "step": 97000
    },
    {
      "epoch": 132.1138211382114,
      "grad_norm": 10.333013534545898,
      "learning_rate": 4.339430894308943e-05,
      "loss": 5.6608,
      "step": 97500
    },
    {
      "epoch": 132.79132791327913,
      "grad_norm": 12.077011108398438,
      "learning_rate": 4.336043360433605e-05,
      "loss": 5.6582,
      "step": 98000
    },
    {
      "epoch": 133.46883468834687,
      "grad_norm": 7.491289138793945,
      "learning_rate": 4.332655826558266e-05,
      "loss": 5.6638,
      "step": 98500
    },
    {
      "epoch": 134.14634146341464,
      "grad_norm": 18.78850555419922,
      "learning_rate": 4.329268292682927e-05,
      "loss": 5.693,
      "step": 99000
    },
    {
      "epoch": 134.82384823848238,
      "grad_norm": 11.873628616333008,
      "learning_rate": 4.3258807588075886e-05,
      "loss": 5.6734,
      "step": 99500
    },
    {
      "epoch": 135.50135501355012,
      "grad_norm": 6.847982406616211,
      "learning_rate": 4.3224932249322496e-05,
      "loss": 5.6551,
      "step": 100000
    },
    {
      "epoch": 136.1788617886179,
      "grad_norm": 10.645559310913086,
      "learning_rate": 4.3191056910569106e-05,
      "loss": 5.6491,
      "step": 100500
    },
    {
      "epoch": 136.85636856368563,
      "grad_norm": 9.155359268188477,
      "learning_rate": 4.315718157181572e-05,
      "loss": 5.6783,
      "step": 101000
    },
    {
      "epoch": 137.53387533875338,
      "grad_norm": 9.349299430847168,
      "learning_rate": 4.312330623306233e-05,
      "loss": 5.6559,
      "step": 101500
    },
    {
      "epoch": 138.21138211382114,
      "grad_norm": 8.001784324645996,
      "learning_rate": 4.308943089430895e-05,
      "loss": 5.6998,
      "step": 102000
    },
    {
      "epoch": 138.88888888888889,
      "grad_norm": 9.066689491271973,
      "learning_rate": 4.305555555555556e-05,
      "loss": 5.6705,
      "step": 102500
    },
    {
      "epoch": 139.56639566395663,
      "grad_norm": 8.939154624938965,
      "learning_rate": 4.302168021680217e-05,
      "loss": 5.6288,
      "step": 103000
    },
    {
      "epoch": 140.2439024390244,
      "grad_norm": 12.34425163269043,
      "learning_rate": 4.298780487804878e-05,
      "loss": 5.6979,
      "step": 103500
    },
    {
      "epoch": 140.92140921409214,
      "grad_norm": 12.495677947998047,
      "learning_rate": 4.2953929539295396e-05,
      "loss": 5.6732,
      "step": 104000
    },
    {
      "epoch": 141.59891598915988,
      "grad_norm": 7.094862461090088,
      "learning_rate": 4.2920054200542006e-05,
      "loss": 5.6865,
      "step": 104500
    },
    {
      "epoch": 142.27642276422765,
      "grad_norm": 14.707009315490723,
      "learning_rate": 4.2886178861788616e-05,
      "loss": 5.6574,
      "step": 105000
    },
    {
      "epoch": 142.9539295392954,
      "grad_norm": 7.82401704788208,
      "learning_rate": 4.285230352303523e-05,
      "loss": 5.6533,
      "step": 105500
    },
    {
      "epoch": 143.63143631436316,
      "grad_norm": 12.804460525512695,
      "learning_rate": 4.281842818428184e-05,
      "loss": 5.6438,
      "step": 106000
    },
    {
      "epoch": 144.3089430894309,
      "grad_norm": 16.44430160522461,
      "learning_rate": 4.278455284552846e-05,
      "loss": 5.6972,
      "step": 106500
    },
    {
      "epoch": 144.98644986449864,
      "grad_norm": 14.833091735839844,
      "learning_rate": 4.275067750677507e-05,
      "loss": 5.6578,
      "step": 107000
    },
    {
      "epoch": 145.6639566395664,
      "grad_norm": 11.061844825744629,
      "learning_rate": 4.271680216802168e-05,
      "loss": 5.6809,
      "step": 107500
    },
    {
      "epoch": 146.34146341463415,
      "grad_norm": 8.878724098205566,
      "learning_rate": 4.26829268292683e-05,
      "loss": 5.64,
      "step": 108000
    },
    {
      "epoch": 147.0189701897019,
      "grad_norm": 10.256917953491211,
      "learning_rate": 4.264905149051491e-05,
      "loss": 5.7029,
      "step": 108500
    },
    {
      "epoch": 147.69647696476966,
      "grad_norm": 7.7750244140625,
      "learning_rate": 4.2615176151761524e-05,
      "loss": 5.6495,
      "step": 109000
    },
    {
      "epoch": 148.3739837398374,
      "grad_norm": 12.138590812683105,
      "learning_rate": 4.2581300813008134e-05,
      "loss": 5.6479,
      "step": 109500
    },
    {
      "epoch": 149.05149051490514,
      "grad_norm": 11.45523738861084,
      "learning_rate": 4.2547425474254744e-05,
      "loss": 5.6925,
      "step": 110000
    },
    {
      "epoch": 149.7289972899729,
      "grad_norm": 7.21810245513916,
      "learning_rate": 4.2513550135501354e-05,
      "loss": 5.6429,
      "step": 110500
    },
    {
      "epoch": 150.40650406504065,
      "grad_norm": 7.866654396057129,
      "learning_rate": 4.247967479674797e-05,
      "loss": 5.6948,
      "step": 111000
    },
    {
      "epoch": 151.0840108401084,
      "grad_norm": 8.225414276123047,
      "learning_rate": 4.244579945799459e-05,
      "loss": 5.6728,
      "step": 111500
    },
    {
      "epoch": 151.76151761517616,
      "grad_norm": 9.346206665039062,
      "learning_rate": 4.241192411924119e-05,
      "loss": 5.6457,
      "step": 112000
    },
    {
      "epoch": 152.4390243902439,
      "grad_norm": 7.987818717956543,
      "learning_rate": 4.237804878048781e-05,
      "loss": 5.6619,
      "step": 112500
    },
    {
      "epoch": 153.11653116531164,
      "grad_norm": 6.3528876304626465,
      "learning_rate": 4.234417344173442e-05,
      "loss": 5.6738,
      "step": 113000
    },
    {
      "epoch": 153.7940379403794,
      "grad_norm": 8.427619934082031,
      "learning_rate": 4.2310298102981034e-05,
      "loss": 5.6466,
      "step": 113500
    },
    {
      "epoch": 154.47154471544715,
      "grad_norm": 6.445390701293945,
      "learning_rate": 4.2276422764227644e-05,
      "loss": 5.6817,
      "step": 114000
    },
    {
      "epoch": 155.1490514905149,
      "grad_norm": 10.24166488647461,
      "learning_rate": 4.2242547425474254e-05,
      "loss": 5.6673,
      "step": 114500
    },
    {
      "epoch": 155.82655826558266,
      "grad_norm": 9.981703758239746,
      "learning_rate": 4.220867208672087e-05,
      "loss": 5.6608,
      "step": 115000
    },
    {
      "epoch": 156.5040650406504,
      "grad_norm": 9.267243385314941,
      "learning_rate": 4.217479674796748e-05,
      "loss": 5.6548,
      "step": 115500
    },
    {
      "epoch": 157.18157181571814,
      "grad_norm": 10.711848258972168,
      "learning_rate": 4.21409214092141e-05,
      "loss": 5.6473,
      "step": 116000
    },
    {
      "epoch": 157.8590785907859,
      "grad_norm": 12.067351341247559,
      "learning_rate": 4.21070460704607e-05,
      "loss": 5.6985,
      "step": 116500
    },
    {
      "epoch": 158.53658536585365,
      "grad_norm": 8.329667091369629,
      "learning_rate": 4.207317073170732e-05,
      "loss": 5.6546,
      "step": 117000
    },
    {
      "epoch": 159.21409214092142,
      "grad_norm": 8.025691032409668,
      "learning_rate": 4.2039295392953935e-05,
      "loss": 5.6679,
      "step": 117500
    },
    {
      "epoch": 159.89159891598916,
      "grad_norm": 8.354793548583984,
      "learning_rate": 4.2005420054200545e-05,
      "loss": 5.657,
      "step": 118000
    },
    {
      "epoch": 160.5691056910569,
      "grad_norm": 6.5379743576049805,
      "learning_rate": 4.197154471544716e-05,
      "loss": 5.6516,
      "step": 118500
    },
    {
      "epoch": 161.24661246612467,
      "grad_norm": 7.582290172576904,
      "learning_rate": 4.1937669376693765e-05,
      "loss": 5.6638,
      "step": 119000
    },
    {
      "epoch": 161.92411924119241,
      "grad_norm": 8.392108917236328,
      "learning_rate": 4.190379403794038e-05,
      "loss": 5.6486,
      "step": 119500
    },
    {
      "epoch": 162.60162601626016,
      "grad_norm": 12.599149703979492,
      "learning_rate": 4.186991869918699e-05,
      "loss": 5.6889,
      "step": 120000
    },
    {
      "epoch": 163.27913279132792,
      "grad_norm": 6.67156457901001,
      "learning_rate": 4.183604336043361e-05,
      "loss": 5.6781,
      "step": 120500
    },
    {
      "epoch": 163.95663956639567,
      "grad_norm": 6.853057384490967,
      "learning_rate": 4.180216802168022e-05,
      "loss": 5.6397,
      "step": 121000
    },
    {
      "epoch": 164.6341463414634,
      "grad_norm": 8.395889282226562,
      "learning_rate": 4.176829268292683e-05,
      "loss": 5.645,
      "step": 121500
    },
    {
      "epoch": 165.31165311653118,
      "grad_norm": 8.158552169799805,
      "learning_rate": 4.1734417344173445e-05,
      "loss": 5.7062,
      "step": 122000
    },
    {
      "epoch": 165.98915989159892,
      "grad_norm": 7.307242393493652,
      "learning_rate": 4.1700542005420055e-05,
      "loss": 5.6388,
      "step": 122500
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 10.447093963623047,
      "learning_rate": 4.166666666666667e-05,
      "loss": 5.654,
      "step": 123000
    },
    {
      "epoch": 167.34417344173443,
      "grad_norm": 9.50565242767334,
      "learning_rate": 4.1632791327913275e-05,
      "loss": 5.662,
      "step": 123500
    },
    {
      "epoch": 168.02168021680217,
      "grad_norm": 6.1766862869262695,
      "learning_rate": 4.159891598915989e-05,
      "loss": 5.6712,
      "step": 124000
    },
    {
      "epoch": 168.6991869918699,
      "grad_norm": 11.88149642944336,
      "learning_rate": 4.156504065040651e-05,
      "loss": 5.6642,
      "step": 124500
    },
    {
      "epoch": 169.37669376693768,
      "grad_norm": 9.362071990966797,
      "learning_rate": 4.153116531165312e-05,
      "loss": 5.6449,
      "step": 125000
    },
    {
      "epoch": 170.05420054200542,
      "grad_norm": 8.339675903320312,
      "learning_rate": 4.1497289972899736e-05,
      "loss": 5.664,
      "step": 125500
    },
    {
      "epoch": 170.73170731707316,
      "grad_norm": 15.789373397827148,
      "learning_rate": 4.146341463414634e-05,
      "loss": 5.6478,
      "step": 126000
    },
    {
      "epoch": 171.40921409214093,
      "grad_norm": 6.899135112762451,
      "learning_rate": 4.1429539295392956e-05,
      "loss": 5.6431,
      "step": 126500
    },
    {
      "epoch": 172.08672086720867,
      "grad_norm": 8.920034408569336,
      "learning_rate": 4.1395663956639566e-05,
      "loss": 5.6842,
      "step": 127000
    },
    {
      "epoch": 172.7642276422764,
      "grad_norm": 7.965541839599609,
      "learning_rate": 4.136178861788618e-05,
      "loss": 5.6805,
      "step": 127500
    },
    {
      "epoch": 173.44173441734418,
      "grad_norm": 16.888904571533203,
      "learning_rate": 4.132791327913279e-05,
      "loss": 5.6693,
      "step": 128000
    },
    {
      "epoch": 174.11924119241192,
      "grad_norm": 9.602978706359863,
      "learning_rate": 4.12940379403794e-05,
      "loss": 5.6099,
      "step": 128500
    },
    {
      "epoch": 174.79674796747966,
      "grad_norm": 9.29850959777832,
      "learning_rate": 4.126016260162602e-05,
      "loss": 5.6672,
      "step": 129000
    },
    {
      "epoch": 175.47425474254743,
      "grad_norm": 7.787965297698975,
      "learning_rate": 4.122628726287263e-05,
      "loss": 5.6557,
      "step": 129500
    },
    {
      "epoch": 176.15176151761517,
      "grad_norm": 8.20883846282959,
      "learning_rate": 4.1192411924119246e-05,
      "loss": 5.6861,
      "step": 130000
    },
    {
      "epoch": 176.82926829268294,
      "grad_norm": 9.473078727722168,
      "learning_rate": 4.1158536585365856e-05,
      "loss": 5.6377,
      "step": 130500
    },
    {
      "epoch": 177.50677506775068,
      "grad_norm": 7.341607093811035,
      "learning_rate": 4.1124661246612466e-05,
      "loss": 5.6604,
      "step": 131000
    },
    {
      "epoch": 178.18428184281842,
      "grad_norm": 8.631403923034668,
      "learning_rate": 4.109078590785908e-05,
      "loss": 5.6325,
      "step": 131500
    },
    {
      "epoch": 178.8617886178862,
      "grad_norm": 7.949875831604004,
      "learning_rate": 4.105691056910569e-05,
      "loss": 5.6564,
      "step": 132000
    },
    {
      "epoch": 179.53929539295393,
      "grad_norm": 8.560153007507324,
      "learning_rate": 4.102303523035231e-05,
      "loss": 5.7206,
      "step": 132500
    },
    {
      "epoch": 180.21680216802167,
      "grad_norm": 10.500035285949707,
      "learning_rate": 4.098915989159891e-05,
      "loss": 5.6112,
      "step": 133000
    },
    {
      "epoch": 180.89430894308944,
      "grad_norm": 10.8402099609375,
      "learning_rate": 4.095528455284553e-05,
      "loss": 5.6569,
      "step": 133500
    },
    {
      "epoch": 181.57181571815718,
      "grad_norm": 10.266831398010254,
      "learning_rate": 4.092140921409214e-05,
      "loss": 5.6663,
      "step": 134000
    },
    {
      "epoch": 182.24932249322492,
      "grad_norm": 8.167292594909668,
      "learning_rate": 4.088753387533876e-05,
      "loss": 5.6518,
      "step": 134500
    },
    {
      "epoch": 182.9268292682927,
      "grad_norm": 7.827050685882568,
      "learning_rate": 4.085365853658537e-05,
      "loss": 5.6641,
      "step": 135000
    },
    {
      "epoch": 183.60433604336043,
      "grad_norm": 7.57320499420166,
      "learning_rate": 4.081978319783198e-05,
      "loss": 5.6534,
      "step": 135500
    },
    {
      "epoch": 184.28184281842817,
      "grad_norm": 9.93310260772705,
      "learning_rate": 4.0785907859078594e-05,
      "loss": 5.704,
      "step": 136000
    },
    {
      "epoch": 184.95934959349594,
      "grad_norm": 6.780989170074463,
      "learning_rate": 4.0752032520325204e-05,
      "loss": 5.6179,
      "step": 136500
    },
    {
      "epoch": 185.63685636856368,
      "grad_norm": 12.014443397521973,
      "learning_rate": 4.071815718157182e-05,
      "loss": 5.6593,
      "step": 137000
    },
    {
      "epoch": 186.31436314363143,
      "grad_norm": 9.791462898254395,
      "learning_rate": 4.068428184281843e-05,
      "loss": 5.6654,
      "step": 137500
    },
    {
      "epoch": 186.9918699186992,
      "grad_norm": 11.05911636352539,
      "learning_rate": 4.065040650406504e-05,
      "loss": 5.6482,
      "step": 138000
    },
    {
      "epoch": 187.66937669376694,
      "grad_norm": 8.499839782714844,
      "learning_rate": 4.061653116531166e-05,
      "loss": 5.6758,
      "step": 138500
    },
    {
      "epoch": 188.34688346883468,
      "grad_norm": 7.396834850311279,
      "learning_rate": 4.058265582655827e-05,
      "loss": 5.6485,
      "step": 139000
    },
    {
      "epoch": 189.02439024390245,
      "grad_norm": 11.54627513885498,
      "learning_rate": 4.0548780487804884e-05,
      "loss": 5.6542,
      "step": 139500
    },
    {
      "epoch": 189.7018970189702,
      "grad_norm": 9.54122257232666,
      "learning_rate": 4.051490514905149e-05,
      "loss": 5.6452,
      "step": 140000
    },
    {
      "epoch": 190.37940379403793,
      "grad_norm": 8.34105110168457,
      "learning_rate": 4.0481029810298104e-05,
      "loss": 5.6446,
      "step": 140500
    },
    {
      "epoch": 191.0569105691057,
      "grad_norm": 6.819817543029785,
      "learning_rate": 4.044715447154472e-05,
      "loss": 5.6739,
      "step": 141000
    },
    {
      "epoch": 191.73441734417344,
      "grad_norm": 15.750468254089355,
      "learning_rate": 4.041327913279133e-05,
      "loss": 5.6413,
      "step": 141500
    },
    {
      "epoch": 192.4119241192412,
      "grad_norm": 9.076817512512207,
      "learning_rate": 4.037940379403794e-05,
      "loss": 5.6487,
      "step": 142000
    },
    {
      "epoch": 193.08943089430895,
      "grad_norm": 8.491498947143555,
      "learning_rate": 4.034552845528455e-05,
      "loss": 5.6549,
      "step": 142500
    },
    {
      "epoch": 193.7669376693767,
      "grad_norm": 8.075066566467285,
      "learning_rate": 4.031165311653117e-05,
      "loss": 5.6699,
      "step": 143000
    },
    {
      "epoch": 194.44444444444446,
      "grad_norm": 10.725753784179688,
      "learning_rate": 4.027777777777778e-05,
      "loss": 5.6544,
      "step": 143500
    },
    {
      "epoch": 195.1219512195122,
      "grad_norm": 6.903799533843994,
      "learning_rate": 4.0243902439024395e-05,
      "loss": 5.6535,
      "step": 144000
    },
    {
      "epoch": 195.79945799457994,
      "grad_norm": 9.128844261169434,
      "learning_rate": 4.0210027100271005e-05,
      "loss": 5.6779,
      "step": 144500
    },
    {
      "epoch": 196.4769647696477,
      "grad_norm": 8.367902755737305,
      "learning_rate": 4.0176151761517615e-05,
      "loss": 5.627,
      "step": 145000
    },
    {
      "epoch": 197.15447154471545,
      "grad_norm": 9.316651344299316,
      "learning_rate": 4.014227642276423e-05,
      "loss": 5.647,
      "step": 145500
    },
    {
      "epoch": 197.8319783197832,
      "grad_norm": 10.669677734375,
      "learning_rate": 4.010840108401084e-05,
      "loss": 5.6503,
      "step": 146000
    },
    {
      "epoch": 198.50948509485096,
      "grad_norm": 7.944241046905518,
      "learning_rate": 4.007452574525745e-05,
      "loss": 5.6784,
      "step": 146500
    },
    {
      "epoch": 199.1869918699187,
      "grad_norm": 7.175065517425537,
      "learning_rate": 4.004065040650407e-05,
      "loss": 5.6314,
      "step": 147000
    },
    {
      "epoch": 199.86449864498644,
      "grad_norm": 11.005393981933594,
      "learning_rate": 4.000677506775068e-05,
      "loss": 5.656,
      "step": 147500
    },
    {
      "epoch": 200.5420054200542,
      "grad_norm": 10.021561622619629,
      "learning_rate": 3.9972899728997295e-05,
      "loss": 5.6573,
      "step": 148000
    },
    {
      "epoch": 201.21951219512195,
      "grad_norm": 11.198566436767578,
      "learning_rate": 3.9939024390243905e-05,
      "loss": 5.6437,
      "step": 148500
    },
    {
      "epoch": 201.8970189701897,
      "grad_norm": 14.999792098999023,
      "learning_rate": 3.9905149051490515e-05,
      "loss": 5.6599,
      "step": 149000
    },
    {
      "epoch": 202.57452574525746,
      "grad_norm": 8.5000638961792,
      "learning_rate": 3.9871273712737125e-05,
      "loss": 5.6694,
      "step": 149500
    },
    {
      "epoch": 203.2520325203252,
      "grad_norm": 12.03326416015625,
      "learning_rate": 3.983739837398374e-05,
      "loss": 5.6339,
      "step": 150000
    },
    {
      "epoch": 203.92953929539294,
      "grad_norm": 8.606671333312988,
      "learning_rate": 3.980352303523035e-05,
      "loss": 5.6621,
      "step": 150500
    },
    {
      "epoch": 204.6070460704607,
      "grad_norm": 9.03147029876709,
      "learning_rate": 3.976964769647697e-05,
      "loss": 5.6437,
      "step": 151000
    },
    {
      "epoch": 205.28455284552845,
      "grad_norm": 8.310561180114746,
      "learning_rate": 3.973577235772358e-05,
      "loss": 5.6406,
      "step": 151500
    },
    {
      "epoch": 205.9620596205962,
      "grad_norm": 11.409261703491211,
      "learning_rate": 3.970189701897019e-05,
      "loss": 5.6758,
      "step": 152000
    },
    {
      "epoch": 206.63956639566396,
      "grad_norm": 10.863382339477539,
      "learning_rate": 3.9668021680216806e-05,
      "loss": 5.6323,
      "step": 152500
    },
    {
      "epoch": 207.3170731707317,
      "grad_norm": 7.978518486022949,
      "learning_rate": 3.9634146341463416e-05,
      "loss": 5.6937,
      "step": 153000
    },
    {
      "epoch": 207.99457994579944,
      "grad_norm": 9.219703674316406,
      "learning_rate": 3.9600271002710026e-05,
      "loss": 5.6548,
      "step": 153500
    },
    {
      "epoch": 208.6720867208672,
      "grad_norm": 8.745495796203613,
      "learning_rate": 3.956639566395664e-05,
      "loss": 5.6468,
      "step": 154000
    },
    {
      "epoch": 209.34959349593495,
      "grad_norm": 8.665778160095215,
      "learning_rate": 3.953252032520325e-05,
      "loss": 5.6554,
      "step": 154500
    },
    {
      "epoch": 210.02710027100272,
      "grad_norm": 5.874342918395996,
      "learning_rate": 3.949864498644987e-05,
      "loss": 5.6384,
      "step": 155000
    },
    {
      "epoch": 210.70460704607046,
      "grad_norm": 8.319355010986328,
      "learning_rate": 3.946476964769648e-05,
      "loss": 5.6451,
      "step": 155500
    },
    {
      "epoch": 211.3821138211382,
      "grad_norm": 11.407994270324707,
      "learning_rate": 3.943089430894309e-05,
      "loss": 5.6659,
      "step": 156000
    },
    {
      "epoch": 212.05962059620597,
      "grad_norm": 8.689823150634766,
      "learning_rate": 3.93970189701897e-05,
      "loss": 5.6514,
      "step": 156500
    },
    {
      "epoch": 212.73712737127371,
      "grad_norm": 14.360003471374512,
      "learning_rate": 3.9363143631436316e-05,
      "loss": 5.6562,
      "step": 157000
    },
    {
      "epoch": 213.41463414634146,
      "grad_norm": 12.288654327392578,
      "learning_rate": 3.932926829268293e-05,
      "loss": 5.6515,
      "step": 157500
    },
    {
      "epoch": 214.09214092140923,
      "grad_norm": 11.783580780029297,
      "learning_rate": 3.9295392953929537e-05,
      "loss": 5.6496,
      "step": 158000
    },
    {
      "epoch": 214.76964769647697,
      "grad_norm": 6.4767255783081055,
      "learning_rate": 3.926151761517615e-05,
      "loss": 5.6512,
      "step": 158500
    },
    {
      "epoch": 215.4471544715447,
      "grad_norm": 9.947468757629395,
      "learning_rate": 3.922764227642276e-05,
      "loss": 5.6097,
      "step": 159000
    },
    {
      "epoch": 216.12466124661248,
      "grad_norm": 11.325965881347656,
      "learning_rate": 3.919376693766938e-05,
      "loss": 5.6951,
      "step": 159500
    },
    {
      "epoch": 216.80216802168022,
      "grad_norm": 9.506473541259766,
      "learning_rate": 3.915989159891599e-05,
      "loss": 5.6343,
      "step": 160000
    },
    {
      "epoch": 217.47967479674796,
      "grad_norm": 6.306151866912842,
      "learning_rate": 3.91260162601626e-05,
      "loss": 5.6673,
      "step": 160500
    },
    {
      "epoch": 218.15718157181573,
      "grad_norm": 11.059097290039062,
      "learning_rate": 3.909214092140922e-05,
      "loss": 5.6666,
      "step": 161000
    },
    {
      "epoch": 218.83468834688347,
      "grad_norm": 8.186347961425781,
      "learning_rate": 3.905826558265583e-05,
      "loss": 5.6451,
      "step": 161500
    },
    {
      "epoch": 219.5121951219512,
      "grad_norm": 13.263635635375977,
      "learning_rate": 3.9024390243902444e-05,
      "loss": 5.661,
      "step": 162000
    },
    {
      "epoch": 220.18970189701898,
      "grad_norm": 8.844897270202637,
      "learning_rate": 3.8990514905149054e-05,
      "loss": 5.6398,
      "step": 162500
    },
    {
      "epoch": 220.86720867208672,
      "grad_norm": 8.628641128540039,
      "learning_rate": 3.8956639566395664e-05,
      "loss": 5.6352,
      "step": 163000
    },
    {
      "epoch": 221.54471544715446,
      "grad_norm": 10.12667179107666,
      "learning_rate": 3.892276422764228e-05,
      "loss": 5.6349,
      "step": 163500
    },
    {
      "epoch": 222.22222222222223,
      "grad_norm": 7.523834705352783,
      "learning_rate": 3.888888888888889e-05,
      "loss": 5.6931,
      "step": 164000
    },
    {
      "epoch": 222.89972899728997,
      "grad_norm": 8.079325675964355,
      "learning_rate": 3.885501355013551e-05,
      "loss": 5.6395,
      "step": 164500
    },
    {
      "epoch": 223.5772357723577,
      "grad_norm": 9.81254768371582,
      "learning_rate": 3.882113821138211e-05,
      "loss": 5.6883,
      "step": 165000
    },
    {
      "epoch": 224.25474254742548,
      "grad_norm": 8.419825553894043,
      "learning_rate": 3.878726287262873e-05,
      "loss": 5.6472,
      "step": 165500
    },
    {
      "epoch": 224.93224932249322,
      "grad_norm": 13.789162635803223,
      "learning_rate": 3.875338753387534e-05,
      "loss": 5.6378,
      "step": 166000
    },
    {
      "epoch": 225.609756097561,
      "grad_norm": 8.11507797241211,
      "learning_rate": 3.8719512195121954e-05,
      "loss": 5.652,
      "step": 166500
    },
    {
      "epoch": 226.28726287262873,
      "grad_norm": 7.329874515533447,
      "learning_rate": 3.8685636856368564e-05,
      "loss": 5.6791,
      "step": 167000
    },
    {
      "epoch": 226.96476964769647,
      "grad_norm": 11.427009582519531,
      "learning_rate": 3.8651761517615174e-05,
      "loss": 5.6205,
      "step": 167500
    },
    {
      "epoch": 227.64227642276424,
      "grad_norm": 7.879823207855225,
      "learning_rate": 3.861788617886179e-05,
      "loss": 5.619,
      "step": 168000
    },
    {
      "epoch": 228.31978319783198,
      "grad_norm": 12.941329002380371,
      "learning_rate": 3.85840108401084e-05,
      "loss": 5.672,
      "step": 168500
    },
    {
      "epoch": 228.99728997289972,
      "grad_norm": 10.653696060180664,
      "learning_rate": 3.855013550135502e-05,
      "loss": 5.654,
      "step": 169000
    },
    {
      "epoch": 229.6747967479675,
      "grad_norm": 8.766142845153809,
      "learning_rate": 3.851626016260163e-05,
      "loss": 5.6443,
      "step": 169500
    },
    {
      "epoch": 230.35230352303523,
      "grad_norm": 7.289042949676514,
      "learning_rate": 3.848238482384824e-05,
      "loss": 5.6524,
      "step": 170000
    },
    {
      "epoch": 231.02981029810297,
      "grad_norm": 9.484582901000977,
      "learning_rate": 3.8448509485094855e-05,
      "loss": 5.6491,
      "step": 170500
    },
    {
      "epoch": 231.70731707317074,
      "grad_norm": 11.203863143920898,
      "learning_rate": 3.8414634146341465e-05,
      "loss": 5.6457,
      "step": 171000
    },
    {
      "epoch": 232.38482384823848,
      "grad_norm": 6.91785192489624,
      "learning_rate": 3.838075880758808e-05,
      "loss": 5.6363,
      "step": 171500
    },
    {
      "epoch": 233.06233062330622,
      "grad_norm": 8.789227485656738,
      "learning_rate": 3.8346883468834685e-05,
      "loss": 5.6626,
      "step": 172000
    },
    {
      "epoch": 233.739837398374,
      "grad_norm": 8.564908981323242,
      "learning_rate": 3.83130081300813e-05,
      "loss": 5.6667,
      "step": 172500
    },
    {
      "epoch": 234.41734417344173,
      "grad_norm": 6.6852707862854,
      "learning_rate": 3.827913279132791e-05,
      "loss": 5.6319,
      "step": 173000
    },
    {
      "epoch": 235.09485094850947,
      "grad_norm": 15.418218612670898,
      "learning_rate": 3.824525745257453e-05,
      "loss": 5.6361,
      "step": 173500
    },
    {
      "epoch": 235.77235772357724,
      "grad_norm": 7.55927038192749,
      "learning_rate": 3.8211382113821145e-05,
      "loss": 5.6453,
      "step": 174000
    },
    {
      "epoch": 236.44986449864498,
      "grad_norm": 7.34635591506958,
      "learning_rate": 3.817750677506775e-05,
      "loss": 5.6315,
      "step": 174500
    },
    {
      "epoch": 237.12737127371273,
      "grad_norm": 7.687874794006348,
      "learning_rate": 3.8143631436314366e-05,
      "loss": 5.6566,
      "step": 175000
    },
    {
      "epoch": 237.8048780487805,
      "grad_norm": 7.580517292022705,
      "learning_rate": 3.8109756097560976e-05,
      "loss": 5.6403,
      "step": 175500
    },
    {
      "epoch": 238.48238482384824,
      "grad_norm": 21.376815795898438,
      "learning_rate": 3.807588075880759e-05,
      "loss": 5.6667,
      "step": 176000
    },
    {
      "epoch": 239.15989159891598,
      "grad_norm": 6.582630634307861,
      "learning_rate": 3.80420054200542e-05,
      "loss": 5.6487,
      "step": 176500
    },
    {
      "epoch": 239.83739837398375,
      "grad_norm": 10.589590072631836,
      "learning_rate": 3.800813008130081e-05,
      "loss": 5.6495,
      "step": 177000
    },
    {
      "epoch": 240.5149051490515,
      "grad_norm": 8.026455879211426,
      "learning_rate": 3.797425474254743e-05,
      "loss": 5.6245,
      "step": 177500
    },
    {
      "epoch": 241.19241192411923,
      "grad_norm": 11.354104995727539,
      "learning_rate": 3.794037940379404e-05,
      "loss": 5.6711,
      "step": 178000
    },
    {
      "epoch": 241.869918699187,
      "grad_norm": 8.525466918945312,
      "learning_rate": 3.7906504065040656e-05,
      "loss": 5.6458,
      "step": 178500
    },
    {
      "epoch": 242.54742547425474,
      "grad_norm": 11.042203903198242,
      "learning_rate": 3.787262872628726e-05,
      "loss": 5.6454,
      "step": 179000
    },
    {
      "epoch": 243.2249322493225,
      "grad_norm": 6.108914375305176,
      "learning_rate": 3.7838753387533876e-05,
      "loss": 5.6552,
      "step": 179500
    },
    {
      "epoch": 243.90243902439025,
      "grad_norm": 9.15170955657959,
      "learning_rate": 3.780487804878049e-05,
      "loss": 5.6325,
      "step": 180000
    },
    {
      "epoch": 244.579945799458,
      "grad_norm": 9.118218421936035,
      "learning_rate": 3.77710027100271e-05,
      "loss": 5.6671,
      "step": 180500
    },
    {
      "epoch": 245.25745257452576,
      "grad_norm": 7.162714004516602,
      "learning_rate": 3.773712737127372e-05,
      "loss": 5.6436,
      "step": 181000
    },
    {
      "epoch": 245.9349593495935,
      "grad_norm": 14.006855010986328,
      "learning_rate": 3.770325203252032e-05,
      "loss": 5.6345,
      "step": 181500
    },
    {
      "epoch": 246.61246612466124,
      "grad_norm": 9.820459365844727,
      "learning_rate": 3.766937669376694e-05,
      "loss": 5.6191,
      "step": 182000
    },
    {
      "epoch": 247.289972899729,
      "grad_norm": 7.360935688018799,
      "learning_rate": 3.763550135501355e-05,
      "loss": 5.6888,
      "step": 182500
    },
    {
      "epoch": 247.96747967479675,
      "grad_norm": 10.865755081176758,
      "learning_rate": 3.760162601626017e-05,
      "loss": 5.6179,
      "step": 183000
    },
    {
      "epoch": 248.6449864498645,
      "grad_norm": 9.121543884277344,
      "learning_rate": 3.756775067750678e-05,
      "loss": 5.6372,
      "step": 183500
    },
    {
      "epoch": 249.32249322493226,
      "grad_norm": 10.905919075012207,
      "learning_rate": 3.753387533875339e-05,
      "loss": 5.6463,
      "step": 184000
    },
    {
      "epoch": 250.0,
      "grad_norm": 7.210629463195801,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 5.6479,
      "step": 184500
    },
    {
      "epoch": 250.67750677506774,
      "grad_norm": 7.172852993011475,
      "learning_rate": 3.7466124661246614e-05,
      "loss": 5.6634,
      "step": 185000
    },
    {
      "epoch": 251.3550135501355,
      "grad_norm": 8.932280540466309,
      "learning_rate": 3.743224932249323e-05,
      "loss": 5.6469,
      "step": 185500
    },
    {
      "epoch": 252.03252032520325,
      "grad_norm": 9.475967407226562,
      "learning_rate": 3.739837398373984e-05,
      "loss": 5.6372,
      "step": 186000
    },
    {
      "epoch": 252.710027100271,
      "grad_norm": 8.934803009033203,
      "learning_rate": 3.736449864498645e-05,
      "loss": 5.6115,
      "step": 186500
    },
    {
      "epoch": 253.38753387533876,
      "grad_norm": 10.826531410217285,
      "learning_rate": 3.733062330623307e-05,
      "loss": 5.6811,
      "step": 187000
    },
    {
      "epoch": 254.0650406504065,
      "grad_norm": 6.963500022888184,
      "learning_rate": 3.729674796747968e-05,
      "loss": 5.6258,
      "step": 187500
    },
    {
      "epoch": 254.74254742547424,
      "grad_norm": 8.574974060058594,
      "learning_rate": 3.726287262872629e-05,
      "loss": 5.6465,
      "step": 188000
    },
    {
      "epoch": 255.420054200542,
      "grad_norm": 7.295594215393066,
      "learning_rate": 3.72289972899729e-05,
      "loss": 5.6509,
      "step": 188500
    },
    {
      "epoch": 256.0975609756098,
      "grad_norm": 9.164753913879395,
      "learning_rate": 3.7195121951219514e-05,
      "loss": 5.6258,
      "step": 189000
    },
    {
      "epoch": 256.7750677506775,
      "grad_norm": 8.523214340209961,
      "learning_rate": 3.7161246612466124e-05,
      "loss": 5.6235,
      "step": 189500
    },
    {
      "epoch": 257.45257452574526,
      "grad_norm": 8.026851654052734,
      "learning_rate": 3.712737127371274e-05,
      "loss": 5.6746,
      "step": 190000
    },
    {
      "epoch": 258.130081300813,
      "grad_norm": 7.911179065704346,
      "learning_rate": 3.709349593495935e-05,
      "loss": 5.6463,
      "step": 190500
    },
    {
      "epoch": 258.80758807588074,
      "grad_norm": 8.934882164001465,
      "learning_rate": 3.705962059620596e-05,
      "loss": 5.633,
      "step": 191000
    },
    {
      "epoch": 259.4850948509485,
      "grad_norm": 8.663004875183105,
      "learning_rate": 3.702574525745258e-05,
      "loss": 5.6615,
      "step": 191500
    },
    {
      "epoch": 260.1626016260163,
      "grad_norm": 10.49855899810791,
      "learning_rate": 3.699186991869919e-05,
      "loss": 5.6469,
      "step": 192000
    },
    {
      "epoch": 260.840108401084,
      "grad_norm": 10.633378982543945,
      "learning_rate": 3.6957994579945805e-05,
      "loss": 5.6505,
      "step": 192500
    },
    {
      "epoch": 261.51761517615176,
      "grad_norm": 7.790566921234131,
      "learning_rate": 3.6924119241192415e-05,
      "loss": 5.6356,
      "step": 193000
    },
    {
      "epoch": 262.1951219512195,
      "grad_norm": 8.197284698486328,
      "learning_rate": 3.6890243902439025e-05,
      "loss": 5.6502,
      "step": 193500
    },
    {
      "epoch": 262.87262872628725,
      "grad_norm": 7.55950927734375,
      "learning_rate": 3.685636856368564e-05,
      "loss": 5.639,
      "step": 194000
    },
    {
      "epoch": 263.550135501355,
      "grad_norm": 8.10146713256836,
      "learning_rate": 3.682249322493225e-05,
      "loss": 5.6498,
      "step": 194500
    },
    {
      "epoch": 264.2276422764228,
      "grad_norm": 8.515738487243652,
      "learning_rate": 3.678861788617886e-05,
      "loss": 5.6642,
      "step": 195000
    },
    {
      "epoch": 264.9051490514905,
      "grad_norm": 9.482948303222656,
      "learning_rate": 3.675474254742547e-05,
      "loss": 5.6172,
      "step": 195500
    },
    {
      "epoch": 265.58265582655827,
      "grad_norm": 6.859283447265625,
      "learning_rate": 3.672086720867209e-05,
      "loss": 5.6578,
      "step": 196000
    },
    {
      "epoch": 266.260162601626,
      "grad_norm": 12.24489974975586,
      "learning_rate": 3.6686991869918705e-05,
      "loss": 5.6246,
      "step": 196500
    },
    {
      "epoch": 266.93766937669375,
      "grad_norm": 9.491538047790527,
      "learning_rate": 3.6653116531165315e-05,
      "loss": 5.6504,
      "step": 197000
    },
    {
      "epoch": 267.61517615176155,
      "grad_norm": 9.953487396240234,
      "learning_rate": 3.6619241192411925e-05,
      "loss": 5.6559,
      "step": 197500
    },
    {
      "epoch": 268.2926829268293,
      "grad_norm": 9.916776657104492,
      "learning_rate": 3.6585365853658535e-05,
      "loss": 5.6371,
      "step": 198000
    },
    {
      "epoch": 268.970189701897,
      "grad_norm": 8.83138656616211,
      "learning_rate": 3.655149051490515e-05,
      "loss": 5.6377,
      "step": 198500
    },
    {
      "epoch": 269.64769647696477,
      "grad_norm": 8.935249328613281,
      "learning_rate": 3.651761517615176e-05,
      "loss": 5.6355,
      "step": 199000
    },
    {
      "epoch": 270.3252032520325,
      "grad_norm": 13.528648376464844,
      "learning_rate": 3.648373983739837e-05,
      "loss": 5.6482,
      "step": 199500
    },
    {
      "epoch": 271.00271002710025,
      "grad_norm": 15.816442489624023,
      "learning_rate": 3.644986449864499e-05,
      "loss": 5.6583,
      "step": 200000
    },
    {
      "epoch": 271.68021680216805,
      "grad_norm": 7.723995685577393,
      "learning_rate": 3.64159891598916e-05,
      "loss": 5.6204,
      "step": 200500
    },
    {
      "epoch": 272.3577235772358,
      "grad_norm": 7.0365071296691895,
      "learning_rate": 3.6382113821138216e-05,
      "loss": 5.636,
      "step": 201000
    },
    {
      "epoch": 273.03523035230353,
      "grad_norm": 8.030179977416992,
      "learning_rate": 3.6348238482384826e-05,
      "loss": 5.6637,
      "step": 201500
    },
    {
      "epoch": 273.71273712737127,
      "grad_norm": 5.488485336303711,
      "learning_rate": 3.6314363143631436e-05,
      "loss": 5.6323,
      "step": 202000
    },
    {
      "epoch": 274.390243902439,
      "grad_norm": 10.184182167053223,
      "learning_rate": 3.628048780487805e-05,
      "loss": 5.6544,
      "step": 202500
    },
    {
      "epoch": 275.06775067750675,
      "grad_norm": 11.952322006225586,
      "learning_rate": 3.624661246612466e-05,
      "loss": 5.6358,
      "step": 203000
    },
    {
      "epoch": 275.74525745257455,
      "grad_norm": 11.470171928405762,
      "learning_rate": 3.621273712737128e-05,
      "loss": 5.6406,
      "step": 203500
    },
    {
      "epoch": 276.4227642276423,
      "grad_norm": 7.208085060119629,
      "learning_rate": 3.617886178861789e-05,
      "loss": 5.6567,
      "step": 204000
    },
    {
      "epoch": 277.10027100271003,
      "grad_norm": 15.083871841430664,
      "learning_rate": 3.61449864498645e-05,
      "loss": 5.6287,
      "step": 204500
    },
    {
      "epoch": 277.77777777777777,
      "grad_norm": 13.283829689025879,
      "learning_rate": 3.611111111111111e-05,
      "loss": 5.6675,
      "step": 205000
    },
    {
      "epoch": 278.4552845528455,
      "grad_norm": 8.447561264038086,
      "learning_rate": 3.6077235772357726e-05,
      "loss": 5.6419,
      "step": 205500
    },
    {
      "epoch": 279.13279132791325,
      "grad_norm": 8.673116683959961,
      "learning_rate": 3.6043360433604336e-05,
      "loss": 5.6225,
      "step": 206000
    },
    {
      "epoch": 279.81029810298105,
      "grad_norm": 8.276495933532715,
      "learning_rate": 3.6009485094850946e-05,
      "loss": 5.6273,
      "step": 206500
    },
    {
      "epoch": 280.4878048780488,
      "grad_norm": 22.931875228881836,
      "learning_rate": 3.597560975609756e-05,
      "loss": 5.6311,
      "step": 207000
    },
    {
      "epoch": 281.16531165311653,
      "grad_norm": 5.805251121520996,
      "learning_rate": 3.594173441734417e-05,
      "loss": 5.6582,
      "step": 207500
    },
    {
      "epoch": 281.8428184281843,
      "grad_norm": 7.636198997497559,
      "learning_rate": 3.590785907859079e-05,
      "loss": 5.6428,
      "step": 208000
    },
    {
      "epoch": 282.520325203252,
      "grad_norm": 7.073180675506592,
      "learning_rate": 3.58739837398374e-05,
      "loss": 5.6286,
      "step": 208500
    },
    {
      "epoch": 283.19783197831975,
      "grad_norm": 9.140508651733398,
      "learning_rate": 3.584010840108401e-05,
      "loss": 5.6443,
      "step": 209000
    },
    {
      "epoch": 283.87533875338755,
      "grad_norm": 8.637862205505371,
      "learning_rate": 3.580623306233063e-05,
      "loss": 5.6502,
      "step": 209500
    },
    {
      "epoch": 284.5528455284553,
      "grad_norm": 7.539974689483643,
      "learning_rate": 3.577235772357724e-05,
      "loss": 5.6159,
      "step": 210000
    },
    {
      "epoch": 285.23035230352303,
      "grad_norm": 8.314375877380371,
      "learning_rate": 3.5738482384823854e-05,
      "loss": 5.6294,
      "step": 210500
    },
    {
      "epoch": 285.9078590785908,
      "grad_norm": 10.356468200683594,
      "learning_rate": 3.5704607046070464e-05,
      "loss": 5.6493,
      "step": 211000
    },
    {
      "epoch": 286.5853658536585,
      "grad_norm": 7.9699578285217285,
      "learning_rate": 3.5670731707317074e-05,
      "loss": 5.6586,
      "step": 211500
    },
    {
      "epoch": 287.2628726287263,
      "grad_norm": 9.322332382202148,
      "learning_rate": 3.5636856368563684e-05,
      "loss": 5.6228,
      "step": 212000
    },
    {
      "epoch": 287.94037940379405,
      "grad_norm": 11.088629722595215,
      "learning_rate": 3.56029810298103e-05,
      "loss": 5.6556,
      "step": 212500
    },
    {
      "epoch": 288.6178861788618,
      "grad_norm": 10.046131134033203,
      "learning_rate": 3.556910569105692e-05,
      "loss": 5.6514,
      "step": 213000
    },
    {
      "epoch": 289.29539295392954,
      "grad_norm": 11.453797340393066,
      "learning_rate": 3.553523035230352e-05,
      "loss": 5.613,
      "step": 213500
    },
    {
      "epoch": 289.9728997289973,
      "grad_norm": 19.865585327148438,
      "learning_rate": 3.550135501355014e-05,
      "loss": 5.6413,
      "step": 214000
    },
    {
      "epoch": 290.650406504065,
      "grad_norm": 12.953450202941895,
      "learning_rate": 3.546747967479675e-05,
      "loss": 5.6723,
      "step": 214500
    },
    {
      "epoch": 291.3279132791328,
      "grad_norm": 9.453527450561523,
      "learning_rate": 3.5433604336043364e-05,
      "loss": 5.6318,
      "step": 215000
    },
    {
      "epoch": 292.00542005420056,
      "grad_norm": 9.448775291442871,
      "learning_rate": 3.5399728997289974e-05,
      "loss": 5.6297,
      "step": 215500
    },
    {
      "epoch": 292.6829268292683,
      "grad_norm": 8.053380012512207,
      "learning_rate": 3.5365853658536584e-05,
      "loss": 5.6641,
      "step": 216000
    },
    {
      "epoch": 293.36043360433604,
      "grad_norm": 12.836420059204102,
      "learning_rate": 3.53319783197832e-05,
      "loss": 5.5644,
      "step": 216500
    },
    {
      "epoch": 294.0379403794038,
      "grad_norm": 8.850604057312012,
      "learning_rate": 3.529810298102981e-05,
      "loss": 5.7038,
      "step": 217000
    },
    {
      "epoch": 294.7154471544715,
      "grad_norm": 8.01876163482666,
      "learning_rate": 3.526422764227643e-05,
      "loss": 5.6337,
      "step": 217500
    },
    {
      "epoch": 295.3929539295393,
      "grad_norm": 8.343851089477539,
      "learning_rate": 3.523035230352303e-05,
      "loss": 5.6241,
      "step": 218000
    },
    {
      "epoch": 296.07046070460706,
      "grad_norm": 9.858091354370117,
      "learning_rate": 3.519647696476965e-05,
      "loss": 5.6482,
      "step": 218500
    },
    {
      "epoch": 296.7479674796748,
      "grad_norm": 16.99285316467285,
      "learning_rate": 3.5162601626016265e-05,
      "loss": 5.6396,
      "step": 219000
    },
    {
      "epoch": 297.42547425474254,
      "grad_norm": 7.603085994720459,
      "learning_rate": 3.5128726287262875e-05,
      "loss": 5.636,
      "step": 219500
    },
    {
      "epoch": 298.1029810298103,
      "grad_norm": 7.64933967590332,
      "learning_rate": 3.509485094850949e-05,
      "loss": 5.6192,
      "step": 220000
    },
    {
      "epoch": 298.780487804878,
      "grad_norm": 8.70661735534668,
      "learning_rate": 3.5060975609756095e-05,
      "loss": 5.6611,
      "step": 220500
    },
    {
      "epoch": 299.4579945799458,
      "grad_norm": 8.5040922164917,
      "learning_rate": 3.502710027100271e-05,
      "loss": 5.6302,
      "step": 221000
    },
    {
      "epoch": 300.13550135501356,
      "grad_norm": 8.569077491760254,
      "learning_rate": 3.499322493224932e-05,
      "loss": 5.6538,
      "step": 221500
    },
    {
      "epoch": 300.8130081300813,
      "grad_norm": 7.392378330230713,
      "learning_rate": 3.495934959349594e-05,
      "loss": 5.6441,
      "step": 222000
    },
    {
      "epoch": 301.49051490514904,
      "grad_norm": 9.2504301071167,
      "learning_rate": 3.492547425474255e-05,
      "loss": 5.6374,
      "step": 222500
    },
    {
      "epoch": 302.1680216802168,
      "grad_norm": 8.54338264465332,
      "learning_rate": 3.489159891598916e-05,
      "loss": 5.6412,
      "step": 223000
    },
    {
      "epoch": 302.8455284552846,
      "grad_norm": 10.593342781066895,
      "learning_rate": 3.4857723577235775e-05,
      "loss": 5.6473,
      "step": 223500
    },
    {
      "epoch": 303.5230352303523,
      "grad_norm": 9.018580436706543,
      "learning_rate": 3.4823848238482385e-05,
      "loss": 5.6206,
      "step": 224000
    },
    {
      "epoch": 304.20054200542006,
      "grad_norm": 7.166301727294922,
      "learning_rate": 3.4789972899729e-05,
      "loss": 5.6429,
      "step": 224500
    },
    {
      "epoch": 304.8780487804878,
      "grad_norm": 13.861719131469727,
      "learning_rate": 3.475609756097561e-05,
      "loss": 5.655,
      "step": 225000
    },
    {
      "epoch": 305.55555555555554,
      "grad_norm": 15.176480293273926,
      "learning_rate": 3.472222222222222e-05,
      "loss": 5.6277,
      "step": 225500
    },
    {
      "epoch": 306.2330623306233,
      "grad_norm": 7.312662601470947,
      "learning_rate": 3.468834688346884e-05,
      "loss": 5.6253,
      "step": 226000
    },
    {
      "epoch": 306.9105691056911,
      "grad_norm": 9.807488441467285,
      "learning_rate": 3.465447154471545e-05,
      "loss": 5.6385,
      "step": 226500
    },
    {
      "epoch": 307.5880758807588,
      "grad_norm": 7.384364604949951,
      "learning_rate": 3.4620596205962066e-05,
      "loss": 5.6691,
      "step": 227000
    },
    {
      "epoch": 308.26558265582656,
      "grad_norm": 7.606192111968994,
      "learning_rate": 3.458672086720867e-05,
      "loss": 5.6301,
      "step": 227500
    },
    {
      "epoch": 308.9430894308943,
      "grad_norm": 7.418758392333984,
      "learning_rate": 3.4552845528455286e-05,
      "loss": 5.6331,
      "step": 228000
    },
    {
      "epoch": 309.62059620596204,
      "grad_norm": 9.298274993896484,
      "learning_rate": 3.4518970189701896e-05,
      "loss": 5.6579,
      "step": 228500
    },
    {
      "epoch": 310.2981029810298,
      "grad_norm": 5.825813293457031,
      "learning_rate": 3.448509485094851e-05,
      "loss": 5.6071,
      "step": 229000
    },
    {
      "epoch": 310.9756097560976,
      "grad_norm": 10.774868965148926,
      "learning_rate": 3.445121951219512e-05,
      "loss": 5.6654,
      "step": 229500
    },
    {
      "epoch": 311.6531165311653,
      "grad_norm": 12.535643577575684,
      "learning_rate": 3.441734417344173e-05,
      "loss": 5.6205,
      "step": 230000
    },
    {
      "epoch": 312.33062330623306,
      "grad_norm": 10.739433288574219,
      "learning_rate": 3.438346883468835e-05,
      "loss": 5.6377,
      "step": 230500
    },
    {
      "epoch": 313.0081300813008,
      "grad_norm": 7.974839210510254,
      "learning_rate": 3.434959349593496e-05,
      "loss": 5.65,
      "step": 231000
    },
    {
      "epoch": 313.68563685636855,
      "grad_norm": 22.491466522216797,
      "learning_rate": 3.4315718157181576e-05,
      "loss": 5.6226,
      "step": 231500
    },
    {
      "epoch": 314.3631436314363,
      "grad_norm": 7.048492908477783,
      "learning_rate": 3.4281842818428186e-05,
      "loss": 5.6296,
      "step": 232000
    },
    {
      "epoch": 315.0406504065041,
      "grad_norm": 8.479637145996094,
      "learning_rate": 3.4247967479674796e-05,
      "loss": 5.6526,
      "step": 232500
    },
    {
      "epoch": 315.7181571815718,
      "grad_norm": 9.428337097167969,
      "learning_rate": 3.421409214092141e-05,
      "loss": 5.6214,
      "step": 233000
    },
    {
      "epoch": 316.39566395663957,
      "grad_norm": 7.254103660583496,
      "learning_rate": 3.418021680216802e-05,
      "loss": 5.6373,
      "step": 233500
    },
    {
      "epoch": 317.0731707317073,
      "grad_norm": 10.298555374145508,
      "learning_rate": 3.414634146341464e-05,
      "loss": 5.6503,
      "step": 234000
    },
    {
      "epoch": 317.75067750677505,
      "grad_norm": 9.144974708557129,
      "learning_rate": 3.411246612466124e-05,
      "loss": 5.6273,
      "step": 234500
    },
    {
      "epoch": 318.42818428184285,
      "grad_norm": 7.277905464172363,
      "learning_rate": 3.407859078590786e-05,
      "loss": 5.6228,
      "step": 235000
    },
    {
      "epoch": 319.1056910569106,
      "grad_norm": 10.316221237182617,
      "learning_rate": 3.404471544715448e-05,
      "loss": 5.6544,
      "step": 235500
    },
    {
      "epoch": 319.7831978319783,
      "grad_norm": 9.275721549987793,
      "learning_rate": 3.401084010840109e-05,
      "loss": 5.6346,
      "step": 236000
    },
    {
      "epoch": 320.46070460704607,
      "grad_norm": 9.33399772644043,
      "learning_rate": 3.39769647696477e-05,
      "loss": 5.6361,
      "step": 236500
    },
    {
      "epoch": 321.1382113821138,
      "grad_norm": 8.025078773498535,
      "learning_rate": 3.394308943089431e-05,
      "loss": 5.6452,
      "step": 237000
    },
    {
      "epoch": 321.81571815718155,
      "grad_norm": 8.789301872253418,
      "learning_rate": 3.3909214092140924e-05,
      "loss": 5.6235,
      "step": 237500
    },
    {
      "epoch": 322.49322493224935,
      "grad_norm": 7.047425746917725,
      "learning_rate": 3.3875338753387534e-05,
      "loss": 5.6552,
      "step": 238000
    },
    {
      "epoch": 323.1707317073171,
      "grad_norm": 8.790125846862793,
      "learning_rate": 3.384146341463415e-05,
      "loss": 5.6351,
      "step": 238500
    },
    {
      "epoch": 323.84823848238483,
      "grad_norm": 5.554738521575928,
      "learning_rate": 3.380758807588076e-05,
      "loss": 5.6509,
      "step": 239000
    },
    {
      "epoch": 324.52574525745257,
      "grad_norm": 9.12978744506836,
      "learning_rate": 3.377371273712737e-05,
      "loss": 5.6153,
      "step": 239500
    },
    {
      "epoch": 325.2032520325203,
      "grad_norm": 10.973477363586426,
      "learning_rate": 3.373983739837399e-05,
      "loss": 5.6088,
      "step": 240000
    },
    {
      "epoch": 325.88075880758805,
      "grad_norm": 11.238083839416504,
      "learning_rate": 3.37059620596206e-05,
      "loss": 5.6557,
      "step": 240500
    },
    {
      "epoch": 326.55826558265585,
      "grad_norm": 6.917649745941162,
      "learning_rate": 3.3672086720867214e-05,
      "loss": 5.6573,
      "step": 241000
    },
    {
      "epoch": 327.2357723577236,
      "grad_norm": 13.260126113891602,
      "learning_rate": 3.3638211382113824e-05,
      "loss": 5.6376,
      "step": 241500
    },
    {
      "epoch": 327.91327913279133,
      "grad_norm": 14.946444511413574,
      "learning_rate": 3.3604336043360434e-05,
      "loss": 5.6318,
      "step": 242000
    },
    {
      "epoch": 328.5907859078591,
      "grad_norm": 19.169536590576172,
      "learning_rate": 3.357046070460705e-05,
      "loss": 5.6315,
      "step": 242500
    },
    {
      "epoch": 329.2682926829268,
      "grad_norm": 9.64830493927002,
      "learning_rate": 3.353658536585366e-05,
      "loss": 5.636,
      "step": 243000
    },
    {
      "epoch": 329.94579945799455,
      "grad_norm": 8.478034973144531,
      "learning_rate": 3.350271002710027e-05,
      "loss": 5.6457,
      "step": 243500
    },
    {
      "epoch": 330.62330623306235,
      "grad_norm": 9.721768379211426,
      "learning_rate": 3.346883468834688e-05,
      "loss": 5.6509,
      "step": 244000
    },
    {
      "epoch": 331.3008130081301,
      "grad_norm": 8.703048706054688,
      "learning_rate": 3.34349593495935e-05,
      "loss": 5.6084,
      "step": 244500
    },
    {
      "epoch": 331.97831978319783,
      "grad_norm": 6.074817657470703,
      "learning_rate": 3.340108401084011e-05,
      "loss": 5.6431,
      "step": 245000
    },
    {
      "epoch": 332.6558265582656,
      "grad_norm": 10.028462409973145,
      "learning_rate": 3.3367208672086725e-05,
      "loss": 5.6408,
      "step": 245500
    },
    {
      "epoch": 333.3333333333333,
      "grad_norm": 10.228201866149902,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 5.6431,
      "step": 246000
    },
    {
      "epoch": 334.0108401084011,
      "grad_norm": 6.065526485443115,
      "learning_rate": 3.3299457994579945e-05,
      "loss": 5.6362,
      "step": 246500
    },
    {
      "epoch": 334.68834688346885,
      "grad_norm": 12.137288093566895,
      "learning_rate": 3.326558265582656e-05,
      "loss": 5.6392,
      "step": 247000
    },
    {
      "epoch": 335.3658536585366,
      "grad_norm": 10.260432243347168,
      "learning_rate": 3.323170731707317e-05,
      "loss": 5.6386,
      "step": 247500
    },
    {
      "epoch": 336.04336043360433,
      "grad_norm": 11.00155258178711,
      "learning_rate": 3.319783197831978e-05,
      "loss": 5.6232,
      "step": 248000
    },
    {
      "epoch": 336.7208672086721,
      "grad_norm": 8.80339241027832,
      "learning_rate": 3.31639566395664e-05,
      "loss": 5.6662,
      "step": 248500
    },
    {
      "epoch": 337.3983739837398,
      "grad_norm": 10.507308959960938,
      "learning_rate": 3.313008130081301e-05,
      "loss": 5.5736,
      "step": 249000
    },
    {
      "epoch": 338.0758807588076,
      "grad_norm": 11.039789199829102,
      "learning_rate": 3.3096205962059625e-05,
      "loss": 5.6507,
      "step": 249500
    },
    {
      "epoch": 338.75338753387535,
      "grad_norm": 6.931565761566162,
      "learning_rate": 3.3062330623306235e-05,
      "loss": 5.6313,
      "step": 250000
    },
    {
      "epoch": 339.4308943089431,
      "grad_norm": 13.058575630187988,
      "learning_rate": 3.3028455284552845e-05,
      "loss": 5.6434,
      "step": 250500
    },
    {
      "epoch": 340.10840108401084,
      "grad_norm": 12.30347728729248,
      "learning_rate": 3.2994579945799456e-05,
      "loss": 5.6415,
      "step": 251000
    },
    {
      "epoch": 340.7859078590786,
      "grad_norm": 9.079402923583984,
      "learning_rate": 3.296070460704607e-05,
      "loss": 5.6212,
      "step": 251500
    },
    {
      "epoch": 341.4634146341463,
      "grad_norm": 17.50655746459961,
      "learning_rate": 3.292682926829269e-05,
      "loss": 5.6513,
      "step": 252000
    },
    {
      "epoch": 342.1409214092141,
      "grad_norm": 9.59919261932373,
      "learning_rate": 3.28929539295393e-05,
      "loss": 5.6362,
      "step": 252500
    },
    {
      "epoch": 342.81842818428186,
      "grad_norm": 6.403855800628662,
      "learning_rate": 3.285907859078591e-05,
      "loss": 5.6283,
      "step": 253000
    },
    {
      "epoch": 343.4959349593496,
      "grad_norm": 6.834600448608398,
      "learning_rate": 3.282520325203252e-05,
      "loss": 5.6177,
      "step": 253500
    },
    {
      "epoch": 344.17344173441734,
      "grad_norm": 9.10738468170166,
      "learning_rate": 3.2791327913279136e-05,
      "loss": 5.6338,
      "step": 254000
    },
    {
      "epoch": 344.8509485094851,
      "grad_norm": 8.326763153076172,
      "learning_rate": 3.2757452574525746e-05,
      "loss": 5.6524,
      "step": 254500
    },
    {
      "epoch": 345.5284552845528,
      "grad_norm": 8.131766319274902,
      "learning_rate": 3.2723577235772356e-05,
      "loss": 5.5908,
      "step": 255000
    },
    {
      "epoch": 346.2059620596206,
      "grad_norm": 9.731898307800293,
      "learning_rate": 3.268970189701897e-05,
      "loss": 5.6622,
      "step": 255500
    },
    {
      "epoch": 346.88346883468836,
      "grad_norm": 6.899817943572998,
      "learning_rate": 3.265582655826558e-05,
      "loss": 5.6405,
      "step": 256000
    },
    {
      "epoch": 347.5609756097561,
      "grad_norm": 6.952796936035156,
      "learning_rate": 3.26219512195122e-05,
      "loss": 5.645,
      "step": 256500
    },
    {
      "epoch": 348.23848238482384,
      "grad_norm": 7.54498815536499,
      "learning_rate": 3.258807588075881e-05,
      "loss": 5.6624,
      "step": 257000
    },
    {
      "epoch": 348.9159891598916,
      "grad_norm": 7.000537395477295,
      "learning_rate": 3.255420054200542e-05,
      "loss": 5.607,
      "step": 257500
    },
    {
      "epoch": 349.5934959349593,
      "grad_norm": 9.078072547912598,
      "learning_rate": 3.2520325203252037e-05,
      "loss": 5.6425,
      "step": 258000
    },
    {
      "epoch": 350.2710027100271,
      "grad_norm": 10.158866882324219,
      "learning_rate": 3.2486449864498647e-05,
      "loss": 5.6362,
      "step": 258500
    },
    {
      "epoch": 350.94850948509486,
      "grad_norm": 7.343793869018555,
      "learning_rate": 3.245257452574526e-05,
      "loss": 5.62,
      "step": 259000
    },
    {
      "epoch": 351.6260162601626,
      "grad_norm": 10.157678604125977,
      "learning_rate": 3.241869918699187e-05,
      "loss": 5.6448,
      "step": 259500
    },
    {
      "epoch": 352.30352303523034,
      "grad_norm": 21.466419219970703,
      "learning_rate": 3.2384823848238483e-05,
      "loss": 5.6176,
      "step": 260000
    },
    {
      "epoch": 352.9810298102981,
      "grad_norm": 7.733177661895752,
      "learning_rate": 3.2350948509485093e-05,
      "loss": 5.6507,
      "step": 260500
    },
    {
      "epoch": 353.6585365853659,
      "grad_norm": 12.053696632385254,
      "learning_rate": 3.231707317073171e-05,
      "loss": 5.6248,
      "step": 261000
    },
    {
      "epoch": 354.3360433604336,
      "grad_norm": 7.20143461227417,
      "learning_rate": 3.228319783197832e-05,
      "loss": 5.6388,
      "step": 261500
    },
    {
      "epoch": 355.01355013550136,
      "grad_norm": 8.536059379577637,
      "learning_rate": 3.224932249322493e-05,
      "loss": 5.6295,
      "step": 262000
    },
    {
      "epoch": 355.6910569105691,
      "grad_norm": 10.032732963562012,
      "learning_rate": 3.221544715447155e-05,
      "loss": 5.6423,
      "step": 262500
    },
    {
      "epoch": 356.36856368563684,
      "grad_norm": 17.669389724731445,
      "learning_rate": 3.218157181571816e-05,
      "loss": 5.6073,
      "step": 263000
    },
    {
      "epoch": 357.0460704607046,
      "grad_norm": 10.581758499145508,
      "learning_rate": 3.2147696476964774e-05,
      "loss": 5.6459,
      "step": 263500
    },
    {
      "epoch": 357.7235772357724,
      "grad_norm": 8.224884986877441,
      "learning_rate": 3.2113821138211384e-05,
      "loss": 5.6236,
      "step": 264000
    },
    {
      "epoch": 358.4010840108401,
      "grad_norm": 8.530622482299805,
      "learning_rate": 3.2079945799457994e-05,
      "loss": 5.6237,
      "step": 264500
    },
    {
      "epoch": 359.07859078590786,
      "grad_norm": 8.433812141418457,
      "learning_rate": 3.204607046070461e-05,
      "loss": 5.6516,
      "step": 265000
    },
    {
      "epoch": 359.7560975609756,
      "grad_norm": 10.720961570739746,
      "learning_rate": 3.201219512195122e-05,
      "loss": 5.6479,
      "step": 265500
    },
    {
      "epoch": 360.43360433604335,
      "grad_norm": 8.914022445678711,
      "learning_rate": 3.197831978319784e-05,
      "loss": 5.6036,
      "step": 266000
    },
    {
      "epoch": 361.1111111111111,
      "grad_norm": 9.036809921264648,
      "learning_rate": 3.194444444444444e-05,
      "loss": 5.659,
      "step": 266500
    },
    {
      "epoch": 361.7886178861789,
      "grad_norm": 11.355944633483887,
      "learning_rate": 3.191056910569106e-05,
      "loss": 5.632,
      "step": 267000
    },
    {
      "epoch": 362.4661246612466,
      "grad_norm": 8.82209300994873,
      "learning_rate": 3.187669376693767e-05,
      "loss": 5.6325,
      "step": 267500
    },
    {
      "epoch": 363.14363143631437,
      "grad_norm": 9.35318660736084,
      "learning_rate": 3.1842818428184285e-05,
      "loss": 5.6377,
      "step": 268000
    },
    {
      "epoch": 363.8211382113821,
      "grad_norm": 8.12077808380127,
      "learning_rate": 3.18089430894309e-05,
      "loss": 5.6318,
      "step": 268500
    },
    {
      "epoch": 364.49864498644985,
      "grad_norm": 8.098655700683594,
      "learning_rate": 3.1775067750677505e-05,
      "loss": 5.6309,
      "step": 269000
    },
    {
      "epoch": 365.1761517615176,
      "grad_norm": 5.7750325202941895,
      "learning_rate": 3.174119241192412e-05,
      "loss": 5.6317,
      "step": 269500
    },
    {
      "epoch": 365.8536585365854,
      "grad_norm": 9.858672142028809,
      "learning_rate": 3.170731707317073e-05,
      "loss": 5.6392,
      "step": 270000
    },
    {
      "epoch": 366.5311653116531,
      "grad_norm": 9.507521629333496,
      "learning_rate": 3.167344173441735e-05,
      "loss": 5.6344,
      "step": 270500
    },
    {
      "epoch": 367.20867208672087,
      "grad_norm": 15.29931926727295,
      "learning_rate": 3.163956639566396e-05,
      "loss": 5.6182,
      "step": 271000
    },
    {
      "epoch": 367.8861788617886,
      "grad_norm": 10.747172355651855,
      "learning_rate": 3.160569105691057e-05,
      "loss": 5.6513,
      "step": 271500
    },
    {
      "epoch": 368.56368563685635,
      "grad_norm": 13.830399513244629,
      "learning_rate": 3.1571815718157185e-05,
      "loss": 5.6422,
      "step": 272000
    },
    {
      "epoch": 369.24119241192415,
      "grad_norm": 8.30903434753418,
      "learning_rate": 3.1537940379403795e-05,
      "loss": 5.6163,
      "step": 272500
    },
    {
      "epoch": 369.9186991869919,
      "grad_norm": 7.160650730133057,
      "learning_rate": 3.150406504065041e-05,
      "loss": 5.6479,
      "step": 273000
    },
    {
      "epoch": 370.5962059620596,
      "grad_norm": 8.375289916992188,
      "learning_rate": 3.1470189701897015e-05,
      "loss": 5.6606,
      "step": 273500
    },
    {
      "epoch": 371.27371273712737,
      "grad_norm": 10.299137115478516,
      "learning_rate": 3.143631436314363e-05,
      "loss": 5.599,
      "step": 274000
    },
    {
      "epoch": 371.9512195121951,
      "grad_norm": 10.528438568115234,
      "learning_rate": 3.140243902439025e-05,
      "loss": 5.6342,
      "step": 274500
    },
    {
      "epoch": 372.62872628726285,
      "grad_norm": 7.8246917724609375,
      "learning_rate": 3.136856368563686e-05,
      "loss": 5.6059,
      "step": 275000
    },
    {
      "epoch": 373.30623306233065,
      "grad_norm": 11.204476356506348,
      "learning_rate": 3.1334688346883476e-05,
      "loss": 5.6472,
      "step": 275500
    },
    {
      "epoch": 373.9837398373984,
      "grad_norm": 11.783681869506836,
      "learning_rate": 3.130081300813008e-05,
      "loss": 5.6188,
      "step": 276000
    },
    {
      "epoch": 374.66124661246613,
      "grad_norm": 7.432352542877197,
      "learning_rate": 3.1266937669376696e-05,
      "loss": 5.6603,
      "step": 276500
    },
    {
      "epoch": 375.33875338753387,
      "grad_norm": 7.837210178375244,
      "learning_rate": 3.1233062330623306e-05,
      "loss": 5.5971,
      "step": 277000
    },
    {
      "epoch": 376.0162601626016,
      "grad_norm": 8.424173355102539,
      "learning_rate": 3.119918699186992e-05,
      "loss": 5.6416,
      "step": 277500
    },
    {
      "epoch": 376.69376693766935,
      "grad_norm": 7.403470039367676,
      "learning_rate": 3.116531165311653e-05,
      "loss": 5.6226,
      "step": 278000
    },
    {
      "epoch": 377.37127371273715,
      "grad_norm": 7.226387977600098,
      "learning_rate": 3.113143631436314e-05,
      "loss": 5.6421,
      "step": 278500
    },
    {
      "epoch": 378.0487804878049,
      "grad_norm": 9.231013298034668,
      "learning_rate": 3.109756097560976e-05,
      "loss": 5.6103,
      "step": 279000
    },
    {
      "epoch": 378.72628726287263,
      "grad_norm": 9.06235408782959,
      "learning_rate": 3.106368563685637e-05,
      "loss": 5.6487,
      "step": 279500
    },
    {
      "epoch": 379.4037940379404,
      "grad_norm": 9.246187210083008,
      "learning_rate": 3.1029810298102986e-05,
      "loss": 5.6121,
      "step": 280000
    },
    {
      "epoch": 380.0813008130081,
      "grad_norm": 8.529306411743164,
      "learning_rate": 3.0995934959349596e-05,
      "loss": 5.6525,
      "step": 280500
    },
    {
      "epoch": 380.75880758807585,
      "grad_norm": 7.675319671630859,
      "learning_rate": 3.0962059620596206e-05,
      "loss": 5.6218,
      "step": 281000
    },
    {
      "epoch": 381.43631436314365,
      "grad_norm": 12.075783729553223,
      "learning_rate": 3.092818428184282e-05,
      "loss": 5.6459,
      "step": 281500
    },
    {
      "epoch": 382.1138211382114,
      "grad_norm": 6.870710849761963,
      "learning_rate": 3.089430894308943e-05,
      "loss": 5.6266,
      "step": 282000
    },
    {
      "epoch": 382.79132791327913,
      "grad_norm": 9.719407081604004,
      "learning_rate": 3.086043360433605e-05,
      "loss": 5.6039,
      "step": 282500
    },
    {
      "epoch": 383.4688346883469,
      "grad_norm": 11.197888374328613,
      "learning_rate": 3.082655826558265e-05,
      "loss": 5.6567,
      "step": 283000
    },
    {
      "epoch": 384.1463414634146,
      "grad_norm": 13.31100845336914,
      "learning_rate": 3.079268292682927e-05,
      "loss": 5.6091,
      "step": 283500
    },
    {
      "epoch": 384.8238482384824,
      "grad_norm": 6.425230026245117,
      "learning_rate": 3.075880758807588e-05,
      "loss": 5.6447,
      "step": 284000
    },
    {
      "epoch": 385.50135501355015,
      "grad_norm": 9.868284225463867,
      "learning_rate": 3.07249322493225e-05,
      "loss": 5.6373,
      "step": 284500
    },
    {
      "epoch": 386.1788617886179,
      "grad_norm": 10.42194652557373,
      "learning_rate": 3.069105691056911e-05,
      "loss": 5.6305,
      "step": 285000
    },
    {
      "epoch": 386.85636856368563,
      "grad_norm": 11.201037406921387,
      "learning_rate": 3.065718157181572e-05,
      "loss": 5.6274,
      "step": 285500
    },
    {
      "epoch": 387.5338753387534,
      "grad_norm": 8.402122497558594,
      "learning_rate": 3.0623306233062334e-05,
      "loss": 5.5912,
      "step": 286000
    },
    {
      "epoch": 388.2113821138211,
      "grad_norm": 11.065324783325195,
      "learning_rate": 3.0589430894308944e-05,
      "loss": 5.6598,
      "step": 286500
    },
    {
      "epoch": 388.8888888888889,
      "grad_norm": 6.329516410827637,
      "learning_rate": 3.055555555555556e-05,
      "loss": 5.624,
      "step": 287000
    },
    {
      "epoch": 389.56639566395665,
      "grad_norm": 8.163415908813477,
      "learning_rate": 3.052168021680217e-05,
      "loss": 5.6432,
      "step": 287500
    },
    {
      "epoch": 390.2439024390244,
      "grad_norm": 9.789966583251953,
      "learning_rate": 3.048780487804878e-05,
      "loss": 5.6574,
      "step": 288000
    },
    {
      "epoch": 390.92140921409214,
      "grad_norm": 8.170208930969238,
      "learning_rate": 3.0453929539295394e-05,
      "loss": 5.6057,
      "step": 288500
    },
    {
      "epoch": 391.5989159891599,
      "grad_norm": 10.893257141113281,
      "learning_rate": 3.0420054200542007e-05,
      "loss": 5.6451,
      "step": 289000
    },
    {
      "epoch": 392.2764227642276,
      "grad_norm": 8.410544395446777,
      "learning_rate": 3.0386178861788617e-05,
      "loss": 5.6418,
      "step": 289500
    },
    {
      "epoch": 392.9539295392954,
      "grad_norm": 6.687424182891846,
      "learning_rate": 3.035230352303523e-05,
      "loss": 5.606,
      "step": 290000
    },
    {
      "epoch": 393.63143631436316,
      "grad_norm": 7.322537899017334,
      "learning_rate": 3.0318428184281844e-05,
      "loss": 5.6031,
      "step": 290500
    },
    {
      "epoch": 394.3089430894309,
      "grad_norm": 8.329278945922852,
      "learning_rate": 3.0284552845528458e-05,
      "loss": 5.6553,
      "step": 291000
    },
    {
      "epoch": 394.98644986449864,
      "grad_norm": 13.820444107055664,
      "learning_rate": 3.025067750677507e-05,
      "loss": 5.6262,
      "step": 291500
    },
    {
      "epoch": 395.6639566395664,
      "grad_norm": 6.979198932647705,
      "learning_rate": 3.021680216802168e-05,
      "loss": 5.6268,
      "step": 292000
    },
    {
      "epoch": 396.3414634146341,
      "grad_norm": 6.506889343261719,
      "learning_rate": 3.0182926829268294e-05,
      "loss": 5.6679,
      "step": 292500
    },
    {
      "epoch": 397.0189701897019,
      "grad_norm": 9.374408721923828,
      "learning_rate": 3.0149051490514908e-05,
      "loss": 5.6031,
      "step": 293000
    },
    {
      "epoch": 397.69647696476966,
      "grad_norm": 11.771394729614258,
      "learning_rate": 3.011517615176152e-05,
      "loss": 5.5983,
      "step": 293500
    },
    {
      "epoch": 398.3739837398374,
      "grad_norm": 6.908692836761475,
      "learning_rate": 3.0081300813008135e-05,
      "loss": 5.6614,
      "step": 294000
    },
    {
      "epoch": 399.05149051490514,
      "grad_norm": 9.985855102539062,
      "learning_rate": 3.004742547425474e-05,
      "loss": 5.6406,
      "step": 294500
    },
    {
      "epoch": 399.7289972899729,
      "grad_norm": 10.821776390075684,
      "learning_rate": 3.0013550135501355e-05,
      "loss": 5.6273,
      "step": 295000
    },
    {
      "epoch": 400.4065040650407,
      "grad_norm": 8.633293151855469,
      "learning_rate": 2.9979674796747968e-05,
      "loss": 5.605,
      "step": 295500
    },
    {
      "epoch": 401.0840108401084,
      "grad_norm": 11.643775939941406,
      "learning_rate": 2.9945799457994585e-05,
      "loss": 5.6564,
      "step": 296000
    },
    {
      "epoch": 401.76151761517616,
      "grad_norm": 7.973424434661865,
      "learning_rate": 2.991192411924119e-05,
      "loss": 5.6162,
      "step": 296500
    },
    {
      "epoch": 402.4390243902439,
      "grad_norm": 13.279562950134277,
      "learning_rate": 2.9878048780487805e-05,
      "loss": 5.6468,
      "step": 297000
    },
    {
      "epoch": 403.11653116531164,
      "grad_norm": 6.707332611083984,
      "learning_rate": 2.984417344173442e-05,
      "loss": 5.6092,
      "step": 297500
    },
    {
      "epoch": 403.7940379403794,
      "grad_norm": 8.703377723693848,
      "learning_rate": 2.9810298102981032e-05,
      "loss": 5.6643,
      "step": 298000
    },
    {
      "epoch": 404.4715447154472,
      "grad_norm": 7.319753170013428,
      "learning_rate": 2.9776422764227645e-05,
      "loss": 5.5772,
      "step": 298500
    },
    {
      "epoch": 405.1490514905149,
      "grad_norm": 8.566414833068848,
      "learning_rate": 2.9742547425474255e-05,
      "loss": 5.6811,
      "step": 299000
    },
    {
      "epoch": 405.82655826558266,
      "grad_norm": 11.085467338562012,
      "learning_rate": 2.970867208672087e-05,
      "loss": 5.6275,
      "step": 299500
    },
    {
      "epoch": 406.5040650406504,
      "grad_norm": 8.837843894958496,
      "learning_rate": 2.9674796747967482e-05,
      "loss": 5.6229,
      "step": 300000
    },
    {
      "epoch": 407.18157181571814,
      "grad_norm": 11.918191909790039,
      "learning_rate": 2.9640921409214096e-05,
      "loss": 5.6176,
      "step": 300500
    },
    {
      "epoch": 407.8590785907859,
      "grad_norm": 6.790528297424316,
      "learning_rate": 2.9607046070460702e-05,
      "loss": 5.6487,
      "step": 301000
    },
    {
      "epoch": 408.5365853658537,
      "grad_norm": 9.145112991333008,
      "learning_rate": 2.9573170731707316e-05,
      "loss": 5.6191,
      "step": 301500
    },
    {
      "epoch": 409.2140921409214,
      "grad_norm": 12.371138572692871,
      "learning_rate": 2.9539295392953932e-05,
      "loss": 5.6296,
      "step": 302000
    },
    {
      "epoch": 409.89159891598916,
      "grad_norm": 6.470793724060059,
      "learning_rate": 2.9505420054200546e-05,
      "loss": 5.6214,
      "step": 302500
    },
    {
      "epoch": 410.5691056910569,
      "grad_norm": 12.877527236938477,
      "learning_rate": 2.947154471544716e-05,
      "loss": 5.629,
      "step": 303000
    },
    {
      "epoch": 411.24661246612465,
      "grad_norm": 15.587830543518066,
      "learning_rate": 2.9437669376693766e-05,
      "loss": 5.6343,
      "step": 303500
    },
    {
      "epoch": 411.9241192411924,
      "grad_norm": 8.972907066345215,
      "learning_rate": 2.940379403794038e-05,
      "loss": 5.6286,
      "step": 304000
    },
    {
      "epoch": 412.6016260162602,
      "grad_norm": 7.5668182373046875,
      "learning_rate": 2.9369918699186993e-05,
      "loss": 5.6077,
      "step": 304500
    },
    {
      "epoch": 413.2791327913279,
      "grad_norm": 8.336530685424805,
      "learning_rate": 2.9336043360433606e-05,
      "loss": 5.6372,
      "step": 305000
    },
    {
      "epoch": 413.95663956639567,
      "grad_norm": 11.019227981567383,
      "learning_rate": 2.930216802168022e-05,
      "loss": 5.6362,
      "step": 305500
    },
    {
      "epoch": 414.6341463414634,
      "grad_norm": 9.911242485046387,
      "learning_rate": 2.926829268292683e-05,
      "loss": 5.6355,
      "step": 306000
    },
    {
      "epoch": 415.31165311653115,
      "grad_norm": 9.284493446350098,
      "learning_rate": 2.9234417344173443e-05,
      "loss": 5.608,
      "step": 306500
    },
    {
      "epoch": 415.9891598915989,
      "grad_norm": 8.046570777893066,
      "learning_rate": 2.9200542005420056e-05,
      "loss": 5.6379,
      "step": 307000
    },
    {
      "epoch": 416.6666666666667,
      "grad_norm": 11.400776863098145,
      "learning_rate": 2.916666666666667e-05,
      "loss": 5.6444,
      "step": 307500
    },
    {
      "epoch": 417.3441734417344,
      "grad_norm": 7.585434436798096,
      "learning_rate": 2.9132791327913276e-05,
      "loss": 5.6225,
      "step": 308000
    },
    {
      "epoch": 418.02168021680217,
      "grad_norm": 10.414618492126465,
      "learning_rate": 2.9098915989159893e-05,
      "loss": 5.6091,
      "step": 308500
    },
    {
      "epoch": 418.6991869918699,
      "grad_norm": 8.065774917602539,
      "learning_rate": 2.9065040650406507e-05,
      "loss": 5.6246,
      "step": 309000
    },
    {
      "epoch": 419.37669376693765,
      "grad_norm": 9.653799057006836,
      "learning_rate": 2.903116531165312e-05,
      "loss": 5.6533,
      "step": 309500
    },
    {
      "epoch": 420.05420054200545,
      "grad_norm": 12.139659881591797,
      "learning_rate": 2.8997289972899733e-05,
      "loss": 5.5886,
      "step": 310000
    },
    {
      "epoch": 420.7317073170732,
      "grad_norm": 12.345043182373047,
      "learning_rate": 2.896341463414634e-05,
      "loss": 5.6417,
      "step": 310500
    },
    {
      "epoch": 421.4092140921409,
      "grad_norm": 6.785089015960693,
      "learning_rate": 2.8929539295392953e-05,
      "loss": 5.6276,
      "step": 311000
    },
    {
      "epoch": 422.08672086720867,
      "grad_norm": 7.461589336395264,
      "learning_rate": 2.8895663956639567e-05,
      "loss": 5.6504,
      "step": 311500
    },
    {
      "epoch": 422.7642276422764,
      "grad_norm": 22.690824508666992,
      "learning_rate": 2.886178861788618e-05,
      "loss": 5.6395,
      "step": 312000
    },
    {
      "epoch": 423.44173441734415,
      "grad_norm": 7.231919765472412,
      "learning_rate": 2.882791327913279e-05,
      "loss": 5.5994,
      "step": 312500
    },
    {
      "epoch": 424.11924119241195,
      "grad_norm": 7.67570161819458,
      "learning_rate": 2.8794037940379404e-05,
      "loss": 5.6489,
      "step": 313000
    },
    {
      "epoch": 424.7967479674797,
      "grad_norm": 6.570057392120361,
      "learning_rate": 2.8760162601626017e-05,
      "loss": 5.6405,
      "step": 313500
    },
    {
      "epoch": 425.47425474254743,
      "grad_norm": 10.467325210571289,
      "learning_rate": 2.872628726287263e-05,
      "loss": 5.5936,
      "step": 314000
    },
    {
      "epoch": 426.15176151761517,
      "grad_norm": 7.588211536407471,
      "learning_rate": 2.8692411924119244e-05,
      "loss": 5.6301,
      "step": 314500
    },
    {
      "epoch": 426.8292682926829,
      "grad_norm": 9.172679901123047,
      "learning_rate": 2.8658536585365854e-05,
      "loss": 5.6175,
      "step": 315000
    },
    {
      "epoch": 427.50677506775065,
      "grad_norm": 10.043560981750488,
      "learning_rate": 2.8624661246612467e-05,
      "loss": 5.6684,
      "step": 315500
    },
    {
      "epoch": 428.18428184281845,
      "grad_norm": 10.084662437438965,
      "learning_rate": 2.859078590785908e-05,
      "loss": 5.598,
      "step": 316000
    },
    {
      "epoch": 428.8617886178862,
      "grad_norm": 9.44929027557373,
      "learning_rate": 2.8556910569105694e-05,
      "loss": 5.6415,
      "step": 316500
    },
    {
      "epoch": 429.53929539295393,
      "grad_norm": 16.680818557739258,
      "learning_rate": 2.8523035230352308e-05,
      "loss": 5.68,
      "step": 317000
    },
    {
      "epoch": 430.2168021680217,
      "grad_norm": 5.784908771514893,
      "learning_rate": 2.8489159891598914e-05,
      "loss": 5.5944,
      "step": 317500
    },
    {
      "epoch": 430.8943089430894,
      "grad_norm": 17.05877685546875,
      "learning_rate": 2.8455284552845528e-05,
      "loss": 5.6189,
      "step": 318000
    },
    {
      "epoch": 431.57181571815715,
      "grad_norm": 10.207708358764648,
      "learning_rate": 2.842140921409214e-05,
      "loss": 5.6439,
      "step": 318500
    },
    {
      "epoch": 432.24932249322495,
      "grad_norm": 8.4057035446167,
      "learning_rate": 2.8387533875338758e-05,
      "loss": 5.6387,
      "step": 319000
    },
    {
      "epoch": 432.9268292682927,
      "grad_norm": 9.756240844726562,
      "learning_rate": 2.8353658536585365e-05,
      "loss": 5.6227,
      "step": 319500
    },
    {
      "epoch": 433.60433604336043,
      "grad_norm": 7.954012393951416,
      "learning_rate": 2.8319783197831978e-05,
      "loss": 5.6435,
      "step": 320000
    },
    {
      "epoch": 434.2818428184282,
      "grad_norm": 8.120970726013184,
      "learning_rate": 2.828590785907859e-05,
      "loss": 5.6393,
      "step": 320500
    },
    {
      "epoch": 434.9593495934959,
      "grad_norm": 7.416593551635742,
      "learning_rate": 2.8252032520325205e-05,
      "loss": 5.613,
      "step": 321000
    },
    {
      "epoch": 435.6368563685637,
      "grad_norm": 9.080799102783203,
      "learning_rate": 2.8218157181571818e-05,
      "loss": 5.609,
      "step": 321500
    },
    {
      "epoch": 436.31436314363145,
      "grad_norm": 12.603974342346191,
      "learning_rate": 2.8184281842818428e-05,
      "loss": 5.6579,
      "step": 322000
    },
    {
      "epoch": 436.9918699186992,
      "grad_norm": 8.494144439697266,
      "learning_rate": 2.8150406504065042e-05,
      "loss": 5.6174,
      "step": 322500
    },
    {
      "epoch": 437.66937669376694,
      "grad_norm": 7.858898162841797,
      "learning_rate": 2.8116531165311655e-05,
      "loss": 5.6225,
      "step": 323000
    },
    {
      "epoch": 438.3468834688347,
      "grad_norm": 8.55502986907959,
      "learning_rate": 2.808265582655827e-05,
      "loss": 5.6359,
      "step": 323500
    },
    {
      "epoch": 439.0243902439024,
      "grad_norm": 9.367844581604004,
      "learning_rate": 2.8048780487804882e-05,
      "loss": 5.6382,
      "step": 324000
    },
    {
      "epoch": 439.7018970189702,
      "grad_norm": 12.331411361694336,
      "learning_rate": 2.801490514905149e-05,
      "loss": 5.6017,
      "step": 324500
    },
    {
      "epoch": 440.37940379403796,
      "grad_norm": 6.964086055755615,
      "learning_rate": 2.7981029810298105e-05,
      "loss": 5.643,
      "step": 325000
    },
    {
      "epoch": 441.0569105691057,
      "grad_norm": 9.841195106506348,
      "learning_rate": 2.794715447154472e-05,
      "loss": 5.6311,
      "step": 325500
    },
    {
      "epoch": 441.73441734417344,
      "grad_norm": 8.413569450378418,
      "learning_rate": 2.7913279132791332e-05,
      "loss": 5.6135,
      "step": 326000
    },
    {
      "epoch": 442.4119241192412,
      "grad_norm": 9.542010307312012,
      "learning_rate": 2.787940379403794e-05,
      "loss": 5.6485,
      "step": 326500
    },
    {
      "epoch": 443.0894308943089,
      "grad_norm": 8.688329696655273,
      "learning_rate": 2.7845528455284552e-05,
      "loss": 5.6028,
      "step": 327000
    },
    {
      "epoch": 443.7669376693767,
      "grad_norm": 9.2662353515625,
      "learning_rate": 2.7811653116531166e-05,
      "loss": 5.6324,
      "step": 327500
    },
    {
      "epoch": 444.44444444444446,
      "grad_norm": 7.9798712730407715,
      "learning_rate": 2.777777777777778e-05,
      "loss": 5.6481,
      "step": 328000
    },
    {
      "epoch": 445.1219512195122,
      "grad_norm": 7.709316253662109,
      "learning_rate": 2.7743902439024393e-05,
      "loss": 5.645,
      "step": 328500
    },
    {
      "epoch": 445.79945799457994,
      "grad_norm": 8.565438270568848,
      "learning_rate": 2.7710027100271003e-05,
      "loss": 5.6267,
      "step": 329000
    },
    {
      "epoch": 446.4769647696477,
      "grad_norm": 7.365926265716553,
      "learning_rate": 2.7676151761517616e-05,
      "loss": 5.6324,
      "step": 329500
    },
    {
      "epoch": 447.1544715447154,
      "grad_norm": 12.031521797180176,
      "learning_rate": 2.764227642276423e-05,
      "loss": 5.594,
      "step": 330000
    },
    {
      "epoch": 447.8319783197832,
      "grad_norm": 6.061403274536133,
      "learning_rate": 2.7608401084010843e-05,
      "loss": 5.6316,
      "step": 330500
    },
    {
      "epoch": 448.50948509485096,
      "grad_norm": 8.465155601501465,
      "learning_rate": 2.7574525745257453e-05,
      "loss": 5.6444,
      "step": 331000
    },
    {
      "epoch": 449.1869918699187,
      "grad_norm": 12.423921585083008,
      "learning_rate": 2.7540650406504066e-05,
      "loss": 5.6119,
      "step": 331500
    },
    {
      "epoch": 449.86449864498644,
      "grad_norm": 6.938936233520508,
      "learning_rate": 2.750677506775068e-05,
      "loss": 5.6382,
      "step": 332000
    },
    {
      "epoch": 450.5420054200542,
      "grad_norm": 16.987213134765625,
      "learning_rate": 2.7472899728997293e-05,
      "loss": 5.6217,
      "step": 332500
    },
    {
      "epoch": 451.219512195122,
      "grad_norm": 6.5478692054748535,
      "learning_rate": 2.7439024390243906e-05,
      "loss": 5.6227,
      "step": 333000
    },
    {
      "epoch": 451.8970189701897,
      "grad_norm": 7.160295486450195,
      "learning_rate": 2.7405149051490513e-05,
      "loss": 5.6344,
      "step": 333500
    },
    {
      "epoch": 452.57452574525746,
      "grad_norm": 10.778299331665039,
      "learning_rate": 2.7371273712737127e-05,
      "loss": 5.5919,
      "step": 334000
    },
    {
      "epoch": 453.2520325203252,
      "grad_norm": 8.051770210266113,
      "learning_rate": 2.733739837398374e-05,
      "loss": 5.6384,
      "step": 334500
    },
    {
      "epoch": 453.92953929539294,
      "grad_norm": 8.346693992614746,
      "learning_rate": 2.7303523035230353e-05,
      "loss": 5.644,
      "step": 335000
    },
    {
      "epoch": 454.6070460704607,
      "grad_norm": 10.553491592407227,
      "learning_rate": 2.726964769647697e-05,
      "loss": 5.6214,
      "step": 335500
    },
    {
      "epoch": 455.2845528455285,
      "grad_norm": 8.939083099365234,
      "learning_rate": 2.7235772357723577e-05,
      "loss": 5.6108,
      "step": 336000
    },
    {
      "epoch": 455.9620596205962,
      "grad_norm": 8.609760284423828,
      "learning_rate": 2.720189701897019e-05,
      "loss": 5.6197,
      "step": 336500
    },
    {
      "epoch": 456.63956639566396,
      "grad_norm": 10.85909652709961,
      "learning_rate": 2.7168021680216804e-05,
      "loss": 5.6417,
      "step": 337000
    },
    {
      "epoch": 457.3170731707317,
      "grad_norm": 9.675455093383789,
      "learning_rate": 2.7134146341463417e-05,
      "loss": 5.6112,
      "step": 337500
    },
    {
      "epoch": 457.99457994579944,
      "grad_norm": 7.994511604309082,
      "learning_rate": 2.7100271002710027e-05,
      "loss": 5.6501,
      "step": 338000
    },
    {
      "epoch": 458.6720867208672,
      "grad_norm": 9.204487800598145,
      "learning_rate": 2.706639566395664e-05,
      "loss": 5.6246,
      "step": 338500
    },
    {
      "epoch": 459.349593495935,
      "grad_norm": 5.787694454193115,
      "learning_rate": 2.7032520325203254e-05,
      "loss": 5.6433,
      "step": 339000
    },
    {
      "epoch": 460.0271002710027,
      "grad_norm": 8.08414077758789,
      "learning_rate": 2.6998644986449867e-05,
      "loss": 5.6087,
      "step": 339500
    },
    {
      "epoch": 460.70460704607046,
      "grad_norm": 10.01225471496582,
      "learning_rate": 2.696476964769648e-05,
      "loss": 5.6388,
      "step": 340000
    },
    {
      "epoch": 461.3821138211382,
      "grad_norm": 8.17548942565918,
      "learning_rate": 2.6930894308943087e-05,
      "loss": 5.6373,
      "step": 340500
    },
    {
      "epoch": 462.05962059620595,
      "grad_norm": 8.152408599853516,
      "learning_rate": 2.68970189701897e-05,
      "loss": 5.6191,
      "step": 341000
    },
    {
      "epoch": 462.7371273712737,
      "grad_norm": 7.681801795959473,
      "learning_rate": 2.6863143631436318e-05,
      "loss": 5.6132,
      "step": 341500
    },
    {
      "epoch": 463.4146341463415,
      "grad_norm": 7.440188884735107,
      "learning_rate": 2.682926829268293e-05,
      "loss": 5.6086,
      "step": 342000
    },
    {
      "epoch": 464.0921409214092,
      "grad_norm": 6.860572338104248,
      "learning_rate": 2.6795392953929538e-05,
      "loss": 5.6454,
      "step": 342500
    },
    {
      "epoch": 464.76964769647697,
      "grad_norm": 12.91072940826416,
      "learning_rate": 2.676151761517615e-05,
      "loss": 5.6448,
      "step": 343000
    },
    {
      "epoch": 465.4471544715447,
      "grad_norm": 9.507233619689941,
      "learning_rate": 2.6727642276422764e-05,
      "loss": 5.5939,
      "step": 343500
    },
    {
      "epoch": 466.12466124661245,
      "grad_norm": 10.177942276000977,
      "learning_rate": 2.6693766937669378e-05,
      "loss": 5.6407,
      "step": 344000
    },
    {
      "epoch": 466.80216802168025,
      "grad_norm": 13.079660415649414,
      "learning_rate": 2.665989159891599e-05,
      "loss": 5.6342,
      "step": 344500
    },
    {
      "epoch": 467.479674796748,
      "grad_norm": 8.46875,
      "learning_rate": 2.66260162601626e-05,
      "loss": 5.6081,
      "step": 345000
    },
    {
      "epoch": 468.1571815718157,
      "grad_norm": 10.650582313537598,
      "learning_rate": 2.6592140921409215e-05,
      "loss": 5.6206,
      "step": 345500
    },
    {
      "epoch": 468.83468834688347,
      "grad_norm": 9.362503051757812,
      "learning_rate": 2.6558265582655828e-05,
      "loss": 5.637,
      "step": 346000
    },
    {
      "epoch": 469.5121951219512,
      "grad_norm": 6.807567596435547,
      "learning_rate": 2.652439024390244e-05,
      "loss": 5.6188,
      "step": 346500
    },
    {
      "epoch": 470.18970189701895,
      "grad_norm": 10.516081809997559,
      "learning_rate": 2.6490514905149055e-05,
      "loss": 5.6259,
      "step": 347000
    },
    {
      "epoch": 470.86720867208675,
      "grad_norm": 8.810256004333496,
      "learning_rate": 2.6456639566395665e-05,
      "loss": 5.6284,
      "step": 347500
    },
    {
      "epoch": 471.5447154471545,
      "grad_norm": 11.32108211517334,
      "learning_rate": 2.642276422764228e-05,
      "loss": 5.6331,
      "step": 348000
    },
    {
      "epoch": 472.22222222222223,
      "grad_norm": 10.784401893615723,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 5.6324,
      "step": 348500
    },
    {
      "epoch": 472.89972899728997,
      "grad_norm": 8.86186408996582,
      "learning_rate": 2.6355013550135505e-05,
      "loss": 5.6116,
      "step": 349000
    },
    {
      "epoch": 473.5772357723577,
      "grad_norm": 7.476858139038086,
      "learning_rate": 2.6321138211382112e-05,
      "loss": 5.6282,
      "step": 349500
    },
    {
      "epoch": 474.25474254742545,
      "grad_norm": 9.342679023742676,
      "learning_rate": 2.6287262872628725e-05,
      "loss": 5.6248,
      "step": 350000
    },
    {
      "epoch": 474.93224932249325,
      "grad_norm": 7.777608871459961,
      "learning_rate": 2.625338753387534e-05,
      "loss": 5.6152,
      "step": 350500
    },
    {
      "epoch": 475.609756097561,
      "grad_norm": 7.91969633102417,
      "learning_rate": 2.6219512195121952e-05,
      "loss": 5.6352,
      "step": 351000
    },
    {
      "epoch": 476.28726287262873,
      "grad_norm": 10.130812644958496,
      "learning_rate": 2.6185636856368566e-05,
      "loss": 5.621,
      "step": 351500
    },
    {
      "epoch": 476.96476964769647,
      "grad_norm": 7.277176856994629,
      "learning_rate": 2.6151761517615176e-05,
      "loss": 5.6303,
      "step": 352000
    },
    {
      "epoch": 477.6422764227642,
      "grad_norm": 8.229219436645508,
      "learning_rate": 2.611788617886179e-05,
      "loss": 5.6359,
      "step": 352500
    },
    {
      "epoch": 478.31978319783195,
      "grad_norm": 7.53687047958374,
      "learning_rate": 2.6084010840108402e-05,
      "loss": 5.6182,
      "step": 353000
    },
    {
      "epoch": 478.99728997289975,
      "grad_norm": 6.274863243103027,
      "learning_rate": 2.6050135501355016e-05,
      "loss": 5.6301,
      "step": 353500
    },
    {
      "epoch": 479.6747967479675,
      "grad_norm": 6.791721343994141,
      "learning_rate": 2.601626016260163e-05,
      "loss": 5.635,
      "step": 354000
    },
    {
      "epoch": 480.35230352303523,
      "grad_norm": 8.030323028564453,
      "learning_rate": 2.598238482384824e-05,
      "loss": 5.6189,
      "step": 354500
    },
    {
      "epoch": 481.029810298103,
      "grad_norm": 9.467840194702148,
      "learning_rate": 2.5948509485094853e-05,
      "loss": 5.6157,
      "step": 355000
    },
    {
      "epoch": 481.7073170731707,
      "grad_norm": 13.243246078491211,
      "learning_rate": 2.5914634146341466e-05,
      "loss": 5.6204,
      "step": 355500
    },
    {
      "epoch": 482.38482384823845,
      "grad_norm": 8.855727195739746,
      "learning_rate": 2.588075880758808e-05,
      "loss": 5.6145,
      "step": 356000
    },
    {
      "epoch": 483.06233062330625,
      "grad_norm": 6.927013874053955,
      "learning_rate": 2.5846883468834686e-05,
      "loss": 5.6212,
      "step": 356500
    },
    {
      "epoch": 483.739837398374,
      "grad_norm": 8.360072135925293,
      "learning_rate": 2.58130081300813e-05,
      "loss": 5.6147,
      "step": 357000
    },
    {
      "epoch": 484.41734417344173,
      "grad_norm": 6.458791732788086,
      "learning_rate": 2.5779132791327913e-05,
      "loss": 5.6039,
      "step": 357500
    },
    {
      "epoch": 485.0948509485095,
      "grad_norm": 12.613862991333008,
      "learning_rate": 2.574525745257453e-05,
      "loss": 5.6494,
      "step": 358000
    },
    {
      "epoch": 485.7723577235772,
      "grad_norm": 9.957158088684082,
      "learning_rate": 2.5711382113821143e-05,
      "loss": 5.6471,
      "step": 358500
    },
    {
      "epoch": 486.449864498645,
      "grad_norm": 8.653573036193848,
      "learning_rate": 2.567750677506775e-05,
      "loss": 5.6087,
      "step": 359000
    },
    {
      "epoch": 487.12737127371275,
      "grad_norm": 10.465425491333008,
      "learning_rate": 2.5643631436314363e-05,
      "loss": 5.6102,
      "step": 359500
    },
    {
      "epoch": 487.8048780487805,
      "grad_norm": 8.820876121520996,
      "learning_rate": 2.5609756097560977e-05,
      "loss": 5.6337,
      "step": 360000
    },
    {
      "epoch": 488.48238482384824,
      "grad_norm": 8.220757484436035,
      "learning_rate": 2.557588075880759e-05,
      "loss": 5.6095,
      "step": 360500
    },
    {
      "epoch": 489.159891598916,
      "grad_norm": 9.60897159576416,
      "learning_rate": 2.55420054200542e-05,
      "loss": 5.6267,
      "step": 361000
    },
    {
      "epoch": 489.8373983739837,
      "grad_norm": 7.364544868469238,
      "learning_rate": 2.5508130081300814e-05,
      "loss": 5.6405,
      "step": 361500
    },
    {
      "epoch": 490.5149051490515,
      "grad_norm": 7.663372039794922,
      "learning_rate": 2.5474254742547427e-05,
      "loss": 5.6404,
      "step": 362000
    },
    {
      "epoch": 491.19241192411926,
      "grad_norm": 10.619121551513672,
      "learning_rate": 2.544037940379404e-05,
      "loss": 5.6298,
      "step": 362500
    },
    {
      "epoch": 491.869918699187,
      "grad_norm": 12.19437026977539,
      "learning_rate": 2.5406504065040654e-05,
      "loss": 5.6289,
      "step": 363000
    },
    {
      "epoch": 492.54742547425474,
      "grad_norm": 8.992300987243652,
      "learning_rate": 2.537262872628726e-05,
      "loss": 5.588,
      "step": 363500
    },
    {
      "epoch": 493.2249322493225,
      "grad_norm": 8.41504192352295,
      "learning_rate": 2.5338753387533877e-05,
      "loss": 5.6647,
      "step": 364000
    },
    {
      "epoch": 493.9024390243902,
      "grad_norm": 7.762185573577881,
      "learning_rate": 2.530487804878049e-05,
      "loss": 5.593,
      "step": 364500
    },
    {
      "epoch": 494.579945799458,
      "grad_norm": 8.189887046813965,
      "learning_rate": 2.5271002710027104e-05,
      "loss": 5.6581,
      "step": 365000
    },
    {
      "epoch": 495.25745257452576,
      "grad_norm": 7.272666931152344,
      "learning_rate": 2.5237127371273717e-05,
      "loss": 5.6094,
      "step": 365500
    },
    {
      "epoch": 495.9349593495935,
      "grad_norm": 6.906292915344238,
      "learning_rate": 2.5203252032520324e-05,
      "loss": 5.6043,
      "step": 366000
    },
    {
      "epoch": 496.61246612466124,
      "grad_norm": 8.84660816192627,
      "learning_rate": 2.5169376693766938e-05,
      "loss": 5.6494,
      "step": 366500
    },
    {
      "epoch": 497.289972899729,
      "grad_norm": 10.696066856384277,
      "learning_rate": 2.513550135501355e-05,
      "loss": 5.6348,
      "step": 367000
    },
    {
      "epoch": 497.9674796747967,
      "grad_norm": 19.24994468688965,
      "learning_rate": 2.5101626016260164e-05,
      "loss": 5.5867,
      "step": 367500
    },
    {
      "epoch": 498.6449864498645,
      "grad_norm": 8.373651504516602,
      "learning_rate": 2.5067750677506774e-05,
      "loss": 5.6384,
      "step": 368000
    },
    {
      "epoch": 499.32249322493226,
      "grad_norm": 17.1677303314209,
      "learning_rate": 2.5033875338753388e-05,
      "loss": 5.6148,
      "step": 368500
    },
    {
      "epoch": 500.0,
      "grad_norm": 7.75079345703125,
      "learning_rate": 2.5e-05,
      "loss": 5.6243,
      "step": 369000
    },
    {
      "epoch": 500.67750677506774,
      "grad_norm": 14.486295700073242,
      "learning_rate": 2.4966124661246615e-05,
      "loss": 5.6148,
      "step": 369500
    },
    {
      "epoch": 501.3550135501355,
      "grad_norm": 9.818354606628418,
      "learning_rate": 2.4932249322493225e-05,
      "loss": 5.6054,
      "step": 370000
    },
    {
      "epoch": 502.0325203252033,
      "grad_norm": 8.706860542297363,
      "learning_rate": 2.4898373983739838e-05,
      "loss": 5.6478,
      "step": 370500
    },
    {
      "epoch": 502.710027100271,
      "grad_norm": 6.822022438049316,
      "learning_rate": 2.486449864498645e-05,
      "loss": 5.6139,
      "step": 371000
    },
    {
      "epoch": 503.38753387533876,
      "grad_norm": 10.062810897827148,
      "learning_rate": 2.4830623306233065e-05,
      "loss": 5.6276,
      "step": 371500
    },
    {
      "epoch": 504.0650406504065,
      "grad_norm": 8.776409149169922,
      "learning_rate": 2.4796747967479675e-05,
      "loss": 5.632,
      "step": 372000
    },
    {
      "epoch": 504.74254742547424,
      "grad_norm": 9.361395835876465,
      "learning_rate": 2.476287262872629e-05,
      "loss": 5.6029,
      "step": 372500
    },
    {
      "epoch": 505.420054200542,
      "grad_norm": 8.510884284973145,
      "learning_rate": 2.4728997289972902e-05,
      "loss": 5.6298,
      "step": 373000
    },
    {
      "epoch": 506.0975609756098,
      "grad_norm": 10.373930931091309,
      "learning_rate": 2.4695121951219512e-05,
      "loss": 5.6121,
      "step": 373500
    },
    {
      "epoch": 506.7750677506775,
      "grad_norm": 10.744671821594238,
      "learning_rate": 2.4661246612466125e-05,
      "loss": 5.6264,
      "step": 374000
    },
    {
      "epoch": 507.45257452574526,
      "grad_norm": 8.751737594604492,
      "learning_rate": 2.462737127371274e-05,
      "loss": 5.613,
      "step": 374500
    },
    {
      "epoch": 508.130081300813,
      "grad_norm": 9.553487777709961,
      "learning_rate": 2.4593495934959352e-05,
      "loss": 5.628,
      "step": 375000
    },
    {
      "epoch": 508.80758807588074,
      "grad_norm": 6.259963512420654,
      "learning_rate": 2.4559620596205962e-05,
      "loss": 5.6257,
      "step": 375500
    },
    {
      "epoch": 509.4850948509485,
      "grad_norm": 8.131499290466309,
      "learning_rate": 2.4525745257452575e-05,
      "loss": 5.6032,
      "step": 376000
    },
    {
      "epoch": 510.1626016260163,
      "grad_norm": 24.61614227294922,
      "learning_rate": 2.4491869918699185e-05,
      "loss": 5.6602,
      "step": 376500
    },
    {
      "epoch": 510.840108401084,
      "grad_norm": 7.482815742492676,
      "learning_rate": 2.44579945799458e-05,
      "loss": 5.6068,
      "step": 377000
    },
    {
      "epoch": 511.51761517615176,
      "grad_norm": 9.15999698638916,
      "learning_rate": 2.4424119241192416e-05,
      "loss": 5.6217,
      "step": 377500
    },
    {
      "epoch": 512.1951219512196,
      "grad_norm": 12.177998542785645,
      "learning_rate": 2.4390243902439026e-05,
      "loss": 5.6217,
      "step": 378000
    },
    {
      "epoch": 512.8726287262873,
      "grad_norm": 8.449509620666504,
      "learning_rate": 2.435636856368564e-05,
      "loss": 5.6199,
      "step": 378500
    },
    {
      "epoch": 513.550135501355,
      "grad_norm": 10.9780855178833,
      "learning_rate": 2.432249322493225e-05,
      "loss": 5.6454,
      "step": 379000
    },
    {
      "epoch": 514.2276422764228,
      "grad_norm": 8.905372619628906,
      "learning_rate": 2.4288617886178863e-05,
      "loss": 5.6066,
      "step": 379500
    },
    {
      "epoch": 514.9051490514905,
      "grad_norm": 10.460853576660156,
      "learning_rate": 2.4254742547425473e-05,
      "loss": 5.6261,
      "step": 380000
    },
    {
      "epoch": 515.5826558265583,
      "grad_norm": 7.446743965148926,
      "learning_rate": 2.422086720867209e-05,
      "loss": 5.5949,
      "step": 380500
    },
    {
      "epoch": 516.260162601626,
      "grad_norm": 9.157308578491211,
      "learning_rate": 2.4186991869918703e-05,
      "loss": 5.6527,
      "step": 381000
    },
    {
      "epoch": 516.9376693766937,
      "grad_norm": 6.078708171844482,
      "learning_rate": 2.4153116531165313e-05,
      "loss": 5.6028,
      "step": 381500
    },
    {
      "epoch": 517.6151761517615,
      "grad_norm": 7.152129173278809,
      "learning_rate": 2.4119241192411926e-05,
      "loss": 5.6207,
      "step": 382000
    },
    {
      "epoch": 518.2926829268292,
      "grad_norm": 8.833173751831055,
      "learning_rate": 2.4085365853658536e-05,
      "loss": 5.6348,
      "step": 382500
    },
    {
      "epoch": 518.970189701897,
      "grad_norm": 6.446542739868164,
      "learning_rate": 2.405149051490515e-05,
      "loss": 5.6195,
      "step": 383000
    },
    {
      "epoch": 519.6476964769648,
      "grad_norm": 9.283458709716797,
      "learning_rate": 2.4017615176151763e-05,
      "loss": 5.632,
      "step": 383500
    },
    {
      "epoch": 520.3252032520326,
      "grad_norm": 9.311534881591797,
      "learning_rate": 2.3983739837398377e-05,
      "loss": 5.6142,
      "step": 384000
    },
    {
      "epoch": 521.0027100271003,
      "grad_norm": 11.898144721984863,
      "learning_rate": 2.394986449864499e-05,
      "loss": 5.6075,
      "step": 384500
    },
    {
      "epoch": 521.680216802168,
      "grad_norm": 10.379018783569336,
      "learning_rate": 2.39159891598916e-05,
      "loss": 5.6329,
      "step": 385000
    },
    {
      "epoch": 522.3577235772358,
      "grad_norm": 11.097551345825195,
      "learning_rate": 2.3882113821138213e-05,
      "loss": 5.5917,
      "step": 385500
    },
    {
      "epoch": 523.0352303523035,
      "grad_norm": 11.130233764648438,
      "learning_rate": 2.3848238482384823e-05,
      "loss": 5.6526,
      "step": 386000
    },
    {
      "epoch": 523.7127371273713,
      "grad_norm": 13.665587425231934,
      "learning_rate": 2.3814363143631437e-05,
      "loss": 5.6536,
      "step": 386500
    },
    {
      "epoch": 524.390243902439,
      "grad_norm": 7.0631303787231445,
      "learning_rate": 2.378048780487805e-05,
      "loss": 5.6012,
      "step": 387000
    },
    {
      "epoch": 525.0677506775068,
      "grad_norm": 7.525056838989258,
      "learning_rate": 2.3746612466124664e-05,
      "loss": 5.5945,
      "step": 387500
    },
    {
      "epoch": 525.7452574525745,
      "grad_norm": 14.235575675964355,
      "learning_rate": 2.3712737127371277e-05,
      "loss": 5.6254,
      "step": 388000
    },
    {
      "epoch": 526.4227642276422,
      "grad_norm": 7.986546039581299,
      "learning_rate": 2.3678861788617887e-05,
      "loss": 5.627,
      "step": 388500
    },
    {
      "epoch": 527.10027100271,
      "grad_norm": 8.169168472290039,
      "learning_rate": 2.36449864498645e-05,
      "loss": 5.6374,
      "step": 389000
    },
    {
      "epoch": 527.7777777777778,
      "grad_norm": 8.476106643676758,
      "learning_rate": 2.361111111111111e-05,
      "loss": 5.6047,
      "step": 389500
    },
    {
      "epoch": 528.4552845528456,
      "grad_norm": 7.833873271942139,
      "learning_rate": 2.3577235772357724e-05,
      "loss": 5.644,
      "step": 390000
    },
    {
      "epoch": 529.1327913279133,
      "grad_norm": 7.479833602905273,
      "learning_rate": 2.3543360433604337e-05,
      "loss": 5.6101,
      "step": 390500
    },
    {
      "epoch": 529.810298102981,
      "grad_norm": 8.795744895935059,
      "learning_rate": 2.350948509485095e-05,
      "loss": 5.6023,
      "step": 391000
    },
    {
      "epoch": 530.4878048780488,
      "grad_norm": 8.165262222290039,
      "learning_rate": 2.347560975609756e-05,
      "loss": 5.653,
      "step": 391500
    },
    {
      "epoch": 531.1653116531165,
      "grad_norm": 11.273987770080566,
      "learning_rate": 2.3441734417344174e-05,
      "loss": 5.6052,
      "step": 392000
    },
    {
      "epoch": 531.8428184281843,
      "grad_norm": 7.146578788757324,
      "learning_rate": 2.3407859078590788e-05,
      "loss": 5.6209,
      "step": 392500
    },
    {
      "epoch": 532.520325203252,
      "grad_norm": 7.761305332183838,
      "learning_rate": 2.3373983739837398e-05,
      "loss": 5.6286,
      "step": 393000
    },
    {
      "epoch": 533.1978319783198,
      "grad_norm": 8.652149200439453,
      "learning_rate": 2.334010840108401e-05,
      "loss": 5.5843,
      "step": 393500
    },
    {
      "epoch": 533.8753387533875,
      "grad_norm": 7.356147289276123,
      "learning_rate": 2.3306233062330625e-05,
      "loss": 5.6415,
      "step": 394000
    },
    {
      "epoch": 534.5528455284552,
      "grad_norm": 8.94356632232666,
      "learning_rate": 2.3272357723577238e-05,
      "loss": 5.638,
      "step": 394500
    },
    {
      "epoch": 535.2303523035231,
      "grad_norm": 10.195160865783691,
      "learning_rate": 2.3238482384823848e-05,
      "loss": 5.6241,
      "step": 395000
    },
    {
      "epoch": 535.9078590785908,
      "grad_norm": 8.27256965637207,
      "learning_rate": 2.320460704607046e-05,
      "loss": 5.6035,
      "step": 395500
    },
    {
      "epoch": 536.5853658536586,
      "grad_norm": 8.723328590393066,
      "learning_rate": 2.3170731707317075e-05,
      "loss": 5.6132,
      "step": 396000
    },
    {
      "epoch": 537.2628726287263,
      "grad_norm": 8.604212760925293,
      "learning_rate": 2.3136856368563685e-05,
      "loss": 5.63,
      "step": 396500
    },
    {
      "epoch": 537.940379403794,
      "grad_norm": 7.2997145652771,
      "learning_rate": 2.31029810298103e-05,
      "loss": 5.6051,
      "step": 397000
    },
    {
      "epoch": 538.6178861788618,
      "grad_norm": 7.096096515655518,
      "learning_rate": 2.306910569105691e-05,
      "loss": 5.6508,
      "step": 397500
    },
    {
      "epoch": 539.2953929539295,
      "grad_norm": 9.204861640930176,
      "learning_rate": 2.3035230352303525e-05,
      "loss": 5.6045,
      "step": 398000
    },
    {
      "epoch": 539.9728997289973,
      "grad_norm": 6.209732532501221,
      "learning_rate": 2.3001355013550135e-05,
      "loss": 5.61,
      "step": 398500
    },
    {
      "epoch": 540.650406504065,
      "grad_norm": 8.639445304870605,
      "learning_rate": 2.296747967479675e-05,
      "loss": 5.6094,
      "step": 399000
    },
    {
      "epoch": 541.3279132791328,
      "grad_norm": 10.472115516662598,
      "learning_rate": 2.2933604336043362e-05,
      "loss": 5.6675,
      "step": 399500
    },
    {
      "epoch": 542.0054200542005,
      "grad_norm": 9.304813385009766,
      "learning_rate": 2.2899728997289975e-05,
      "loss": 5.5958,
      "step": 400000
    },
    {
      "epoch": 542.6829268292682,
      "grad_norm": 8.5416841506958,
      "learning_rate": 2.286585365853659e-05,
      "loss": 5.6216,
      "step": 400500
    },
    {
      "epoch": 543.3604336043361,
      "grad_norm": 9.594045639038086,
      "learning_rate": 2.28319783197832e-05,
      "loss": 5.6275,
      "step": 401000
    },
    {
      "epoch": 544.0379403794038,
      "grad_norm": 6.870558738708496,
      "learning_rate": 2.2798102981029812e-05,
      "loss": 5.6281,
      "step": 401500
    },
    {
      "epoch": 544.7154471544716,
      "grad_norm": 9.311619758605957,
      "learning_rate": 2.2764227642276422e-05,
      "loss": 5.6218,
      "step": 402000
    },
    {
      "epoch": 545.3929539295393,
      "grad_norm": 7.991934776306152,
      "learning_rate": 2.2730352303523036e-05,
      "loss": 5.6211,
      "step": 402500
    },
    {
      "epoch": 546.0704607046071,
      "grad_norm": 7.15485143661499,
      "learning_rate": 2.269647696476965e-05,
      "loss": 5.6062,
      "step": 403000
    },
    {
      "epoch": 546.7479674796748,
      "grad_norm": 8.254683494567871,
      "learning_rate": 2.2662601626016262e-05,
      "loss": 5.6198,
      "step": 403500
    },
    {
      "epoch": 547.4254742547425,
      "grad_norm": 7.1782941818237305,
      "learning_rate": 2.2628726287262876e-05,
      "loss": 5.6458,
      "step": 404000
    },
    {
      "epoch": 548.1029810298103,
      "grad_norm": 10.954859733581543,
      "learning_rate": 2.2594850948509486e-05,
      "loss": 5.6006,
      "step": 404500
    },
    {
      "epoch": 548.780487804878,
      "grad_norm": 8.251382827758789,
      "learning_rate": 2.25609756097561e-05,
      "loss": 5.6302,
      "step": 405000
    },
    {
      "epoch": 549.4579945799458,
      "grad_norm": 6.455249786376953,
      "learning_rate": 2.252710027100271e-05,
      "loss": 5.6254,
      "step": 405500
    },
    {
      "epoch": 550.1355013550135,
      "grad_norm": 14.382745742797852,
      "learning_rate": 2.2493224932249323e-05,
      "loss": 5.6314,
      "step": 406000
    },
    {
      "epoch": 550.8130081300814,
      "grad_norm": 7.668332099914551,
      "learning_rate": 2.2459349593495936e-05,
      "loss": 5.6054,
      "step": 406500
    },
    {
      "epoch": 551.4905149051491,
      "grad_norm": 7.982634544372559,
      "learning_rate": 2.242547425474255e-05,
      "loss": 5.6079,
      "step": 407000
    },
    {
      "epoch": 552.1680216802168,
      "grad_norm": 7.9047675132751465,
      "learning_rate": 2.2391598915989163e-05,
      "loss": 5.6091,
      "step": 407500
    },
    {
      "epoch": 552.8455284552846,
      "grad_norm": 6.548916816711426,
      "learning_rate": 2.2357723577235773e-05,
      "loss": 5.6323,
      "step": 408000
    },
    {
      "epoch": 553.5230352303523,
      "grad_norm": 8.50401496887207,
      "learning_rate": 2.2323848238482386e-05,
      "loss": 5.6276,
      "step": 408500
    },
    {
      "epoch": 554.2005420054201,
      "grad_norm": 8.09642219543457,
      "learning_rate": 2.2289972899728996e-05,
      "loss": 5.5692,
      "step": 409000
    },
    {
      "epoch": 554.8780487804878,
      "grad_norm": 10.295166969299316,
      "learning_rate": 2.225609756097561e-05,
      "loss": 5.6309,
      "step": 409500
    },
    {
      "epoch": 555.5555555555555,
      "grad_norm": 6.624538898468018,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 5.6488,
      "step": 410000
    },
    {
      "epoch": 556.2330623306233,
      "grad_norm": 9.286982536315918,
      "learning_rate": 2.2188346883468837e-05,
      "loss": 5.5956,
      "step": 410500
    },
    {
      "epoch": 556.910569105691,
      "grad_norm": 12.740503311157227,
      "learning_rate": 2.215447154471545e-05,
      "loss": 5.6276,
      "step": 411000
    },
    {
      "epoch": 557.5880758807588,
      "grad_norm": 8.596744537353516,
      "learning_rate": 2.212059620596206e-05,
      "loss": 5.6342,
      "step": 411500
    },
    {
      "epoch": 558.2655826558265,
      "grad_norm": 11.462056159973145,
      "learning_rate": 2.2086720867208674e-05,
      "loss": 5.6211,
      "step": 412000
    },
    {
      "epoch": 558.9430894308944,
      "grad_norm": 7.781723499298096,
      "learning_rate": 2.2052845528455284e-05,
      "loss": 5.6263,
      "step": 412500
    },
    {
      "epoch": 559.6205962059621,
      "grad_norm": 7.252509593963623,
      "learning_rate": 2.2018970189701897e-05,
      "loss": 5.6089,
      "step": 413000
    },
    {
      "epoch": 560.2981029810298,
      "grad_norm": 7.452907562255859,
      "learning_rate": 2.198509485094851e-05,
      "loss": 5.5988,
      "step": 413500
    },
    {
      "epoch": 560.9756097560976,
      "grad_norm": 7.849555492401123,
      "learning_rate": 2.1951219512195124e-05,
      "loss": 5.6509,
      "step": 414000
    },
    {
      "epoch": 561.6531165311653,
      "grad_norm": 8.2828950881958,
      "learning_rate": 2.1917344173441737e-05,
      "loss": 5.6487,
      "step": 414500
    },
    {
      "epoch": 562.3306233062331,
      "grad_norm": 7.740922451019287,
      "learning_rate": 2.1883468834688347e-05,
      "loss": 5.5955,
      "step": 415000
    },
    {
      "epoch": 563.0081300813008,
      "grad_norm": 8.353001594543457,
      "learning_rate": 2.184959349593496e-05,
      "loss": 5.6189,
      "step": 415500
    },
    {
      "epoch": 563.6856368563685,
      "grad_norm": 11.72700023651123,
      "learning_rate": 2.181571815718157e-05,
      "loss": 5.6158,
      "step": 416000
    },
    {
      "epoch": 564.3631436314363,
      "grad_norm": 7.132308483123779,
      "learning_rate": 2.1781842818428188e-05,
      "loss": 5.6504,
      "step": 416500
    },
    {
      "epoch": 565.040650406504,
      "grad_norm": 8.636921882629395,
      "learning_rate": 2.1747967479674798e-05,
      "loss": 5.5777,
      "step": 417000
    },
    {
      "epoch": 565.7181571815718,
      "grad_norm": 10.555214881896973,
      "learning_rate": 2.171409214092141e-05,
      "loss": 5.6338,
      "step": 417500
    },
    {
      "epoch": 566.3956639566395,
      "grad_norm": 11.548534393310547,
      "learning_rate": 2.1680216802168024e-05,
      "loss": 5.6373,
      "step": 418000
    },
    {
      "epoch": 567.0731707317074,
      "grad_norm": 10.832906723022461,
      "learning_rate": 2.1646341463414634e-05,
      "loss": 5.6076,
      "step": 418500
    },
    {
      "epoch": 567.7506775067751,
      "grad_norm": 10.580251693725586,
      "learning_rate": 2.1612466124661248e-05,
      "loss": 5.62,
      "step": 419000
    },
    {
      "epoch": 568.4281842818428,
      "grad_norm": 6.6232404708862305,
      "learning_rate": 2.157859078590786e-05,
      "loss": 5.62,
      "step": 419500
    },
    {
      "epoch": 569.1056910569106,
      "grad_norm": 8.110249519348145,
      "learning_rate": 2.1544715447154475e-05,
      "loss": 5.6299,
      "step": 420000
    },
    {
      "epoch": 569.7831978319783,
      "grad_norm": 10.955961227416992,
      "learning_rate": 2.1510840108401085e-05,
      "loss": 5.6264,
      "step": 420500
    },
    {
      "epoch": 570.4607046070461,
      "grad_norm": 12.123510360717773,
      "learning_rate": 2.1476964769647698e-05,
      "loss": 5.5552,
      "step": 421000
    },
    {
      "epoch": 571.1382113821138,
      "grad_norm": 8.58526611328125,
      "learning_rate": 2.1443089430894308e-05,
      "loss": 5.6681,
      "step": 421500
    },
    {
      "epoch": 571.8157181571815,
      "grad_norm": 6.461281776428223,
      "learning_rate": 2.140921409214092e-05,
      "loss": 5.6125,
      "step": 422000
    },
    {
      "epoch": 572.4932249322493,
      "grad_norm": 7.811106204986572,
      "learning_rate": 2.1375338753387535e-05,
      "loss": 5.6091,
      "step": 422500
    },
    {
      "epoch": 573.170731707317,
      "grad_norm": 7.0243377685546875,
      "learning_rate": 2.134146341463415e-05,
      "loss": 5.6576,
      "step": 423000
    },
    {
      "epoch": 573.8482384823848,
      "grad_norm": 7.301633834838867,
      "learning_rate": 2.1307588075880762e-05,
      "loss": 5.6106,
      "step": 423500
    },
    {
      "epoch": 574.5257452574526,
      "grad_norm": 9.235163688659668,
      "learning_rate": 2.1273712737127372e-05,
      "loss": 5.6214,
      "step": 424000
    },
    {
      "epoch": 575.2032520325204,
      "grad_norm": 9.544473648071289,
      "learning_rate": 2.1239837398373985e-05,
      "loss": 5.6375,
      "step": 424500
    },
    {
      "epoch": 575.8807588075881,
      "grad_norm": 8.120104789733887,
      "learning_rate": 2.1205962059620595e-05,
      "loss": 5.5994,
      "step": 425000
    },
    {
      "epoch": 576.5582655826558,
      "grad_norm": 12.177764892578125,
      "learning_rate": 2.117208672086721e-05,
      "loss": 5.6301,
      "step": 425500
    },
    {
      "epoch": 577.2357723577236,
      "grad_norm": 6.814171314239502,
      "learning_rate": 2.1138211382113822e-05,
      "loss": 5.5888,
      "step": 426000
    },
    {
      "epoch": 577.9132791327913,
      "grad_norm": 13.300929069519043,
      "learning_rate": 2.1104336043360435e-05,
      "loss": 5.6307,
      "step": 426500
    },
    {
      "epoch": 578.5907859078591,
      "grad_norm": 7.408020973205566,
      "learning_rate": 2.107046070460705e-05,
      "loss": 5.6159,
      "step": 427000
    },
    {
      "epoch": 579.2682926829268,
      "grad_norm": 7.533938407897949,
      "learning_rate": 2.103658536585366e-05,
      "loss": 5.5983,
      "step": 427500
    },
    {
      "epoch": 579.9457994579946,
      "grad_norm": 7.709427356719971,
      "learning_rate": 2.1002710027100272e-05,
      "loss": 5.6347,
      "step": 428000
    },
    {
      "epoch": 580.6233062330623,
      "grad_norm": 8.272014617919922,
      "learning_rate": 2.0968834688346882e-05,
      "loss": 5.6203,
      "step": 428500
    },
    {
      "epoch": 581.30081300813,
      "grad_norm": 7.76689338684082,
      "learning_rate": 2.0934959349593496e-05,
      "loss": 5.5987,
      "step": 429000
    },
    {
      "epoch": 581.9783197831978,
      "grad_norm": 12.891222953796387,
      "learning_rate": 2.090108401084011e-05,
      "loss": 5.6414,
      "step": 429500
    },
    {
      "epoch": 582.6558265582656,
      "grad_norm": 7.321088790893555,
      "learning_rate": 2.0867208672086723e-05,
      "loss": 5.6198,
      "step": 430000
    },
    {
      "epoch": 583.3333333333334,
      "grad_norm": 12.265435218811035,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 5.6051,
      "step": 430500
    },
    {
      "epoch": 584.0108401084011,
      "grad_norm": 9.528738975524902,
      "learning_rate": 2.0799457994579946e-05,
      "loss": 5.6261,
      "step": 431000
    },
    {
      "epoch": 584.6883468834689,
      "grad_norm": 8.366275787353516,
      "learning_rate": 2.076558265582656e-05,
      "loss": 5.6378,
      "step": 431500
    },
    {
      "epoch": 585.3658536585366,
      "grad_norm": 9.835343360900879,
      "learning_rate": 2.073170731707317e-05,
      "loss": 5.5708,
      "step": 432000
    },
    {
      "epoch": 586.0433604336043,
      "grad_norm": 8.76284122467041,
      "learning_rate": 2.0697831978319783e-05,
      "loss": 5.6517,
      "step": 432500
    },
    {
      "epoch": 586.7208672086721,
      "grad_norm": 7.707040786743164,
      "learning_rate": 2.0663956639566396e-05,
      "loss": 5.6029,
      "step": 433000
    },
    {
      "epoch": 587.3983739837398,
      "grad_norm": 5.976442337036133,
      "learning_rate": 2.063008130081301e-05,
      "loss": 5.6501,
      "step": 433500
    },
    {
      "epoch": 588.0758807588076,
      "grad_norm": 7.320681571960449,
      "learning_rate": 2.0596205962059623e-05,
      "loss": 5.6121,
      "step": 434000
    },
    {
      "epoch": 588.7533875338753,
      "grad_norm": 9.311552047729492,
      "learning_rate": 2.0562330623306233e-05,
      "loss": 5.6084,
      "step": 434500
    },
    {
      "epoch": 589.430894308943,
      "grad_norm": 12.792823791503906,
      "learning_rate": 2.0528455284552847e-05,
      "loss": 5.6322,
      "step": 435000
    },
    {
      "epoch": 590.1084010840109,
      "grad_norm": 8.275114059448242,
      "learning_rate": 2.0494579945799457e-05,
      "loss": 5.5995,
      "step": 435500
    },
    {
      "epoch": 590.7859078590786,
      "grad_norm": 13.597760200500488,
      "learning_rate": 2.046070460704607e-05,
      "loss": 5.6121,
      "step": 436000
    },
    {
      "epoch": 591.4634146341464,
      "grad_norm": 8.54489517211914,
      "learning_rate": 2.0426829268292683e-05,
      "loss": 5.6233,
      "step": 436500
    },
    {
      "epoch": 592.1409214092141,
      "grad_norm": 11.918601036071777,
      "learning_rate": 2.0392953929539297e-05,
      "loss": 5.6273,
      "step": 437000
    },
    {
      "epoch": 592.8184281842819,
      "grad_norm": 9.543401718139648,
      "learning_rate": 2.035907859078591e-05,
      "loss": 5.5976,
      "step": 437500
    },
    {
      "epoch": 593.4959349593496,
      "grad_norm": 9.34278392791748,
      "learning_rate": 2.032520325203252e-05,
      "loss": 5.6549,
      "step": 438000
    },
    {
      "epoch": 594.1734417344173,
      "grad_norm": 7.939967155456543,
      "learning_rate": 2.0291327913279134e-05,
      "loss": 5.579,
      "step": 438500
    },
    {
      "epoch": 594.8509485094851,
      "grad_norm": 7.97133731842041,
      "learning_rate": 2.0257452574525744e-05,
      "loss": 5.6429,
      "step": 439000
    },
    {
      "epoch": 595.5284552845528,
      "grad_norm": 7.611886501312256,
      "learning_rate": 2.022357723577236e-05,
      "loss": 5.6061,
      "step": 439500
    },
    {
      "epoch": 596.2059620596206,
      "grad_norm": 7.522385597229004,
      "learning_rate": 2.018970189701897e-05,
      "loss": 5.6137,
      "step": 440000
    },
    {
      "epoch": 596.8834688346883,
      "grad_norm": 7.047245025634766,
      "learning_rate": 2.0155826558265584e-05,
      "loss": 5.6361,
      "step": 440500
    },
    {
      "epoch": 597.560975609756,
      "grad_norm": 9.2984619140625,
      "learning_rate": 2.0121951219512197e-05,
      "loss": 5.6346,
      "step": 441000
    },
    {
      "epoch": 598.2384823848239,
      "grad_norm": 10.796191215515137,
      "learning_rate": 2.0088075880758807e-05,
      "loss": 5.5983,
      "step": 441500
    },
    {
      "epoch": 598.9159891598916,
      "grad_norm": 6.60941743850708,
      "learning_rate": 2.005420054200542e-05,
      "loss": 5.6258,
      "step": 442000
    },
    {
      "epoch": 599.5934959349594,
      "grad_norm": 6.790829181671143,
      "learning_rate": 2.0020325203252034e-05,
      "loss": 5.6475,
      "step": 442500
    },
    {
      "epoch": 600.2710027100271,
      "grad_norm": 7.584039688110352,
      "learning_rate": 1.9986449864498648e-05,
      "loss": 5.5827,
      "step": 443000
    },
    {
      "epoch": 600.9485094850949,
      "grad_norm": 9.01370620727539,
      "learning_rate": 1.9952574525745258e-05,
      "loss": 5.622,
      "step": 443500
    },
    {
      "epoch": 601.6260162601626,
      "grad_norm": 7.911624908447266,
      "learning_rate": 1.991869918699187e-05,
      "loss": 5.6385,
      "step": 444000
    },
    {
      "epoch": 602.3035230352303,
      "grad_norm": 8.450439453125,
      "learning_rate": 1.9884823848238485e-05,
      "loss": 5.5906,
      "step": 444500
    },
    {
      "epoch": 602.9810298102981,
      "grad_norm": 9.675431251525879,
      "learning_rate": 1.9850948509485095e-05,
      "loss": 5.627,
      "step": 445000
    },
    {
      "epoch": 603.6585365853658,
      "grad_norm": 7.844006061553955,
      "learning_rate": 1.9817073170731708e-05,
      "loss": 5.5961,
      "step": 445500
    },
    {
      "epoch": 604.3360433604336,
      "grad_norm": 12.258044242858887,
      "learning_rate": 1.978319783197832e-05,
      "loss": 5.6431,
      "step": 446000
    },
    {
      "epoch": 605.0135501355013,
      "grad_norm": 7.7884039878845215,
      "learning_rate": 1.9749322493224935e-05,
      "loss": 5.6235,
      "step": 446500
    },
    {
      "epoch": 605.6910569105692,
      "grad_norm": 9.954126358032227,
      "learning_rate": 1.9715447154471545e-05,
      "loss": 5.6093,
      "step": 447000
    },
    {
      "epoch": 606.3685636856369,
      "grad_norm": 10.874489784240723,
      "learning_rate": 1.9681571815718158e-05,
      "loss": 5.6118,
      "step": 447500
    },
    {
      "epoch": 607.0460704607046,
      "grad_norm": 9.200199127197266,
      "learning_rate": 1.9647696476964768e-05,
      "loss": 5.6168,
      "step": 448000
    },
    {
      "epoch": 607.7235772357724,
      "grad_norm": 9.912965774536133,
      "learning_rate": 1.961382113821138e-05,
      "loss": 5.6212,
      "step": 448500
    },
    {
      "epoch": 608.4010840108401,
      "grad_norm": 8.530950546264648,
      "learning_rate": 1.9579945799457995e-05,
      "loss": 5.6023,
      "step": 449000
    },
    {
      "epoch": 609.0785907859079,
      "grad_norm": 9.248536109924316,
      "learning_rate": 1.954607046070461e-05,
      "loss": 5.6317,
      "step": 449500
    },
    {
      "epoch": 609.7560975609756,
      "grad_norm": 9.209789276123047,
      "learning_rate": 1.9512195121951222e-05,
      "loss": 5.6334,
      "step": 450000
    },
    {
      "epoch": 610.4336043360433,
      "grad_norm": 6.108071804046631,
      "learning_rate": 1.9478319783197832e-05,
      "loss": 5.6088,
      "step": 450500
    },
    {
      "epoch": 611.1111111111111,
      "grad_norm": 11.71129322052002,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 5.6313,
      "step": 451000
    },
    {
      "epoch": 611.7886178861788,
      "grad_norm": 8.517899513244629,
      "learning_rate": 1.9410569105691055e-05,
      "loss": 5.6099,
      "step": 451500
    },
    {
      "epoch": 612.4661246612466,
      "grad_norm": 7.217385292053223,
      "learning_rate": 1.937669376693767e-05,
      "loss": 5.5948,
      "step": 452000
    },
    {
      "epoch": 613.1436314363143,
      "grad_norm": 8.161787986755371,
      "learning_rate": 1.9342818428184282e-05,
      "loss": 5.6272,
      "step": 452500
    },
    {
      "epoch": 613.8211382113822,
      "grad_norm": 7.441249847412109,
      "learning_rate": 1.9308943089430896e-05,
      "loss": 5.6428,
      "step": 453000
    },
    {
      "epoch": 614.4986449864499,
      "grad_norm": 7.722768783569336,
      "learning_rate": 1.927506775067751e-05,
      "loss": 5.6116,
      "step": 453500
    },
    {
      "epoch": 615.1761517615176,
      "grad_norm": 9.815072059631348,
      "learning_rate": 1.924119241192412e-05,
      "loss": 5.6142,
      "step": 454000
    },
    {
      "epoch": 615.8536585365854,
      "grad_norm": 9.154939651489258,
      "learning_rate": 1.9207317073170733e-05,
      "loss": 5.608,
      "step": 454500
    },
    {
      "epoch": 616.5311653116531,
      "grad_norm": 9.0426025390625,
      "learning_rate": 1.9173441734417343e-05,
      "loss": 5.6234,
      "step": 455000
    },
    {
      "epoch": 617.2086720867209,
      "grad_norm": 11.703999519348145,
      "learning_rate": 1.9139566395663956e-05,
      "loss": 5.6015,
      "step": 455500
    },
    {
      "epoch": 617.8861788617886,
      "grad_norm": 12.329620361328125,
      "learning_rate": 1.9105691056910573e-05,
      "loss": 5.6198,
      "step": 456000
    },
    {
      "epoch": 618.5636856368563,
      "grad_norm": 7.078120708465576,
      "learning_rate": 1.9071815718157183e-05,
      "loss": 5.6255,
      "step": 456500
    },
    {
      "epoch": 619.2411924119241,
      "grad_norm": 7.505267143249512,
      "learning_rate": 1.9037940379403796e-05,
      "loss": 5.6393,
      "step": 457000
    },
    {
      "epoch": 619.9186991869918,
      "grad_norm": 6.508934497833252,
      "learning_rate": 1.9004065040650406e-05,
      "loss": 5.5905,
      "step": 457500
    },
    {
      "epoch": 620.5962059620596,
      "grad_norm": 10.17088794708252,
      "learning_rate": 1.897018970189702e-05,
      "loss": 5.6415,
      "step": 458000
    },
    {
      "epoch": 621.2737127371274,
      "grad_norm": 8.493393898010254,
      "learning_rate": 1.893631436314363e-05,
      "loss": 5.5888,
      "step": 458500
    },
    {
      "epoch": 621.9512195121952,
      "grad_norm": 8.451861381530762,
      "learning_rate": 1.8902439024390246e-05,
      "loss": 5.6126,
      "step": 459000
    },
    {
      "epoch": 622.6287262872629,
      "grad_norm": 13.19775390625,
      "learning_rate": 1.886856368563686e-05,
      "loss": 5.6224,
      "step": 459500
    },
    {
      "epoch": 623.3062330623306,
      "grad_norm": 8.650101661682129,
      "learning_rate": 1.883468834688347e-05,
      "loss": 5.5881,
      "step": 460000
    },
    {
      "epoch": 623.9837398373984,
      "grad_norm": 9.849757194519043,
      "learning_rate": 1.8800813008130083e-05,
      "loss": 5.6494,
      "step": 460500
    },
    {
      "epoch": 624.6612466124661,
      "grad_norm": 8.696847915649414,
      "learning_rate": 1.8766937669376693e-05,
      "loss": 5.5937,
      "step": 461000
    },
    {
      "epoch": 625.3387533875339,
      "grad_norm": 7.144309997558594,
      "learning_rate": 1.8733062330623307e-05,
      "loss": 5.6391,
      "step": 461500
    },
    {
      "epoch": 626.0162601626016,
      "grad_norm": 9.214144706726074,
      "learning_rate": 1.869918699186992e-05,
      "loss": 5.6237,
      "step": 462000
    },
    {
      "epoch": 626.6937669376694,
      "grad_norm": 9.492202758789062,
      "learning_rate": 1.8665311653116534e-05,
      "loss": 5.6044,
      "step": 462500
    },
    {
      "epoch": 627.3712737127371,
      "grad_norm": 10.621522903442383,
      "learning_rate": 1.8631436314363144e-05,
      "loss": 5.6203,
      "step": 463000
    },
    {
      "epoch": 628.0487804878048,
      "grad_norm": 6.523447513580322,
      "learning_rate": 1.8597560975609757e-05,
      "loss": 5.6113,
      "step": 463500
    },
    {
      "epoch": 628.7262872628726,
      "grad_norm": 10.405890464782715,
      "learning_rate": 1.856368563685637e-05,
      "loss": 5.6327,
      "step": 464000
    },
    {
      "epoch": 629.4037940379404,
      "grad_norm": 10.820318222045898,
      "learning_rate": 1.852981029810298e-05,
      "loss": 5.5737,
      "step": 464500
    },
    {
      "epoch": 630.0813008130082,
      "grad_norm": 7.745973587036133,
      "learning_rate": 1.8495934959349594e-05,
      "loss": 5.6645,
      "step": 465000
    },
    {
      "epoch": 630.7588075880759,
      "grad_norm": 11.715062141418457,
      "learning_rate": 1.8462059620596207e-05,
      "loss": 5.6043,
      "step": 465500
    },
    {
      "epoch": 631.4363143631437,
      "grad_norm": 7.164778709411621,
      "learning_rate": 1.842818428184282e-05,
      "loss": 5.6109,
      "step": 466000
    },
    {
      "epoch": 632.1138211382114,
      "grad_norm": 7.869643688201904,
      "learning_rate": 1.839430894308943e-05,
      "loss": 5.6163,
      "step": 466500
    },
    {
      "epoch": 632.7913279132791,
      "grad_norm": 10.053811073303223,
      "learning_rate": 1.8360433604336044e-05,
      "loss": 5.6184,
      "step": 467000
    },
    {
      "epoch": 633.4688346883469,
      "grad_norm": 7.602544784545898,
      "learning_rate": 1.8326558265582658e-05,
      "loss": 5.5916,
      "step": 467500
    },
    {
      "epoch": 634.1463414634146,
      "grad_norm": 8.901407241821289,
      "learning_rate": 1.8292682926829268e-05,
      "loss": 5.6426,
      "step": 468000
    },
    {
      "epoch": 634.8238482384824,
      "grad_norm": 8.347006797790527,
      "learning_rate": 1.825880758807588e-05,
      "loss": 5.607,
      "step": 468500
    },
    {
      "epoch": 635.5013550135501,
      "grad_norm": 8.646784782409668,
      "learning_rate": 1.8224932249322494e-05,
      "loss": 5.6141,
      "step": 469000
    },
    {
      "epoch": 636.1788617886178,
      "grad_norm": 7.2454023361206055,
      "learning_rate": 1.8191056910569108e-05,
      "loss": 5.6182,
      "step": 469500
    },
    {
      "epoch": 636.8563685636857,
      "grad_norm": 10.902389526367188,
      "learning_rate": 1.8157181571815718e-05,
      "loss": 5.6149,
      "step": 470000
    },
    {
      "epoch": 637.5338753387534,
      "grad_norm": 7.121861934661865,
      "learning_rate": 1.812330623306233e-05,
      "loss": 5.5934,
      "step": 470500
    },
    {
      "epoch": 638.2113821138212,
      "grad_norm": 9.319377899169922,
      "learning_rate": 1.8089430894308945e-05,
      "loss": 5.625,
      "step": 471000
    },
    {
      "epoch": 638.8888888888889,
      "grad_norm": 11.547033309936523,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 5.6058,
      "step": 471500
    },
    {
      "epoch": 639.5663956639567,
      "grad_norm": 13.312819480895996,
      "learning_rate": 1.8021680216802168e-05,
      "loss": 5.6019,
      "step": 472000
    },
    {
      "epoch": 640.2439024390244,
      "grad_norm": 12.209074974060059,
      "learning_rate": 1.798780487804878e-05,
      "loss": 5.6396,
      "step": 472500
    },
    {
      "epoch": 640.9214092140921,
      "grad_norm": 10.1730375289917,
      "learning_rate": 1.7953929539295395e-05,
      "loss": 5.6138,
      "step": 473000
    },
    {
      "epoch": 641.5989159891599,
      "grad_norm": 16.92727279663086,
      "learning_rate": 1.7920054200542005e-05,
      "loss": 5.6126,
      "step": 473500
    },
    {
      "epoch": 642.2764227642276,
      "grad_norm": 7.931230068206787,
      "learning_rate": 1.788617886178862e-05,
      "loss": 5.608,
      "step": 474000
    },
    {
      "epoch": 642.9539295392954,
      "grad_norm": 14.35588264465332,
      "learning_rate": 1.7852303523035232e-05,
      "loss": 5.6225,
      "step": 474500
    },
    {
      "epoch": 643.6314363143631,
      "grad_norm": 10.839228630065918,
      "learning_rate": 1.7818428184281842e-05,
      "loss": 5.5963,
      "step": 475000
    },
    {
      "epoch": 644.3089430894308,
      "grad_norm": 11.506475448608398,
      "learning_rate": 1.778455284552846e-05,
      "loss": 5.6433,
      "step": 475500
    },
    {
      "epoch": 644.9864498644987,
      "grad_norm": 7.34269380569458,
      "learning_rate": 1.775067750677507e-05,
      "loss": 5.6098,
      "step": 476000
    },
    {
      "epoch": 645.6639566395664,
      "grad_norm": 9.793448448181152,
      "learning_rate": 1.7716802168021682e-05,
      "loss": 5.6314,
      "step": 476500
    },
    {
      "epoch": 646.3414634146342,
      "grad_norm": 8.539780616760254,
      "learning_rate": 1.7682926829268292e-05,
      "loss": 5.6003,
      "step": 477000
    },
    {
      "epoch": 647.0189701897019,
      "grad_norm": 11.299424171447754,
      "learning_rate": 1.7649051490514906e-05,
      "loss": 5.6019,
      "step": 477500
    },
    {
      "epoch": 647.6964769647697,
      "grad_norm": 12.35637378692627,
      "learning_rate": 1.7615176151761516e-05,
      "loss": 5.6115,
      "step": 478000
    },
    {
      "epoch": 648.3739837398374,
      "grad_norm": 15.741621971130371,
      "learning_rate": 1.7581300813008132e-05,
      "loss": 5.6367,
      "step": 478500
    },
    {
      "epoch": 649.0514905149051,
      "grad_norm": 12.714812278747559,
      "learning_rate": 1.7547425474254746e-05,
      "loss": 5.5992,
      "step": 479000
    },
    {
      "epoch": 649.7289972899729,
      "grad_norm": 6.106247425079346,
      "learning_rate": 1.7513550135501356e-05,
      "loss": 5.6038,
      "step": 479500
    },
    {
      "epoch": 650.4065040650406,
      "grad_norm": 9.355072975158691,
      "learning_rate": 1.747967479674797e-05,
      "loss": 5.6623,
      "step": 480000
    },
    {
      "epoch": 651.0840108401084,
      "grad_norm": 7.954119682312012,
      "learning_rate": 1.744579945799458e-05,
      "loss": 5.6004,
      "step": 480500
    },
    {
      "epoch": 651.7615176151761,
      "grad_norm": 5.527002811431885,
      "learning_rate": 1.7411924119241193e-05,
      "loss": 5.5962,
      "step": 481000
    },
    {
      "epoch": 652.439024390244,
      "grad_norm": 8.69821548461914,
      "learning_rate": 1.7378048780487806e-05,
      "loss": 5.6577,
      "step": 481500
    },
    {
      "epoch": 653.1165311653117,
      "grad_norm": 10.339865684509277,
      "learning_rate": 1.734417344173442e-05,
      "loss": 5.5992,
      "step": 482000
    },
    {
      "epoch": 653.7940379403794,
      "grad_norm": 9.115405082702637,
      "learning_rate": 1.7310298102981033e-05,
      "loss": 5.6012,
      "step": 482500
    },
    {
      "epoch": 654.4715447154472,
      "grad_norm": 10.94769287109375,
      "learning_rate": 1.7276422764227643e-05,
      "loss": 5.5868,
      "step": 483000
    },
    {
      "epoch": 655.1490514905149,
      "grad_norm": 8.069833755493164,
      "learning_rate": 1.7242547425474256e-05,
      "loss": 5.6568,
      "step": 483500
    },
    {
      "epoch": 655.8265582655827,
      "grad_norm": 10.729886054992676,
      "learning_rate": 1.7208672086720866e-05,
      "loss": 5.5952,
      "step": 484000
    },
    {
      "epoch": 656.5040650406504,
      "grad_norm": 9.78488826751709,
      "learning_rate": 1.717479674796748e-05,
      "loss": 5.6179,
      "step": 484500
    },
    {
      "epoch": 657.1815718157181,
      "grad_norm": 7.304262161254883,
      "learning_rate": 1.7140921409214093e-05,
      "loss": 5.6171,
      "step": 485000
    },
    {
      "epoch": 657.8590785907859,
      "grad_norm": 7.464112758636475,
      "learning_rate": 1.7107046070460707e-05,
      "loss": 5.6115,
      "step": 485500
    },
    {
      "epoch": 658.5365853658536,
      "grad_norm": 18.76057243347168,
      "learning_rate": 1.707317073170732e-05,
      "loss": 5.6197,
      "step": 486000
    },
    {
      "epoch": 659.2140921409214,
      "grad_norm": 10.960927963256836,
      "learning_rate": 1.703929539295393e-05,
      "loss": 5.628,
      "step": 486500
    },
    {
      "epoch": 659.8915989159891,
      "grad_norm": 6.93999719619751,
      "learning_rate": 1.7005420054200543e-05,
      "loss": 5.6081,
      "step": 487000
    },
    {
      "epoch": 660.569105691057,
      "grad_norm": 9.09189224243164,
      "learning_rate": 1.6971544715447154e-05,
      "loss": 5.6311,
      "step": 487500
    },
    {
      "epoch": 661.2466124661247,
      "grad_norm": 6.328500747680664,
      "learning_rate": 1.6937669376693767e-05,
      "loss": 5.6299,
      "step": 488000
    },
    {
      "epoch": 661.9241192411924,
      "grad_norm": 8.77177619934082,
      "learning_rate": 1.690379403794038e-05,
      "loss": 5.5975,
      "step": 488500
    },
    {
      "epoch": 662.6016260162602,
      "grad_norm": 10.410411834716797,
      "learning_rate": 1.6869918699186994e-05,
      "loss": 5.6109,
      "step": 489000
    },
    {
      "epoch": 663.2791327913279,
      "grad_norm": 10.702404022216797,
      "learning_rate": 1.6836043360433607e-05,
      "loss": 5.6446,
      "step": 489500
    },
    {
      "epoch": 663.9566395663957,
      "grad_norm": 7.560273170471191,
      "learning_rate": 1.6802168021680217e-05,
      "loss": 5.6012,
      "step": 490000
    },
    {
      "epoch": 664.6341463414634,
      "grad_norm": 6.661181926727295,
      "learning_rate": 1.676829268292683e-05,
      "loss": 5.5996,
      "step": 490500
    },
    {
      "epoch": 665.3116531165311,
      "grad_norm": 7.198784351348877,
      "learning_rate": 1.673441734417344e-05,
      "loss": 5.6142,
      "step": 491000
    },
    {
      "epoch": 665.9891598915989,
      "grad_norm": 8.743172645568848,
      "learning_rate": 1.6700542005420054e-05,
      "loss": 5.6277,
      "step": 491500
    },
    {
      "epoch": 666.6666666666666,
      "grad_norm": 8.96346664428711,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 5.602,
      "step": 492000
    },
    {
      "epoch": 667.3441734417344,
      "grad_norm": 6.236313343048096,
      "learning_rate": 1.663279132791328e-05,
      "loss": 5.628,
      "step": 492500
    },
    {
      "epoch": 668.0216802168022,
      "grad_norm": 9.973054885864258,
      "learning_rate": 1.659891598915989e-05,
      "loss": 5.6288,
      "step": 493000
    },
    {
      "epoch": 668.69918699187,
      "grad_norm": 7.904259204864502,
      "learning_rate": 1.6565040650406504e-05,
      "loss": 5.5991,
      "step": 493500
    },
    {
      "epoch": 669.3766937669377,
      "grad_norm": 11.216300964355469,
      "learning_rate": 1.6531165311653118e-05,
      "loss": 5.6338,
      "step": 494000
    },
    {
      "epoch": 670.0542005420054,
      "grad_norm": 10.021474838256836,
      "learning_rate": 1.6497289972899728e-05,
      "loss": 5.6064,
      "step": 494500
    },
    {
      "epoch": 670.7317073170732,
      "grad_norm": 11.042158126831055,
      "learning_rate": 1.6463414634146345e-05,
      "loss": 5.601,
      "step": 495000
    },
    {
      "epoch": 671.4092140921409,
      "grad_norm": 8.348949432373047,
      "learning_rate": 1.6429539295392955e-05,
      "loss": 5.6356,
      "step": 495500
    },
    {
      "epoch": 672.0867208672087,
      "grad_norm": 9.967007637023926,
      "learning_rate": 1.6395663956639568e-05,
      "loss": 5.6273,
      "step": 496000
    },
    {
      "epoch": 672.7642276422764,
      "grad_norm": 7.153550148010254,
      "learning_rate": 1.6361788617886178e-05,
      "loss": 5.5908,
      "step": 496500
    },
    {
      "epoch": 673.4417344173442,
      "grad_norm": 7.4922943115234375,
      "learning_rate": 1.632791327913279e-05,
      "loss": 5.6172,
      "step": 497000
    },
    {
      "epoch": 674.1192411924119,
      "grad_norm": 7.983096122741699,
      "learning_rate": 1.6294037940379405e-05,
      "loss": 5.633,
      "step": 497500
    },
    {
      "epoch": 674.7967479674796,
      "grad_norm": 7.90873384475708,
      "learning_rate": 1.6260162601626018e-05,
      "loss": 5.6248,
      "step": 498000
    },
    {
      "epoch": 675.4742547425474,
      "grad_norm": 7.807007312774658,
      "learning_rate": 1.622628726287263e-05,
      "loss": 5.6288,
      "step": 498500
    },
    {
      "epoch": 676.1517615176152,
      "grad_norm": 9.994937896728516,
      "learning_rate": 1.6192411924119242e-05,
      "loss": 5.6138,
      "step": 499000
    },
    {
      "epoch": 676.829268292683,
      "grad_norm": 8.177574157714844,
      "learning_rate": 1.6158536585365855e-05,
      "loss": 5.6239,
      "step": 499500
    },
    {
      "epoch": 677.5067750677507,
      "grad_norm": 7.408725738525391,
      "learning_rate": 1.6124661246612465e-05,
      "loss": 5.6223,
      "step": 500000
    },
    {
      "epoch": 678.1842818428185,
      "grad_norm": 9.164578437805176,
      "learning_rate": 1.609078590785908e-05,
      "loss": 5.5988,
      "step": 500500
    },
    {
      "epoch": 678.8617886178862,
      "grad_norm": 11.090773582458496,
      "learning_rate": 1.6056910569105692e-05,
      "loss": 5.6224,
      "step": 501000
    },
    {
      "epoch": 679.5392953929539,
      "grad_norm": 7.139897346496582,
      "learning_rate": 1.6023035230352305e-05,
      "loss": 5.6133,
      "step": 501500
    },
    {
      "epoch": 680.2168021680217,
      "grad_norm": 8.769262313842773,
      "learning_rate": 1.598915989159892e-05,
      "loss": 5.601,
      "step": 502000
    },
    {
      "epoch": 680.8943089430894,
      "grad_norm": 9.159590721130371,
      "learning_rate": 1.595528455284553e-05,
      "loss": 5.6108,
      "step": 502500
    },
    {
      "epoch": 681.5718157181572,
      "grad_norm": 9.453792572021484,
      "learning_rate": 1.5921409214092142e-05,
      "loss": 5.6153,
      "step": 503000
    },
    {
      "epoch": 682.2493224932249,
      "grad_norm": 9.046107292175293,
      "learning_rate": 1.5887533875338752e-05,
      "loss": 5.6274,
      "step": 503500
    },
    {
      "epoch": 682.9268292682926,
      "grad_norm": 9.676170349121094,
      "learning_rate": 1.5853658536585366e-05,
      "loss": 5.6116,
      "step": 504000
    },
    {
      "epoch": 683.6043360433605,
      "grad_norm": 6.875730037689209,
      "learning_rate": 1.581978319783198e-05,
      "loss": 5.6215,
      "step": 504500
    },
    {
      "epoch": 684.2818428184282,
      "grad_norm": 7.199488162994385,
      "learning_rate": 1.5785907859078593e-05,
      "loss": 5.5881,
      "step": 505000
    },
    {
      "epoch": 684.959349593496,
      "grad_norm": 10.353832244873047,
      "learning_rate": 1.5752032520325206e-05,
      "loss": 5.6228,
      "step": 505500
    },
    {
      "epoch": 685.6368563685637,
      "grad_norm": 8.053644180297852,
      "learning_rate": 1.5718157181571816e-05,
      "loss": 5.595,
      "step": 506000
    },
    {
      "epoch": 686.3143631436315,
      "grad_norm": 6.759035110473633,
      "learning_rate": 1.568428184281843e-05,
      "loss": 5.6653,
      "step": 506500
    },
    {
      "epoch": 686.9918699186992,
      "grad_norm": 9.443228721618652,
      "learning_rate": 1.565040650406504e-05,
      "loss": 5.5861,
      "step": 507000
    },
    {
      "epoch": 687.6693766937669,
      "grad_norm": 11.311251640319824,
      "learning_rate": 1.5616531165311653e-05,
      "loss": 5.6277,
      "step": 507500
    },
    {
      "epoch": 688.3468834688347,
      "grad_norm": 13.500197410583496,
      "learning_rate": 1.5582655826558266e-05,
      "loss": 5.6058,
      "step": 508000
    },
    {
      "epoch": 689.0243902439024,
      "grad_norm": 11.986943244934082,
      "learning_rate": 1.554878048780488e-05,
      "loss": 5.6055,
      "step": 508500
    },
    {
      "epoch": 689.7018970189702,
      "grad_norm": 6.4975152015686035,
      "learning_rate": 1.5514905149051493e-05,
      "loss": 5.6037,
      "step": 509000
    },
    {
      "epoch": 690.3794037940379,
      "grad_norm": 5.350210666656494,
      "learning_rate": 1.5481029810298103e-05,
      "loss": 5.5964,
      "step": 509500
    },
    {
      "epoch": 691.0569105691056,
      "grad_norm": 7.4379072189331055,
      "learning_rate": 1.5447154471544717e-05,
      "loss": 5.6519,
      "step": 510000
    },
    {
      "epoch": 691.7344173441735,
      "grad_norm": 9.579365730285645,
      "learning_rate": 1.5413279132791327e-05,
      "loss": 5.6156,
      "step": 510500
    },
    {
      "epoch": 692.4119241192412,
      "grad_norm": 8.755486488342285,
      "learning_rate": 1.537940379403794e-05,
      "loss": 5.605,
      "step": 511000
    },
    {
      "epoch": 693.089430894309,
      "grad_norm": 9.556168556213379,
      "learning_rate": 1.5345528455284553e-05,
      "loss": 5.6162,
      "step": 511500
    },
    {
      "epoch": 693.7669376693767,
      "grad_norm": 10.254982948303223,
      "learning_rate": 1.5311653116531167e-05,
      "loss": 5.6093,
      "step": 512000
    },
    {
      "epoch": 694.4444444444445,
      "grad_norm": 12.460321426391602,
      "learning_rate": 1.527777777777778e-05,
      "loss": 5.6353,
      "step": 512500
    },
    {
      "epoch": 695.1219512195122,
      "grad_norm": 6.929091930389404,
      "learning_rate": 1.524390243902439e-05,
      "loss": 5.632,
      "step": 513000
    },
    {
      "epoch": 695.7994579945799,
      "grad_norm": 8.71640682220459,
      "learning_rate": 1.5210027100271004e-05,
      "loss": 5.6106,
      "step": 513500
    },
    {
      "epoch": 696.4769647696477,
      "grad_norm": 7.901922225952148,
      "learning_rate": 1.5176151761517615e-05,
      "loss": 5.6461,
      "step": 514000
    },
    {
      "epoch": 697.1544715447154,
      "grad_norm": 8.113521575927734,
      "learning_rate": 1.5142276422764229e-05,
      "loss": 5.5769,
      "step": 514500
    },
    {
      "epoch": 697.8319783197832,
      "grad_norm": 9.043577194213867,
      "learning_rate": 1.510840108401084e-05,
      "loss": 5.6206,
      "step": 515000
    },
    {
      "epoch": 698.5094850948509,
      "grad_norm": 11.770359992980957,
      "learning_rate": 1.5074525745257454e-05,
      "loss": 5.626,
      "step": 515500
    },
    {
      "epoch": 699.1869918699186,
      "grad_norm": 9.019120216369629,
      "learning_rate": 1.5040650406504067e-05,
      "loss": 5.6141,
      "step": 516000
    },
    {
      "epoch": 699.8644986449865,
      "grad_norm": 9.634505271911621,
      "learning_rate": 1.5006775067750677e-05,
      "loss": 5.588,
      "step": 516500
    },
    {
      "epoch": 700.5420054200542,
      "grad_norm": 9.43167781829834,
      "learning_rate": 1.4972899728997292e-05,
      "loss": 5.6218,
      "step": 517000
    },
    {
      "epoch": 701.219512195122,
      "grad_norm": 8.3776216506958,
      "learning_rate": 1.4939024390243902e-05,
      "loss": 5.6154,
      "step": 517500
    },
    {
      "epoch": 701.8970189701897,
      "grad_norm": 11.332594871520996,
      "learning_rate": 1.4905149051490516e-05,
      "loss": 5.6446,
      "step": 518000
    },
    {
      "epoch": 702.5745257452575,
      "grad_norm": 9.507049560546875,
      "learning_rate": 1.4871273712737128e-05,
      "loss": 5.5897,
      "step": 518500
    },
    {
      "epoch": 703.2520325203252,
      "grad_norm": 8.555948257446289,
      "learning_rate": 1.4837398373983741e-05,
      "loss": 5.5936,
      "step": 519000
    },
    {
      "epoch": 703.9295392953929,
      "grad_norm": 8.04575252532959,
      "learning_rate": 1.4803523035230351e-05,
      "loss": 5.646,
      "step": 519500
    },
    {
      "epoch": 704.6070460704607,
      "grad_norm": 11.599825859069824,
      "learning_rate": 1.4769647696476966e-05,
      "loss": 5.6202,
      "step": 520000
    },
    {
      "epoch": 705.2845528455284,
      "grad_norm": 7.899038314819336,
      "learning_rate": 1.473577235772358e-05,
      "loss": 5.5762,
      "step": 520500
    },
    {
      "epoch": 705.9620596205962,
      "grad_norm": 8.904762268066406,
      "learning_rate": 1.470189701897019e-05,
      "loss": 5.64,
      "step": 521000
    },
    {
      "epoch": 706.6395663956639,
      "grad_norm": 8.118498802185059,
      "learning_rate": 1.4668021680216803e-05,
      "loss": 5.6377,
      "step": 521500
    },
    {
      "epoch": 707.3170731707318,
      "grad_norm": 11.456326484680176,
      "learning_rate": 1.4634146341463415e-05,
      "loss": 5.6009,
      "step": 522000
    },
    {
      "epoch": 707.9945799457995,
      "grad_norm": 8.686372756958008,
      "learning_rate": 1.4600271002710028e-05,
      "loss": 5.6149,
      "step": 522500
    },
    {
      "epoch": 708.6720867208672,
      "grad_norm": 12.73654556274414,
      "learning_rate": 1.4566395663956638e-05,
      "loss": 5.6193,
      "step": 523000
    },
    {
      "epoch": 709.349593495935,
      "grad_norm": 7.506337642669678,
      "learning_rate": 1.4532520325203253e-05,
      "loss": 5.5911,
      "step": 523500
    },
    {
      "epoch": 710.0271002710027,
      "grad_norm": 7.02324104309082,
      "learning_rate": 1.4498644986449867e-05,
      "loss": 5.6342,
      "step": 524000
    },
    {
      "epoch": 710.7046070460705,
      "grad_norm": 5.690725326538086,
      "learning_rate": 1.4464769647696477e-05,
      "loss": 5.6217,
      "step": 524500
    },
    {
      "epoch": 711.3821138211382,
      "grad_norm": 8.19315242767334,
      "learning_rate": 1.443089430894309e-05,
      "loss": 5.5969,
      "step": 525000
    },
    {
      "epoch": 712.059620596206,
      "grad_norm": 8.119011878967285,
      "learning_rate": 1.4397018970189702e-05,
      "loss": 5.6329,
      "step": 525500
    },
    {
      "epoch": 712.7371273712737,
      "grad_norm": 8.211405754089355,
      "learning_rate": 1.4363143631436315e-05,
      "loss": 5.6203,
      "step": 526000
    },
    {
      "epoch": 713.4146341463414,
      "grad_norm": 8.978253364562988,
      "learning_rate": 1.4329268292682927e-05,
      "loss": 5.6204,
      "step": 526500
    },
    {
      "epoch": 714.0921409214092,
      "grad_norm": 7.559086322784424,
      "learning_rate": 1.429539295392954e-05,
      "loss": 5.6139,
      "step": 527000
    },
    {
      "epoch": 714.7696476964769,
      "grad_norm": 7.698705673217773,
      "learning_rate": 1.4261517615176154e-05,
      "loss": 5.612,
      "step": 527500
    },
    {
      "epoch": 715.4471544715448,
      "grad_norm": 6.531494140625,
      "learning_rate": 1.4227642276422764e-05,
      "loss": 5.5881,
      "step": 528000
    },
    {
      "epoch": 716.1246612466125,
      "grad_norm": 7.407519817352295,
      "learning_rate": 1.4193766937669379e-05,
      "loss": 5.6362,
      "step": 528500
    },
    {
      "epoch": 716.8021680216802,
      "grad_norm": 9.02359390258789,
      "learning_rate": 1.4159891598915989e-05,
      "loss": 5.6141,
      "step": 529000
    },
    {
      "epoch": 717.479674796748,
      "grad_norm": 9.07137680053711,
      "learning_rate": 1.4126016260162602e-05,
      "loss": 5.6298,
      "step": 529500
    },
    {
      "epoch": 718.1571815718157,
      "grad_norm": 5.831789493560791,
      "learning_rate": 1.4092140921409214e-05,
      "loss": 5.6043,
      "step": 530000
    },
    {
      "epoch": 718.8346883468835,
      "grad_norm": 12.18143367767334,
      "learning_rate": 1.4058265582655828e-05,
      "loss": 5.6371,
      "step": 530500
    },
    {
      "epoch": 719.5121951219512,
      "grad_norm": 9.512859344482422,
      "learning_rate": 1.4024390243902441e-05,
      "loss": 5.5943,
      "step": 531000
    },
    {
      "epoch": 720.189701897019,
      "grad_norm": 10.347850799560547,
      "learning_rate": 1.3990514905149053e-05,
      "loss": 5.6286,
      "step": 531500
    },
    {
      "epoch": 720.8672086720867,
      "grad_norm": 6.959736347198486,
      "learning_rate": 1.3956639566395666e-05,
      "loss": 5.6216,
      "step": 532000
    },
    {
      "epoch": 721.5447154471544,
      "grad_norm": 7.5255022048950195,
      "learning_rate": 1.3922764227642276e-05,
      "loss": 5.6017,
      "step": 532500
    },
    {
      "epoch": 722.2222222222222,
      "grad_norm": 8.442205429077148,
      "learning_rate": 1.388888888888889e-05,
      "loss": 5.6447,
      "step": 533000
    },
    {
      "epoch": 722.89972899729,
      "grad_norm": 7.939908027648926,
      "learning_rate": 1.3855013550135501e-05,
      "loss": 5.6039,
      "step": 533500
    },
    {
      "epoch": 723.5772357723578,
      "grad_norm": 10.794251441955566,
      "learning_rate": 1.3821138211382115e-05,
      "loss": 5.6235,
      "step": 534000
    },
    {
      "epoch": 724.2547425474255,
      "grad_norm": 7.0307841300964355,
      "learning_rate": 1.3787262872628726e-05,
      "loss": 5.6281,
      "step": 534500
    },
    {
      "epoch": 724.9322493224932,
      "grad_norm": 9.723440170288086,
      "learning_rate": 1.375338753387534e-05,
      "loss": 5.5895,
      "step": 535000
    },
    {
      "epoch": 725.609756097561,
      "grad_norm": 14.482314109802246,
      "learning_rate": 1.3719512195121953e-05,
      "loss": 5.5967,
      "step": 535500
    },
    {
      "epoch": 726.2872628726287,
      "grad_norm": 8.081705093383789,
      "learning_rate": 1.3685636856368563e-05,
      "loss": 5.6294,
      "step": 536000
    },
    {
      "epoch": 726.9647696476965,
      "grad_norm": 10.102363586425781,
      "learning_rate": 1.3651761517615177e-05,
      "loss": 5.6156,
      "step": 536500
    },
    {
      "epoch": 727.6422764227642,
      "grad_norm": 6.818513870239258,
      "learning_rate": 1.3617886178861788e-05,
      "loss": 5.5929,
      "step": 537000
    },
    {
      "epoch": 728.319783197832,
      "grad_norm": 6.689769268035889,
      "learning_rate": 1.3584010840108402e-05,
      "loss": 5.6458,
      "step": 537500
    },
    {
      "epoch": 728.9972899728997,
      "grad_norm": 11.65676498413086,
      "learning_rate": 1.3550135501355014e-05,
      "loss": 5.6142,
      "step": 538000
    },
    {
      "epoch": 729.6747967479674,
      "grad_norm": 7.536404132843018,
      "learning_rate": 1.3516260162601627e-05,
      "loss": 5.6186,
      "step": 538500
    },
    {
      "epoch": 730.3523035230352,
      "grad_norm": 7.744057655334473,
      "learning_rate": 1.348238482384824e-05,
      "loss": 5.6055,
      "step": 539000
    },
    {
      "epoch": 731.029810298103,
      "grad_norm": 8.000079154968262,
      "learning_rate": 1.344850948509485e-05,
      "loss": 5.6209,
      "step": 539500
    },
    {
      "epoch": 731.7073170731708,
      "grad_norm": 9.87191390991211,
      "learning_rate": 1.3414634146341466e-05,
      "loss": 5.6238,
      "step": 540000
    },
    {
      "epoch": 732.3848238482385,
      "grad_norm": 7.024881362915039,
      "learning_rate": 1.3380758807588076e-05,
      "loss": 5.5944,
      "step": 540500
    },
    {
      "epoch": 733.0623306233063,
      "grad_norm": 8.955106735229492,
      "learning_rate": 1.3346883468834689e-05,
      "loss": 5.6257,
      "step": 541000
    },
    {
      "epoch": 733.739837398374,
      "grad_norm": 11.213032722473145,
      "learning_rate": 1.33130081300813e-05,
      "loss": 5.6309,
      "step": 541500
    },
    {
      "epoch": 734.4173441734417,
      "grad_norm": 9.836861610412598,
      "learning_rate": 1.3279132791327914e-05,
      "loss": 5.5872,
      "step": 542000
    },
    {
      "epoch": 735.0948509485095,
      "grad_norm": 8.096532821655273,
      "learning_rate": 1.3245257452574527e-05,
      "loss": 5.6201,
      "step": 542500
    },
    {
      "epoch": 735.7723577235772,
      "grad_norm": 6.2475972175598145,
      "learning_rate": 1.321138211382114e-05,
      "loss": 5.5919,
      "step": 543000
    },
    {
      "epoch": 736.449864498645,
      "grad_norm": 8.359221458435059,
      "learning_rate": 1.3177506775067753e-05,
      "loss": 5.6635,
      "step": 543500
    },
    {
      "epoch": 737.1273712737127,
      "grad_norm": 6.747983932495117,
      "learning_rate": 1.3143631436314363e-05,
      "loss": 5.5951,
      "step": 544000
    },
    {
      "epoch": 737.8048780487804,
      "grad_norm": 9.542080879211426,
      "learning_rate": 1.3109756097560976e-05,
      "loss": 5.6234,
      "step": 544500
    },
    {
      "epoch": 738.4823848238483,
      "grad_norm": 6.79249906539917,
      "learning_rate": 1.3075880758807588e-05,
      "loss": 5.5711,
      "step": 545000
    },
    {
      "epoch": 739.159891598916,
      "grad_norm": 8.842034339904785,
      "learning_rate": 1.3042005420054201e-05,
      "loss": 5.6328,
      "step": 545500
    }
  ],
  "logging_steps": 500,
  "max_steps": 738000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 9250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.0034446872576e+16,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
