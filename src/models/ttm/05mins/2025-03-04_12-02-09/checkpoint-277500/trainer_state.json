{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 376.0162601626016,
  "eval_steps": 500,
  "global_step": 277500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 5.091018199920654,
      "learning_rate": 4.996612466124662e-05,
      "loss": 6.803,
      "step": 500
    },
    {
      "epoch": 1.3550135501355014,
      "grad_norm": 3.3020758628845215,
      "learning_rate": 4.993224932249323e-05,
      "loss": 6.1589,
      "step": 1000
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 8.262969017028809,
      "learning_rate": 4.989837398373984e-05,
      "loss": 6.1256,
      "step": 1500
    },
    {
      "epoch": 2.710027100271003,
      "grad_norm": 7.3213653564453125,
      "learning_rate": 4.986449864498645e-05,
      "loss": 6.0501,
      "step": 2000
    },
    {
      "epoch": 3.3875338753387534,
      "grad_norm": 15.888947486877441,
      "learning_rate": 4.9830623306233066e-05,
      "loss": 5.9678,
      "step": 2500
    },
    {
      "epoch": 4.065040650406504,
      "grad_norm": 8.027318954467773,
      "learning_rate": 4.9796747967479676e-05,
      "loss": 5.9816,
      "step": 3000
    },
    {
      "epoch": 4.742547425474255,
      "grad_norm": 9.33156681060791,
      "learning_rate": 4.9762872628726286e-05,
      "loss": 5.9411,
      "step": 3500
    },
    {
      "epoch": 5.420054200542006,
      "grad_norm": 10.087059020996094,
      "learning_rate": 4.97289972899729e-05,
      "loss": 5.8978,
      "step": 4000
    },
    {
      "epoch": 6.097560975609756,
      "grad_norm": 9.301751136779785,
      "learning_rate": 4.969512195121951e-05,
      "loss": 5.9311,
      "step": 4500
    },
    {
      "epoch": 6.775067750677507,
      "grad_norm": 11.422779083251953,
      "learning_rate": 4.966124661246613e-05,
      "loss": 5.8857,
      "step": 5000
    },
    {
      "epoch": 7.452574525745257,
      "grad_norm": 6.880033493041992,
      "learning_rate": 4.962737127371274e-05,
      "loss": 5.8476,
      "step": 5500
    },
    {
      "epoch": 8.130081300813009,
      "grad_norm": 7.027878284454346,
      "learning_rate": 4.959349593495935e-05,
      "loss": 5.8935,
      "step": 6000
    },
    {
      "epoch": 8.807588075880759,
      "grad_norm": 7.722722053527832,
      "learning_rate": 4.955962059620597e-05,
      "loss": 5.8373,
      "step": 6500
    },
    {
      "epoch": 9.48509485094851,
      "grad_norm": 9.376630783081055,
      "learning_rate": 4.952574525745258e-05,
      "loss": 5.8788,
      "step": 7000
    },
    {
      "epoch": 10.16260162601626,
      "grad_norm": 11.910229682922363,
      "learning_rate": 4.9491869918699193e-05,
      "loss": 5.8272,
      "step": 7500
    },
    {
      "epoch": 10.840108401084011,
      "grad_norm": 10.906257629394531,
      "learning_rate": 4.9457994579945803e-05,
      "loss": 5.8068,
      "step": 8000
    },
    {
      "epoch": 11.517615176151761,
      "grad_norm": 14.655856132507324,
      "learning_rate": 4.9424119241192414e-05,
      "loss": 5.8539,
      "step": 8500
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 7.715482234954834,
      "learning_rate": 4.9390243902439024e-05,
      "loss": 5.7983,
      "step": 9000
    },
    {
      "epoch": 12.872628726287262,
      "grad_norm": 11.57014274597168,
      "learning_rate": 4.935636856368564e-05,
      "loss": 5.8107,
      "step": 9500
    },
    {
      "epoch": 13.550135501355014,
      "grad_norm": 8.040351867675781,
      "learning_rate": 4.932249322493225e-05,
      "loss": 5.8246,
      "step": 10000
    },
    {
      "epoch": 14.227642276422765,
      "grad_norm": 13.104251861572266,
      "learning_rate": 4.928861788617886e-05,
      "loss": 5.8106,
      "step": 10500
    },
    {
      "epoch": 14.905149051490515,
      "grad_norm": 9.188155174255371,
      "learning_rate": 4.925474254742548e-05,
      "loss": 5.8132,
      "step": 11000
    },
    {
      "epoch": 15.582655826558266,
      "grad_norm": 11.850305557250977,
      "learning_rate": 4.922086720867209e-05,
      "loss": 5.8105,
      "step": 11500
    },
    {
      "epoch": 16.260162601626018,
      "grad_norm": 11.092033386230469,
      "learning_rate": 4.9186991869918704e-05,
      "loss": 5.784,
      "step": 12000
    },
    {
      "epoch": 16.937669376693766,
      "grad_norm": 8.578161239624023,
      "learning_rate": 4.9153116531165314e-05,
      "loss": 5.7848,
      "step": 12500
    },
    {
      "epoch": 17.615176151761517,
      "grad_norm": 10.56583309173584,
      "learning_rate": 4.9119241192411924e-05,
      "loss": 5.8027,
      "step": 13000
    },
    {
      "epoch": 18.29268292682927,
      "grad_norm": 7.775592803955078,
      "learning_rate": 4.908536585365854e-05,
      "loss": 5.7688,
      "step": 13500
    },
    {
      "epoch": 18.97018970189702,
      "grad_norm": 12.018457412719727,
      "learning_rate": 4.905149051490515e-05,
      "loss": 5.7836,
      "step": 14000
    },
    {
      "epoch": 19.647696476964768,
      "grad_norm": 8.249943733215332,
      "learning_rate": 4.901761517615177e-05,
      "loss": 5.7988,
      "step": 14500
    },
    {
      "epoch": 20.32520325203252,
      "grad_norm": 7.565089702606201,
      "learning_rate": 4.898373983739837e-05,
      "loss": 5.7566,
      "step": 15000
    },
    {
      "epoch": 21.00271002710027,
      "grad_norm": 9.527578353881836,
      "learning_rate": 4.894986449864499e-05,
      "loss": 5.7912,
      "step": 15500
    },
    {
      "epoch": 21.680216802168022,
      "grad_norm": 6.278652667999268,
      "learning_rate": 4.89159891598916e-05,
      "loss": 5.7818,
      "step": 16000
    },
    {
      "epoch": 22.357723577235774,
      "grad_norm": 9.314443588256836,
      "learning_rate": 4.8882113821138215e-05,
      "loss": 5.7763,
      "step": 16500
    },
    {
      "epoch": 23.035230352303522,
      "grad_norm": 14.77385425567627,
      "learning_rate": 4.884823848238483e-05,
      "loss": 5.7497,
      "step": 17000
    },
    {
      "epoch": 23.712737127371273,
      "grad_norm": 9.693492889404297,
      "learning_rate": 4.8814363143631435e-05,
      "loss": 5.775,
      "step": 17500
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 15.656332969665527,
      "learning_rate": 4.878048780487805e-05,
      "loss": 5.7446,
      "step": 18000
    },
    {
      "epoch": 25.067750677506776,
      "grad_norm": 9.059091567993164,
      "learning_rate": 4.874661246612466e-05,
      "loss": 5.797,
      "step": 18500
    },
    {
      "epoch": 25.745257452574524,
      "grad_norm": 9.663070678710938,
      "learning_rate": 4.871273712737128e-05,
      "loss": 5.7604,
      "step": 19000
    },
    {
      "epoch": 26.422764227642276,
      "grad_norm": 14.631203651428223,
      "learning_rate": 4.867886178861789e-05,
      "loss": 5.7375,
      "step": 19500
    },
    {
      "epoch": 27.100271002710027,
      "grad_norm": 7.689953804016113,
      "learning_rate": 4.86449864498645e-05,
      "loss": 5.7543,
      "step": 20000
    },
    {
      "epoch": 27.77777777777778,
      "grad_norm": 12.156087875366211,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 5.7679,
      "step": 20500
    },
    {
      "epoch": 28.45528455284553,
      "grad_norm": 8.73508071899414,
      "learning_rate": 4.8577235772357725e-05,
      "loss": 5.754,
      "step": 21000
    },
    {
      "epoch": 29.132791327913278,
      "grad_norm": 6.654566287994385,
      "learning_rate": 4.854336043360434e-05,
      "loss": 5.7533,
      "step": 21500
    },
    {
      "epoch": 29.81029810298103,
      "grad_norm": 13.123746871948242,
      "learning_rate": 4.8509485094850945e-05,
      "loss": 5.7613,
      "step": 22000
    },
    {
      "epoch": 30.48780487804878,
      "grad_norm": 10.495832443237305,
      "learning_rate": 4.847560975609756e-05,
      "loss": 5.7233,
      "step": 22500
    },
    {
      "epoch": 31.165311653116532,
      "grad_norm": 7.0927605628967285,
      "learning_rate": 4.844173441734418e-05,
      "loss": 5.7641,
      "step": 23000
    },
    {
      "epoch": 31.84281842818428,
      "grad_norm": 8.636788368225098,
      "learning_rate": 4.840785907859079e-05,
      "loss": 5.7313,
      "step": 23500
    },
    {
      "epoch": 32.520325203252035,
      "grad_norm": 9.791983604431152,
      "learning_rate": 4.8373983739837406e-05,
      "loss": 5.7351,
      "step": 24000
    },
    {
      "epoch": 33.19783197831978,
      "grad_norm": 24.067975997924805,
      "learning_rate": 4.834010840108401e-05,
      "loss": 5.7762,
      "step": 24500
    },
    {
      "epoch": 33.87533875338753,
      "grad_norm": 8.568990707397461,
      "learning_rate": 4.8306233062330626e-05,
      "loss": 5.7436,
      "step": 25000
    },
    {
      "epoch": 34.552845528455286,
      "grad_norm": 9.561119079589844,
      "learning_rate": 4.8272357723577236e-05,
      "loss": 5.734,
      "step": 25500
    },
    {
      "epoch": 35.230352303523034,
      "grad_norm": 9.91040325164795,
      "learning_rate": 4.823848238482385e-05,
      "loss": 5.742,
      "step": 26000
    },
    {
      "epoch": 35.90785907859079,
      "grad_norm": 11.39449405670166,
      "learning_rate": 4.820460704607046e-05,
      "loss": 5.7499,
      "step": 26500
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 9.793062210083008,
      "learning_rate": 4.817073170731707e-05,
      "loss": 5.7413,
      "step": 27000
    },
    {
      "epoch": 37.262872628726285,
      "grad_norm": 7.896596431732178,
      "learning_rate": 4.813685636856369e-05,
      "loss": 5.7286,
      "step": 27500
    },
    {
      "epoch": 37.94037940379404,
      "grad_norm": 6.42507791519165,
      "learning_rate": 4.81029810298103e-05,
      "loss": 5.7345,
      "step": 28000
    },
    {
      "epoch": 38.61788617886179,
      "grad_norm": 11.094392776489258,
      "learning_rate": 4.8069105691056916e-05,
      "loss": 5.7163,
      "step": 28500
    },
    {
      "epoch": 39.295392953929536,
      "grad_norm": 9.148066520690918,
      "learning_rate": 4.8035230352303526e-05,
      "loss": 5.7792,
      "step": 29000
    },
    {
      "epoch": 39.97289972899729,
      "grad_norm": 10.703146934509277,
      "learning_rate": 4.8001355013550136e-05,
      "loss": 5.7311,
      "step": 29500
    },
    {
      "epoch": 40.65040650406504,
      "grad_norm": 11.24628734588623,
      "learning_rate": 4.796747967479675e-05,
      "loss": 5.7382,
      "step": 30000
    },
    {
      "epoch": 41.327913279132794,
      "grad_norm": 12.71223258972168,
      "learning_rate": 4.793360433604336e-05,
      "loss": 5.713,
      "step": 30500
    },
    {
      "epoch": 42.00542005420054,
      "grad_norm": 7.349093914031982,
      "learning_rate": 4.789972899728998e-05,
      "loss": 5.7301,
      "step": 31000
    },
    {
      "epoch": 42.68292682926829,
      "grad_norm": 8.91130256652832,
      "learning_rate": 4.786585365853658e-05,
      "loss": 5.7398,
      "step": 31500
    },
    {
      "epoch": 43.360433604336045,
      "grad_norm": 5.983088493347168,
      "learning_rate": 4.78319783197832e-05,
      "loss": 5.7064,
      "step": 32000
    },
    {
      "epoch": 44.03794037940379,
      "grad_norm": 9.94255256652832,
      "learning_rate": 4.779810298102981e-05,
      "loss": 5.7605,
      "step": 32500
    },
    {
      "epoch": 44.71544715447155,
      "grad_norm": 11.870882987976074,
      "learning_rate": 4.776422764227643e-05,
      "loss": 5.7491,
      "step": 33000
    },
    {
      "epoch": 45.392953929539296,
      "grad_norm": 11.326759338378906,
      "learning_rate": 4.773035230352304e-05,
      "loss": 5.696,
      "step": 33500
    },
    {
      "epoch": 46.070460704607044,
      "grad_norm": 11.065422058105469,
      "learning_rate": 4.769647696476965e-05,
      "loss": 5.7135,
      "step": 34000
    },
    {
      "epoch": 46.7479674796748,
      "grad_norm": 11.42674446105957,
      "learning_rate": 4.7662601626016264e-05,
      "loss": 5.7237,
      "step": 34500
    },
    {
      "epoch": 47.42547425474255,
      "grad_norm": 10.969059944152832,
      "learning_rate": 4.7628726287262874e-05,
      "loss": 5.7174,
      "step": 35000
    },
    {
      "epoch": 48.1029810298103,
      "grad_norm": 11.1267671585083,
      "learning_rate": 4.759485094850949e-05,
      "loss": 5.7379,
      "step": 35500
    },
    {
      "epoch": 48.78048780487805,
      "grad_norm": 7.527477741241455,
      "learning_rate": 4.75609756097561e-05,
      "loss": 5.7232,
      "step": 36000
    },
    {
      "epoch": 49.4579945799458,
      "grad_norm": 11.792120933532715,
      "learning_rate": 4.752710027100271e-05,
      "loss": 5.7007,
      "step": 36500
    },
    {
      "epoch": 50.13550135501355,
      "grad_norm": 12.336548805236816,
      "learning_rate": 4.749322493224933e-05,
      "loss": 5.7497,
      "step": 37000
    },
    {
      "epoch": 50.8130081300813,
      "grad_norm": 8.872333526611328,
      "learning_rate": 4.745934959349594e-05,
      "loss": 5.7216,
      "step": 37500
    },
    {
      "epoch": 51.49051490514905,
      "grad_norm": 14.99206256866455,
      "learning_rate": 4.7425474254742554e-05,
      "loss": 5.7078,
      "step": 38000
    },
    {
      "epoch": 52.1680216802168,
      "grad_norm": 14.154269218444824,
      "learning_rate": 4.739159891598916e-05,
      "loss": 5.7151,
      "step": 38500
    },
    {
      "epoch": 52.84552845528455,
      "grad_norm": 16.802804946899414,
      "learning_rate": 4.7357723577235774e-05,
      "loss": 5.7296,
      "step": 39000
    },
    {
      "epoch": 53.523035230352306,
      "grad_norm": 8.198492050170898,
      "learning_rate": 4.732384823848239e-05,
      "loss": 5.6997,
      "step": 39500
    },
    {
      "epoch": 54.200542005420054,
      "grad_norm": 8.03692626953125,
      "learning_rate": 4.7289972899729e-05,
      "loss": 5.7388,
      "step": 40000
    },
    {
      "epoch": 54.8780487804878,
      "grad_norm": 9.592068672180176,
      "learning_rate": 4.725609756097561e-05,
      "loss": 5.703,
      "step": 40500
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 8.117247581481934,
      "learning_rate": 4.722222222222222e-05,
      "loss": 5.7057,
      "step": 41000
    },
    {
      "epoch": 56.233062330623305,
      "grad_norm": 9.578608512878418,
      "learning_rate": 4.718834688346884e-05,
      "loss": 5.7093,
      "step": 41500
    },
    {
      "epoch": 56.91056910569106,
      "grad_norm": 20.542922973632812,
      "learning_rate": 4.715447154471545e-05,
      "loss": 5.7104,
      "step": 42000
    },
    {
      "epoch": 57.58807588075881,
      "grad_norm": 10.634708404541016,
      "learning_rate": 4.7120596205962065e-05,
      "loss": 5.6843,
      "step": 42500
    },
    {
      "epoch": 58.265582655826556,
      "grad_norm": 13.108489036560059,
      "learning_rate": 4.7086720867208675e-05,
      "loss": 5.7408,
      "step": 43000
    },
    {
      "epoch": 58.94308943089431,
      "grad_norm": 7.956690311431885,
      "learning_rate": 4.7052845528455285e-05,
      "loss": 5.7162,
      "step": 43500
    },
    {
      "epoch": 59.62059620596206,
      "grad_norm": 10.43460750579834,
      "learning_rate": 4.70189701897019e-05,
      "loss": 5.7095,
      "step": 44000
    },
    {
      "epoch": 60.29810298102981,
      "grad_norm": 8.457387924194336,
      "learning_rate": 4.698509485094851e-05,
      "loss": 5.7415,
      "step": 44500
    },
    {
      "epoch": 60.97560975609756,
      "grad_norm": 8.040995597839355,
      "learning_rate": 4.695121951219512e-05,
      "loss": 5.6758,
      "step": 45000
    },
    {
      "epoch": 61.65311653116531,
      "grad_norm": 8.13553524017334,
      "learning_rate": 4.691734417344174e-05,
      "loss": 5.6884,
      "step": 45500
    },
    {
      "epoch": 62.330623306233065,
      "grad_norm": 10.497289657592773,
      "learning_rate": 4.688346883468835e-05,
      "loss": 5.7133,
      "step": 46000
    },
    {
      "epoch": 63.00813008130081,
      "grad_norm": 15.844855308532715,
      "learning_rate": 4.6849593495934965e-05,
      "loss": 5.7167,
      "step": 46500
    },
    {
      "epoch": 63.68563685636856,
      "grad_norm": 9.074761390686035,
      "learning_rate": 4.6815718157181575e-05,
      "loss": 5.69,
      "step": 47000
    },
    {
      "epoch": 64.36314363143632,
      "grad_norm": 9.115644454956055,
      "learning_rate": 4.6781842818428185e-05,
      "loss": 5.7144,
      "step": 47500
    },
    {
      "epoch": 65.04065040650407,
      "grad_norm": 7.438839912414551,
      "learning_rate": 4.6747967479674795e-05,
      "loss": 5.7096,
      "step": 48000
    },
    {
      "epoch": 65.71815718157181,
      "grad_norm": 13.010701179504395,
      "learning_rate": 4.671409214092141e-05,
      "loss": 5.7045,
      "step": 48500
    },
    {
      "epoch": 66.39566395663957,
      "grad_norm": 8.549729347229004,
      "learning_rate": 4.668021680216802e-05,
      "loss": 5.7061,
      "step": 49000
    },
    {
      "epoch": 67.07317073170732,
      "grad_norm": 7.454829216003418,
      "learning_rate": 4.664634146341464e-05,
      "loss": 5.6976,
      "step": 49500
    },
    {
      "epoch": 67.75067750677506,
      "grad_norm": 11.433731079101562,
      "learning_rate": 4.661246612466125e-05,
      "loss": 5.7339,
      "step": 50000
    },
    {
      "epoch": 68.42818428184282,
      "grad_norm": 9.491683959960938,
      "learning_rate": 4.657859078590786e-05,
      "loss": 5.6896,
      "step": 50500
    },
    {
      "epoch": 69.10569105691057,
      "grad_norm": 18.037641525268555,
      "learning_rate": 4.6544715447154476e-05,
      "loss": 5.7133,
      "step": 51000
    },
    {
      "epoch": 69.78319783197831,
      "grad_norm": 10.448026657104492,
      "learning_rate": 4.6510840108401086e-05,
      "loss": 5.6725,
      "step": 51500
    },
    {
      "epoch": 70.46070460704607,
      "grad_norm": 8.89246940612793,
      "learning_rate": 4.6476964769647696e-05,
      "loss": 5.701,
      "step": 52000
    },
    {
      "epoch": 71.13821138211382,
      "grad_norm": 9.766633987426758,
      "learning_rate": 4.644308943089431e-05,
      "loss": 5.7397,
      "step": 52500
    },
    {
      "epoch": 71.81571815718158,
      "grad_norm": 7.610794544219971,
      "learning_rate": 4.640921409214092e-05,
      "loss": 5.6946,
      "step": 53000
    },
    {
      "epoch": 72.49322493224932,
      "grad_norm": 11.947199821472168,
      "learning_rate": 4.637533875338754e-05,
      "loss": 5.6909,
      "step": 53500
    },
    {
      "epoch": 73.17073170731707,
      "grad_norm": 9.854961395263672,
      "learning_rate": 4.634146341463415e-05,
      "loss": 5.6739,
      "step": 54000
    },
    {
      "epoch": 73.84823848238483,
      "grad_norm": 6.153323173522949,
      "learning_rate": 4.630758807588076e-05,
      "loss": 5.6826,
      "step": 54500
    },
    {
      "epoch": 74.52574525745257,
      "grad_norm": 9.705251693725586,
      "learning_rate": 4.627371273712737e-05,
      "loss": 5.7062,
      "step": 55000
    },
    {
      "epoch": 75.20325203252033,
      "grad_norm": 8.191561698913574,
      "learning_rate": 4.6239837398373986e-05,
      "loss": 5.6917,
      "step": 55500
    },
    {
      "epoch": 75.88075880758808,
      "grad_norm": 8.579245567321777,
      "learning_rate": 4.62059620596206e-05,
      "loss": 5.6945,
      "step": 56000
    },
    {
      "epoch": 76.55826558265582,
      "grad_norm": 13.465226173400879,
      "learning_rate": 4.617208672086721e-05,
      "loss": 5.7235,
      "step": 56500
    },
    {
      "epoch": 77.23577235772358,
      "grad_norm": 7.795341968536377,
      "learning_rate": 4.613821138211382e-05,
      "loss": 5.6659,
      "step": 57000
    },
    {
      "epoch": 77.91327913279133,
      "grad_norm": 8.340959548950195,
      "learning_rate": 4.610433604336043e-05,
      "loss": 5.7015,
      "step": 57500
    },
    {
      "epoch": 78.59078590785907,
      "grad_norm": 9.505107879638672,
      "learning_rate": 4.607046070460705e-05,
      "loss": 5.7274,
      "step": 58000
    },
    {
      "epoch": 79.26829268292683,
      "grad_norm": 17.29327392578125,
      "learning_rate": 4.603658536585366e-05,
      "loss": 5.6947,
      "step": 58500
    },
    {
      "epoch": 79.94579945799458,
      "grad_norm": 12.023770332336426,
      "learning_rate": 4.600271002710027e-05,
      "loss": 5.6738,
      "step": 59000
    },
    {
      "epoch": 80.62330623306234,
      "grad_norm": 10.241198539733887,
      "learning_rate": 4.596883468834689e-05,
      "loss": 5.7083,
      "step": 59500
    },
    {
      "epoch": 81.30081300813008,
      "grad_norm": 18.282018661499023,
      "learning_rate": 4.59349593495935e-05,
      "loss": 5.6599,
      "step": 60000
    },
    {
      "epoch": 81.97831978319783,
      "grad_norm": 10.7692289352417,
      "learning_rate": 4.5901084010840114e-05,
      "loss": 5.7053,
      "step": 60500
    },
    {
      "epoch": 82.65582655826559,
      "grad_norm": 11.36131763458252,
      "learning_rate": 4.5867208672086724e-05,
      "loss": 5.7152,
      "step": 61000
    },
    {
      "epoch": 83.33333333333333,
      "grad_norm": 12.090702056884766,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 5.6963,
      "step": 61500
    },
    {
      "epoch": 84.01084010840108,
      "grad_norm": 11.715343475341797,
      "learning_rate": 4.579945799457995e-05,
      "loss": 5.6697,
      "step": 62000
    },
    {
      "epoch": 84.68834688346884,
      "grad_norm": 10.861991882324219,
      "learning_rate": 4.576558265582656e-05,
      "loss": 5.7116,
      "step": 62500
    },
    {
      "epoch": 85.36585365853658,
      "grad_norm": 7.252991199493408,
      "learning_rate": 4.573170731707318e-05,
      "loss": 5.686,
      "step": 63000
    },
    {
      "epoch": 86.04336043360433,
      "grad_norm": 7.696922779083252,
      "learning_rate": 4.569783197831978e-05,
      "loss": 5.669,
      "step": 63500
    },
    {
      "epoch": 86.72086720867209,
      "grad_norm": 8.799280166625977,
      "learning_rate": 4.56639566395664e-05,
      "loss": 5.684,
      "step": 64000
    },
    {
      "epoch": 87.39837398373983,
      "grad_norm": 7.576893329620361,
      "learning_rate": 4.563008130081301e-05,
      "loss": 5.7047,
      "step": 64500
    },
    {
      "epoch": 88.07588075880759,
      "grad_norm": 9.006096839904785,
      "learning_rate": 4.5596205962059624e-05,
      "loss": 5.6738,
      "step": 65000
    },
    {
      "epoch": 88.75338753387534,
      "grad_norm": 9.088390350341797,
      "learning_rate": 4.5562330623306234e-05,
      "loss": 5.6929,
      "step": 65500
    },
    {
      "epoch": 89.4308943089431,
      "grad_norm": 10.151029586791992,
      "learning_rate": 4.5528455284552844e-05,
      "loss": 5.7001,
      "step": 66000
    },
    {
      "epoch": 90.10840108401084,
      "grad_norm": 7.036646366119385,
      "learning_rate": 4.549457994579946e-05,
      "loss": 5.7139,
      "step": 66500
    },
    {
      "epoch": 90.78590785907859,
      "grad_norm": 20.453771591186523,
      "learning_rate": 4.546070460704607e-05,
      "loss": 5.6628,
      "step": 67000
    },
    {
      "epoch": 91.46341463414635,
      "grad_norm": 7.753979682922363,
      "learning_rate": 4.542682926829269e-05,
      "loss": 5.7063,
      "step": 67500
    },
    {
      "epoch": 92.14092140921409,
      "grad_norm": 8.421381950378418,
      "learning_rate": 4.53929539295393e-05,
      "loss": 5.675,
      "step": 68000
    },
    {
      "epoch": 92.81842818428184,
      "grad_norm": 10.344338417053223,
      "learning_rate": 4.535907859078591e-05,
      "loss": 5.693,
      "step": 68500
    },
    {
      "epoch": 93.4959349593496,
      "grad_norm": 12.800844192504883,
      "learning_rate": 4.5325203252032525e-05,
      "loss": 5.671,
      "step": 69000
    },
    {
      "epoch": 94.17344173441734,
      "grad_norm": 8.62265396118164,
      "learning_rate": 4.5291327913279135e-05,
      "loss": 5.7068,
      "step": 69500
    },
    {
      "epoch": 94.8509485094851,
      "grad_norm": 6.375396728515625,
      "learning_rate": 4.525745257452575e-05,
      "loss": 5.6704,
      "step": 70000
    },
    {
      "epoch": 95.52845528455285,
      "grad_norm": 6.98372220993042,
      "learning_rate": 4.5223577235772355e-05,
      "loss": 5.6863,
      "step": 70500
    },
    {
      "epoch": 96.2059620596206,
      "grad_norm": 6.998977184295654,
      "learning_rate": 4.518970189701897e-05,
      "loss": 5.6901,
      "step": 71000
    },
    {
      "epoch": 96.88346883468834,
      "grad_norm": 10.754554748535156,
      "learning_rate": 4.515582655826558e-05,
      "loss": 5.6476,
      "step": 71500
    },
    {
      "epoch": 97.5609756097561,
      "grad_norm": 6.208372592926025,
      "learning_rate": 4.51219512195122e-05,
      "loss": 5.7224,
      "step": 72000
    },
    {
      "epoch": 98.23848238482385,
      "grad_norm": 7.487302780151367,
      "learning_rate": 4.5088075880758815e-05,
      "loss": 5.6849,
      "step": 72500
    },
    {
      "epoch": 98.9159891598916,
      "grad_norm": 12.949740409851074,
      "learning_rate": 4.505420054200542e-05,
      "loss": 5.6571,
      "step": 73000
    },
    {
      "epoch": 99.59349593495935,
      "grad_norm": 9.150491714477539,
      "learning_rate": 4.5020325203252035e-05,
      "loss": 5.6813,
      "step": 73500
    },
    {
      "epoch": 100.2710027100271,
      "grad_norm": 7.2192254066467285,
      "learning_rate": 4.4986449864498645e-05,
      "loss": 5.6746,
      "step": 74000
    },
    {
      "epoch": 100.94850948509485,
      "grad_norm": 8.052684783935547,
      "learning_rate": 4.495257452574526e-05,
      "loss": 5.6955,
      "step": 74500
    },
    {
      "epoch": 101.6260162601626,
      "grad_norm": 12.560942649841309,
      "learning_rate": 4.491869918699187e-05,
      "loss": 5.7026,
      "step": 75000
    },
    {
      "epoch": 102.30352303523036,
      "grad_norm": 16.167072296142578,
      "learning_rate": 4.488482384823848e-05,
      "loss": 5.6309,
      "step": 75500
    },
    {
      "epoch": 102.9810298102981,
      "grad_norm": 10.525370597839355,
      "learning_rate": 4.48509485094851e-05,
      "loss": 5.7072,
      "step": 76000
    },
    {
      "epoch": 103.65853658536585,
      "grad_norm": 9.487058639526367,
      "learning_rate": 4.481707317073171e-05,
      "loss": 5.6746,
      "step": 76500
    },
    {
      "epoch": 104.3360433604336,
      "grad_norm": 7.807596683502197,
      "learning_rate": 4.4783197831978326e-05,
      "loss": 5.7144,
      "step": 77000
    },
    {
      "epoch": 105.01355013550136,
      "grad_norm": 9.954623222351074,
      "learning_rate": 4.474932249322493e-05,
      "loss": 5.6621,
      "step": 77500
    },
    {
      "epoch": 105.6910569105691,
      "grad_norm": 10.59250259399414,
      "learning_rate": 4.4715447154471546e-05,
      "loss": 5.6918,
      "step": 78000
    },
    {
      "epoch": 106.36856368563686,
      "grad_norm": 10.967586517333984,
      "learning_rate": 4.468157181571816e-05,
      "loss": 5.6809,
      "step": 78500
    },
    {
      "epoch": 107.04607046070461,
      "grad_norm": 11.346611022949219,
      "learning_rate": 4.464769647696477e-05,
      "loss": 5.6848,
      "step": 79000
    },
    {
      "epoch": 107.72357723577235,
      "grad_norm": 9.244811058044434,
      "learning_rate": 4.461382113821139e-05,
      "loss": 5.6826,
      "step": 79500
    },
    {
      "epoch": 108.40108401084011,
      "grad_norm": 8.84953784942627,
      "learning_rate": 4.457994579945799e-05,
      "loss": 5.6783,
      "step": 80000
    },
    {
      "epoch": 109.07859078590786,
      "grad_norm": 11.30982494354248,
      "learning_rate": 4.454607046070461e-05,
      "loss": 5.679,
      "step": 80500
    },
    {
      "epoch": 109.7560975609756,
      "grad_norm": 9.001773834228516,
      "learning_rate": 4.451219512195122e-05,
      "loss": 5.6881,
      "step": 81000
    },
    {
      "epoch": 110.43360433604336,
      "grad_norm": 10.962065696716309,
      "learning_rate": 4.4478319783197837e-05,
      "loss": 5.6598,
      "step": 81500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 9.60132122039795,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 5.6873,
      "step": 82000
    },
    {
      "epoch": 111.78861788617886,
      "grad_norm": 9.14824104309082,
      "learning_rate": 4.4410569105691057e-05,
      "loss": 5.6482,
      "step": 82500
    },
    {
      "epoch": 112.46612466124661,
      "grad_norm": 7.440793514251709,
      "learning_rate": 4.4376693766937673e-05,
      "loss": 5.7015,
      "step": 83000
    },
    {
      "epoch": 113.14363143631437,
      "grad_norm": 8.676054000854492,
      "learning_rate": 4.4342818428184283e-05,
      "loss": 5.6775,
      "step": 83500
    },
    {
      "epoch": 113.82113821138212,
      "grad_norm": 10.077384948730469,
      "learning_rate": 4.43089430894309e-05,
      "loss": 5.6775,
      "step": 84000
    },
    {
      "epoch": 114.49864498644986,
      "grad_norm": 10.0122652053833,
      "learning_rate": 4.427506775067751e-05,
      "loss": 5.6795,
      "step": 84500
    },
    {
      "epoch": 115.17615176151762,
      "grad_norm": 8.472055435180664,
      "learning_rate": 4.424119241192412e-05,
      "loss": 5.6649,
      "step": 85000
    },
    {
      "epoch": 115.85365853658537,
      "grad_norm": 7.391278266906738,
      "learning_rate": 4.420731707317074e-05,
      "loss": 5.6827,
      "step": 85500
    },
    {
      "epoch": 116.53116531165311,
      "grad_norm": 9.232333183288574,
      "learning_rate": 4.417344173441735e-05,
      "loss": 5.6822,
      "step": 86000
    },
    {
      "epoch": 117.20867208672087,
      "grad_norm": 7.815216541290283,
      "learning_rate": 4.413956639566396e-05,
      "loss": 5.6691,
      "step": 86500
    },
    {
      "epoch": 117.88617886178862,
      "grad_norm": 8.958131790161133,
      "learning_rate": 4.410569105691057e-05,
      "loss": 5.6709,
      "step": 87000
    },
    {
      "epoch": 118.56368563685636,
      "grad_norm": 10.196486473083496,
      "learning_rate": 4.4071815718157184e-05,
      "loss": 5.6763,
      "step": 87500
    },
    {
      "epoch": 119.24119241192412,
      "grad_norm": 7.570178985595703,
      "learning_rate": 4.4037940379403794e-05,
      "loss": 5.6697,
      "step": 88000
    },
    {
      "epoch": 119.91869918699187,
      "grad_norm": 11.12653636932373,
      "learning_rate": 4.400406504065041e-05,
      "loss": 5.6838,
      "step": 88500
    },
    {
      "epoch": 120.59620596205961,
      "grad_norm": 7.945408821105957,
      "learning_rate": 4.397018970189702e-05,
      "loss": 5.6789,
      "step": 89000
    },
    {
      "epoch": 121.27371273712737,
      "grad_norm": 10.780491828918457,
      "learning_rate": 4.393631436314363e-05,
      "loss": 5.6862,
      "step": 89500
    },
    {
      "epoch": 121.95121951219512,
      "grad_norm": 12.257450103759766,
      "learning_rate": 4.390243902439025e-05,
      "loss": 5.683,
      "step": 90000
    },
    {
      "epoch": 122.62872628726288,
      "grad_norm": 10.77872085571289,
      "learning_rate": 4.386856368563686e-05,
      "loss": 5.6704,
      "step": 90500
    },
    {
      "epoch": 123.30623306233062,
      "grad_norm": 8.932414054870605,
      "learning_rate": 4.3834688346883474e-05,
      "loss": 5.6586,
      "step": 91000
    },
    {
      "epoch": 123.98373983739837,
      "grad_norm": 8.264118194580078,
      "learning_rate": 4.3800813008130085e-05,
      "loss": 5.6734,
      "step": 91500
    },
    {
      "epoch": 124.66124661246613,
      "grad_norm": 8.977347373962402,
      "learning_rate": 4.3766937669376695e-05,
      "loss": 5.6708,
      "step": 92000
    },
    {
      "epoch": 125.33875338753387,
      "grad_norm": 7.906527519226074,
      "learning_rate": 4.373306233062331e-05,
      "loss": 5.6883,
      "step": 92500
    },
    {
      "epoch": 126.01626016260163,
      "grad_norm": 11.390049934387207,
      "learning_rate": 4.369918699186992e-05,
      "loss": 5.6622,
      "step": 93000
    },
    {
      "epoch": 126.69376693766938,
      "grad_norm": 8.2907133102417,
      "learning_rate": 4.366531165311653e-05,
      "loss": 5.6695,
      "step": 93500
    },
    {
      "epoch": 127.37127371273712,
      "grad_norm": 15.299290657043457,
      "learning_rate": 4.363143631436314e-05,
      "loss": 5.6923,
      "step": 94000
    },
    {
      "epoch": 128.0487804878049,
      "grad_norm": 11.34947681427002,
      "learning_rate": 4.359756097560976e-05,
      "loss": 5.6475,
      "step": 94500
    },
    {
      "epoch": 128.72628726287263,
      "grad_norm": 8.687628746032715,
      "learning_rate": 4.3563685636856375e-05,
      "loss": 5.6752,
      "step": 95000
    },
    {
      "epoch": 129.40379403794037,
      "grad_norm": 7.222461223602295,
      "learning_rate": 4.3529810298102985e-05,
      "loss": 5.6421,
      "step": 95500
    },
    {
      "epoch": 130.08130081300814,
      "grad_norm": 16.749404907226562,
      "learning_rate": 4.3495934959349595e-05,
      "loss": 5.6976,
      "step": 96000
    },
    {
      "epoch": 130.75880758807588,
      "grad_norm": 7.7512898445129395,
      "learning_rate": 4.3462059620596205e-05,
      "loss": 5.6698,
      "step": 96500
    },
    {
      "epoch": 131.43631436314362,
      "grad_norm": 7.948211669921875,
      "learning_rate": 4.342818428184282e-05,
      "loss": 5.684,
      "step": 97000
    },
    {
      "epoch": 132.1138211382114,
      "grad_norm": 10.333013534545898,
      "learning_rate": 4.339430894308943e-05,
      "loss": 5.6608,
      "step": 97500
    },
    {
      "epoch": 132.79132791327913,
      "grad_norm": 12.077011108398438,
      "learning_rate": 4.336043360433605e-05,
      "loss": 5.6582,
      "step": 98000
    },
    {
      "epoch": 133.46883468834687,
      "grad_norm": 7.491289138793945,
      "learning_rate": 4.332655826558266e-05,
      "loss": 5.6638,
      "step": 98500
    },
    {
      "epoch": 134.14634146341464,
      "grad_norm": 18.78850555419922,
      "learning_rate": 4.329268292682927e-05,
      "loss": 5.693,
      "step": 99000
    },
    {
      "epoch": 134.82384823848238,
      "grad_norm": 11.873628616333008,
      "learning_rate": 4.3258807588075886e-05,
      "loss": 5.6734,
      "step": 99500
    },
    {
      "epoch": 135.50135501355012,
      "grad_norm": 6.847982406616211,
      "learning_rate": 4.3224932249322496e-05,
      "loss": 5.6551,
      "step": 100000
    },
    {
      "epoch": 136.1788617886179,
      "grad_norm": 10.645559310913086,
      "learning_rate": 4.3191056910569106e-05,
      "loss": 5.6491,
      "step": 100500
    },
    {
      "epoch": 136.85636856368563,
      "grad_norm": 9.155359268188477,
      "learning_rate": 4.315718157181572e-05,
      "loss": 5.6783,
      "step": 101000
    },
    {
      "epoch": 137.53387533875338,
      "grad_norm": 9.349299430847168,
      "learning_rate": 4.312330623306233e-05,
      "loss": 5.6559,
      "step": 101500
    },
    {
      "epoch": 138.21138211382114,
      "grad_norm": 8.001784324645996,
      "learning_rate": 4.308943089430895e-05,
      "loss": 5.6998,
      "step": 102000
    },
    {
      "epoch": 138.88888888888889,
      "grad_norm": 9.066689491271973,
      "learning_rate": 4.305555555555556e-05,
      "loss": 5.6705,
      "step": 102500
    },
    {
      "epoch": 139.56639566395663,
      "grad_norm": 8.939154624938965,
      "learning_rate": 4.302168021680217e-05,
      "loss": 5.6288,
      "step": 103000
    },
    {
      "epoch": 140.2439024390244,
      "grad_norm": 12.34425163269043,
      "learning_rate": 4.298780487804878e-05,
      "loss": 5.6979,
      "step": 103500
    },
    {
      "epoch": 140.92140921409214,
      "grad_norm": 12.495677947998047,
      "learning_rate": 4.2953929539295396e-05,
      "loss": 5.6732,
      "step": 104000
    },
    {
      "epoch": 141.59891598915988,
      "grad_norm": 7.094862461090088,
      "learning_rate": 4.2920054200542006e-05,
      "loss": 5.6865,
      "step": 104500
    },
    {
      "epoch": 142.27642276422765,
      "grad_norm": 14.707009315490723,
      "learning_rate": 4.2886178861788616e-05,
      "loss": 5.6574,
      "step": 105000
    },
    {
      "epoch": 142.9539295392954,
      "grad_norm": 7.82401704788208,
      "learning_rate": 4.285230352303523e-05,
      "loss": 5.6533,
      "step": 105500
    },
    {
      "epoch": 143.63143631436316,
      "grad_norm": 12.804460525512695,
      "learning_rate": 4.281842818428184e-05,
      "loss": 5.6438,
      "step": 106000
    },
    {
      "epoch": 144.3089430894309,
      "grad_norm": 16.44430160522461,
      "learning_rate": 4.278455284552846e-05,
      "loss": 5.6972,
      "step": 106500
    },
    {
      "epoch": 144.98644986449864,
      "grad_norm": 14.833091735839844,
      "learning_rate": 4.275067750677507e-05,
      "loss": 5.6578,
      "step": 107000
    },
    {
      "epoch": 145.6639566395664,
      "grad_norm": 11.061844825744629,
      "learning_rate": 4.271680216802168e-05,
      "loss": 5.6809,
      "step": 107500
    },
    {
      "epoch": 146.34146341463415,
      "grad_norm": 8.878724098205566,
      "learning_rate": 4.26829268292683e-05,
      "loss": 5.64,
      "step": 108000
    },
    {
      "epoch": 147.0189701897019,
      "grad_norm": 10.256917953491211,
      "learning_rate": 4.264905149051491e-05,
      "loss": 5.7029,
      "step": 108500
    },
    {
      "epoch": 147.69647696476966,
      "grad_norm": 7.7750244140625,
      "learning_rate": 4.2615176151761524e-05,
      "loss": 5.6495,
      "step": 109000
    },
    {
      "epoch": 148.3739837398374,
      "grad_norm": 12.138590812683105,
      "learning_rate": 4.2581300813008134e-05,
      "loss": 5.6479,
      "step": 109500
    },
    {
      "epoch": 149.05149051490514,
      "grad_norm": 11.45523738861084,
      "learning_rate": 4.2547425474254744e-05,
      "loss": 5.6925,
      "step": 110000
    },
    {
      "epoch": 149.7289972899729,
      "grad_norm": 7.21810245513916,
      "learning_rate": 4.2513550135501354e-05,
      "loss": 5.6429,
      "step": 110500
    },
    {
      "epoch": 150.40650406504065,
      "grad_norm": 7.866654396057129,
      "learning_rate": 4.247967479674797e-05,
      "loss": 5.6948,
      "step": 111000
    },
    {
      "epoch": 151.0840108401084,
      "grad_norm": 8.225414276123047,
      "learning_rate": 4.244579945799459e-05,
      "loss": 5.6728,
      "step": 111500
    },
    {
      "epoch": 151.76151761517616,
      "grad_norm": 9.346206665039062,
      "learning_rate": 4.241192411924119e-05,
      "loss": 5.6457,
      "step": 112000
    },
    {
      "epoch": 152.4390243902439,
      "grad_norm": 7.987818717956543,
      "learning_rate": 4.237804878048781e-05,
      "loss": 5.6619,
      "step": 112500
    },
    {
      "epoch": 153.11653116531164,
      "grad_norm": 6.3528876304626465,
      "learning_rate": 4.234417344173442e-05,
      "loss": 5.6738,
      "step": 113000
    },
    {
      "epoch": 153.7940379403794,
      "grad_norm": 8.427619934082031,
      "learning_rate": 4.2310298102981034e-05,
      "loss": 5.6466,
      "step": 113500
    },
    {
      "epoch": 154.47154471544715,
      "grad_norm": 6.445390701293945,
      "learning_rate": 4.2276422764227644e-05,
      "loss": 5.6817,
      "step": 114000
    },
    {
      "epoch": 155.1490514905149,
      "grad_norm": 10.24166488647461,
      "learning_rate": 4.2242547425474254e-05,
      "loss": 5.6673,
      "step": 114500
    },
    {
      "epoch": 155.82655826558266,
      "grad_norm": 9.981703758239746,
      "learning_rate": 4.220867208672087e-05,
      "loss": 5.6608,
      "step": 115000
    },
    {
      "epoch": 156.5040650406504,
      "grad_norm": 9.267243385314941,
      "learning_rate": 4.217479674796748e-05,
      "loss": 5.6548,
      "step": 115500
    },
    {
      "epoch": 157.18157181571814,
      "grad_norm": 10.711848258972168,
      "learning_rate": 4.21409214092141e-05,
      "loss": 5.6473,
      "step": 116000
    },
    {
      "epoch": 157.8590785907859,
      "grad_norm": 12.067351341247559,
      "learning_rate": 4.21070460704607e-05,
      "loss": 5.6985,
      "step": 116500
    },
    {
      "epoch": 158.53658536585365,
      "grad_norm": 8.329667091369629,
      "learning_rate": 4.207317073170732e-05,
      "loss": 5.6546,
      "step": 117000
    },
    {
      "epoch": 159.21409214092142,
      "grad_norm": 8.025691032409668,
      "learning_rate": 4.2039295392953935e-05,
      "loss": 5.6679,
      "step": 117500
    },
    {
      "epoch": 159.89159891598916,
      "grad_norm": 8.354793548583984,
      "learning_rate": 4.2005420054200545e-05,
      "loss": 5.657,
      "step": 118000
    },
    {
      "epoch": 160.5691056910569,
      "grad_norm": 6.5379743576049805,
      "learning_rate": 4.197154471544716e-05,
      "loss": 5.6516,
      "step": 118500
    },
    {
      "epoch": 161.24661246612467,
      "grad_norm": 7.582290172576904,
      "learning_rate": 4.1937669376693765e-05,
      "loss": 5.6638,
      "step": 119000
    },
    {
      "epoch": 161.92411924119241,
      "grad_norm": 8.392108917236328,
      "learning_rate": 4.190379403794038e-05,
      "loss": 5.6486,
      "step": 119500
    },
    {
      "epoch": 162.60162601626016,
      "grad_norm": 12.599149703979492,
      "learning_rate": 4.186991869918699e-05,
      "loss": 5.6889,
      "step": 120000
    },
    {
      "epoch": 163.27913279132792,
      "grad_norm": 6.67156457901001,
      "learning_rate": 4.183604336043361e-05,
      "loss": 5.6781,
      "step": 120500
    },
    {
      "epoch": 163.95663956639567,
      "grad_norm": 6.853057384490967,
      "learning_rate": 4.180216802168022e-05,
      "loss": 5.6397,
      "step": 121000
    },
    {
      "epoch": 164.6341463414634,
      "grad_norm": 8.395889282226562,
      "learning_rate": 4.176829268292683e-05,
      "loss": 5.645,
      "step": 121500
    },
    {
      "epoch": 165.31165311653118,
      "grad_norm": 8.158552169799805,
      "learning_rate": 4.1734417344173445e-05,
      "loss": 5.7062,
      "step": 122000
    },
    {
      "epoch": 165.98915989159892,
      "grad_norm": 7.307242393493652,
      "learning_rate": 4.1700542005420055e-05,
      "loss": 5.6388,
      "step": 122500
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 10.447093963623047,
      "learning_rate": 4.166666666666667e-05,
      "loss": 5.654,
      "step": 123000
    },
    {
      "epoch": 167.34417344173443,
      "grad_norm": 9.50565242767334,
      "learning_rate": 4.1632791327913275e-05,
      "loss": 5.662,
      "step": 123500
    },
    {
      "epoch": 168.02168021680217,
      "grad_norm": 6.1766862869262695,
      "learning_rate": 4.159891598915989e-05,
      "loss": 5.6712,
      "step": 124000
    },
    {
      "epoch": 168.6991869918699,
      "grad_norm": 11.88149642944336,
      "learning_rate": 4.156504065040651e-05,
      "loss": 5.6642,
      "step": 124500
    },
    {
      "epoch": 169.37669376693768,
      "grad_norm": 9.362071990966797,
      "learning_rate": 4.153116531165312e-05,
      "loss": 5.6449,
      "step": 125000
    },
    {
      "epoch": 170.05420054200542,
      "grad_norm": 8.339675903320312,
      "learning_rate": 4.1497289972899736e-05,
      "loss": 5.664,
      "step": 125500
    },
    {
      "epoch": 170.73170731707316,
      "grad_norm": 15.789373397827148,
      "learning_rate": 4.146341463414634e-05,
      "loss": 5.6478,
      "step": 126000
    },
    {
      "epoch": 171.40921409214093,
      "grad_norm": 6.899135112762451,
      "learning_rate": 4.1429539295392956e-05,
      "loss": 5.6431,
      "step": 126500
    },
    {
      "epoch": 172.08672086720867,
      "grad_norm": 8.920034408569336,
      "learning_rate": 4.1395663956639566e-05,
      "loss": 5.6842,
      "step": 127000
    },
    {
      "epoch": 172.7642276422764,
      "grad_norm": 7.965541839599609,
      "learning_rate": 4.136178861788618e-05,
      "loss": 5.6805,
      "step": 127500
    },
    {
      "epoch": 173.44173441734418,
      "grad_norm": 16.888904571533203,
      "learning_rate": 4.132791327913279e-05,
      "loss": 5.6693,
      "step": 128000
    },
    {
      "epoch": 174.11924119241192,
      "grad_norm": 9.602978706359863,
      "learning_rate": 4.12940379403794e-05,
      "loss": 5.6099,
      "step": 128500
    },
    {
      "epoch": 174.79674796747966,
      "grad_norm": 9.29850959777832,
      "learning_rate": 4.126016260162602e-05,
      "loss": 5.6672,
      "step": 129000
    },
    {
      "epoch": 175.47425474254743,
      "grad_norm": 7.787965297698975,
      "learning_rate": 4.122628726287263e-05,
      "loss": 5.6557,
      "step": 129500
    },
    {
      "epoch": 176.15176151761517,
      "grad_norm": 8.20883846282959,
      "learning_rate": 4.1192411924119246e-05,
      "loss": 5.6861,
      "step": 130000
    },
    {
      "epoch": 176.82926829268294,
      "grad_norm": 9.473078727722168,
      "learning_rate": 4.1158536585365856e-05,
      "loss": 5.6377,
      "step": 130500
    },
    {
      "epoch": 177.50677506775068,
      "grad_norm": 7.341607093811035,
      "learning_rate": 4.1124661246612466e-05,
      "loss": 5.6604,
      "step": 131000
    },
    {
      "epoch": 178.18428184281842,
      "grad_norm": 8.631403923034668,
      "learning_rate": 4.109078590785908e-05,
      "loss": 5.6325,
      "step": 131500
    },
    {
      "epoch": 178.8617886178862,
      "grad_norm": 7.949875831604004,
      "learning_rate": 4.105691056910569e-05,
      "loss": 5.6564,
      "step": 132000
    },
    {
      "epoch": 179.53929539295393,
      "grad_norm": 8.560153007507324,
      "learning_rate": 4.102303523035231e-05,
      "loss": 5.7206,
      "step": 132500
    },
    {
      "epoch": 180.21680216802167,
      "grad_norm": 10.500035285949707,
      "learning_rate": 4.098915989159891e-05,
      "loss": 5.6112,
      "step": 133000
    },
    {
      "epoch": 180.89430894308944,
      "grad_norm": 10.8402099609375,
      "learning_rate": 4.095528455284553e-05,
      "loss": 5.6569,
      "step": 133500
    },
    {
      "epoch": 181.57181571815718,
      "grad_norm": 10.266831398010254,
      "learning_rate": 4.092140921409214e-05,
      "loss": 5.6663,
      "step": 134000
    },
    {
      "epoch": 182.24932249322492,
      "grad_norm": 8.167292594909668,
      "learning_rate": 4.088753387533876e-05,
      "loss": 5.6518,
      "step": 134500
    },
    {
      "epoch": 182.9268292682927,
      "grad_norm": 7.827050685882568,
      "learning_rate": 4.085365853658537e-05,
      "loss": 5.6641,
      "step": 135000
    },
    {
      "epoch": 183.60433604336043,
      "grad_norm": 7.57320499420166,
      "learning_rate": 4.081978319783198e-05,
      "loss": 5.6534,
      "step": 135500
    },
    {
      "epoch": 184.28184281842817,
      "grad_norm": 9.93310260772705,
      "learning_rate": 4.0785907859078594e-05,
      "loss": 5.704,
      "step": 136000
    },
    {
      "epoch": 184.95934959349594,
      "grad_norm": 6.780989170074463,
      "learning_rate": 4.0752032520325204e-05,
      "loss": 5.6179,
      "step": 136500
    },
    {
      "epoch": 185.63685636856368,
      "grad_norm": 12.014443397521973,
      "learning_rate": 4.071815718157182e-05,
      "loss": 5.6593,
      "step": 137000
    },
    {
      "epoch": 186.31436314363143,
      "grad_norm": 9.791462898254395,
      "learning_rate": 4.068428184281843e-05,
      "loss": 5.6654,
      "step": 137500
    },
    {
      "epoch": 186.9918699186992,
      "grad_norm": 11.05911636352539,
      "learning_rate": 4.065040650406504e-05,
      "loss": 5.6482,
      "step": 138000
    },
    {
      "epoch": 187.66937669376694,
      "grad_norm": 8.499839782714844,
      "learning_rate": 4.061653116531166e-05,
      "loss": 5.6758,
      "step": 138500
    },
    {
      "epoch": 188.34688346883468,
      "grad_norm": 7.396834850311279,
      "learning_rate": 4.058265582655827e-05,
      "loss": 5.6485,
      "step": 139000
    },
    {
      "epoch": 189.02439024390245,
      "grad_norm": 11.54627513885498,
      "learning_rate": 4.0548780487804884e-05,
      "loss": 5.6542,
      "step": 139500
    },
    {
      "epoch": 189.7018970189702,
      "grad_norm": 9.54122257232666,
      "learning_rate": 4.051490514905149e-05,
      "loss": 5.6452,
      "step": 140000
    },
    {
      "epoch": 190.37940379403793,
      "grad_norm": 8.34105110168457,
      "learning_rate": 4.0481029810298104e-05,
      "loss": 5.6446,
      "step": 140500
    },
    {
      "epoch": 191.0569105691057,
      "grad_norm": 6.819817543029785,
      "learning_rate": 4.044715447154472e-05,
      "loss": 5.6739,
      "step": 141000
    },
    {
      "epoch": 191.73441734417344,
      "grad_norm": 15.750468254089355,
      "learning_rate": 4.041327913279133e-05,
      "loss": 5.6413,
      "step": 141500
    },
    {
      "epoch": 192.4119241192412,
      "grad_norm": 9.076817512512207,
      "learning_rate": 4.037940379403794e-05,
      "loss": 5.6487,
      "step": 142000
    },
    {
      "epoch": 193.08943089430895,
      "grad_norm": 8.491498947143555,
      "learning_rate": 4.034552845528455e-05,
      "loss": 5.6549,
      "step": 142500
    },
    {
      "epoch": 193.7669376693767,
      "grad_norm": 8.075066566467285,
      "learning_rate": 4.031165311653117e-05,
      "loss": 5.6699,
      "step": 143000
    },
    {
      "epoch": 194.44444444444446,
      "grad_norm": 10.725753784179688,
      "learning_rate": 4.027777777777778e-05,
      "loss": 5.6544,
      "step": 143500
    },
    {
      "epoch": 195.1219512195122,
      "grad_norm": 6.903799533843994,
      "learning_rate": 4.0243902439024395e-05,
      "loss": 5.6535,
      "step": 144000
    },
    {
      "epoch": 195.79945799457994,
      "grad_norm": 9.128844261169434,
      "learning_rate": 4.0210027100271005e-05,
      "loss": 5.6779,
      "step": 144500
    },
    {
      "epoch": 196.4769647696477,
      "grad_norm": 8.367902755737305,
      "learning_rate": 4.0176151761517615e-05,
      "loss": 5.627,
      "step": 145000
    },
    {
      "epoch": 197.15447154471545,
      "grad_norm": 9.316651344299316,
      "learning_rate": 4.014227642276423e-05,
      "loss": 5.647,
      "step": 145500
    },
    {
      "epoch": 197.8319783197832,
      "grad_norm": 10.669677734375,
      "learning_rate": 4.010840108401084e-05,
      "loss": 5.6503,
      "step": 146000
    },
    {
      "epoch": 198.50948509485096,
      "grad_norm": 7.944241046905518,
      "learning_rate": 4.007452574525745e-05,
      "loss": 5.6784,
      "step": 146500
    },
    {
      "epoch": 199.1869918699187,
      "grad_norm": 7.175065517425537,
      "learning_rate": 4.004065040650407e-05,
      "loss": 5.6314,
      "step": 147000
    },
    {
      "epoch": 199.86449864498644,
      "grad_norm": 11.005393981933594,
      "learning_rate": 4.000677506775068e-05,
      "loss": 5.656,
      "step": 147500
    },
    {
      "epoch": 200.5420054200542,
      "grad_norm": 10.021561622619629,
      "learning_rate": 3.9972899728997295e-05,
      "loss": 5.6573,
      "step": 148000
    },
    {
      "epoch": 201.21951219512195,
      "grad_norm": 11.198566436767578,
      "learning_rate": 3.9939024390243905e-05,
      "loss": 5.6437,
      "step": 148500
    },
    {
      "epoch": 201.8970189701897,
      "grad_norm": 14.999792098999023,
      "learning_rate": 3.9905149051490515e-05,
      "loss": 5.6599,
      "step": 149000
    },
    {
      "epoch": 202.57452574525746,
      "grad_norm": 8.5000638961792,
      "learning_rate": 3.9871273712737125e-05,
      "loss": 5.6694,
      "step": 149500
    },
    {
      "epoch": 203.2520325203252,
      "grad_norm": 12.03326416015625,
      "learning_rate": 3.983739837398374e-05,
      "loss": 5.6339,
      "step": 150000
    },
    {
      "epoch": 203.92953929539294,
      "grad_norm": 8.606671333312988,
      "learning_rate": 3.980352303523035e-05,
      "loss": 5.6621,
      "step": 150500
    },
    {
      "epoch": 204.6070460704607,
      "grad_norm": 9.03147029876709,
      "learning_rate": 3.976964769647697e-05,
      "loss": 5.6437,
      "step": 151000
    },
    {
      "epoch": 205.28455284552845,
      "grad_norm": 8.310561180114746,
      "learning_rate": 3.973577235772358e-05,
      "loss": 5.6406,
      "step": 151500
    },
    {
      "epoch": 205.9620596205962,
      "grad_norm": 11.409261703491211,
      "learning_rate": 3.970189701897019e-05,
      "loss": 5.6758,
      "step": 152000
    },
    {
      "epoch": 206.63956639566396,
      "grad_norm": 10.863382339477539,
      "learning_rate": 3.9668021680216806e-05,
      "loss": 5.6323,
      "step": 152500
    },
    {
      "epoch": 207.3170731707317,
      "grad_norm": 7.978518486022949,
      "learning_rate": 3.9634146341463416e-05,
      "loss": 5.6937,
      "step": 153000
    },
    {
      "epoch": 207.99457994579944,
      "grad_norm": 9.219703674316406,
      "learning_rate": 3.9600271002710026e-05,
      "loss": 5.6548,
      "step": 153500
    },
    {
      "epoch": 208.6720867208672,
      "grad_norm": 8.745495796203613,
      "learning_rate": 3.956639566395664e-05,
      "loss": 5.6468,
      "step": 154000
    },
    {
      "epoch": 209.34959349593495,
      "grad_norm": 8.665778160095215,
      "learning_rate": 3.953252032520325e-05,
      "loss": 5.6554,
      "step": 154500
    },
    {
      "epoch": 210.02710027100272,
      "grad_norm": 5.874342918395996,
      "learning_rate": 3.949864498644987e-05,
      "loss": 5.6384,
      "step": 155000
    },
    {
      "epoch": 210.70460704607046,
      "grad_norm": 8.319355010986328,
      "learning_rate": 3.946476964769648e-05,
      "loss": 5.6451,
      "step": 155500
    },
    {
      "epoch": 211.3821138211382,
      "grad_norm": 11.407994270324707,
      "learning_rate": 3.943089430894309e-05,
      "loss": 5.6659,
      "step": 156000
    },
    {
      "epoch": 212.05962059620597,
      "grad_norm": 8.689823150634766,
      "learning_rate": 3.93970189701897e-05,
      "loss": 5.6514,
      "step": 156500
    },
    {
      "epoch": 212.73712737127371,
      "grad_norm": 14.360003471374512,
      "learning_rate": 3.9363143631436316e-05,
      "loss": 5.6562,
      "step": 157000
    },
    {
      "epoch": 213.41463414634146,
      "grad_norm": 12.288654327392578,
      "learning_rate": 3.932926829268293e-05,
      "loss": 5.6515,
      "step": 157500
    },
    {
      "epoch": 214.09214092140923,
      "grad_norm": 11.783580780029297,
      "learning_rate": 3.9295392953929537e-05,
      "loss": 5.6496,
      "step": 158000
    },
    {
      "epoch": 214.76964769647697,
      "grad_norm": 6.4767255783081055,
      "learning_rate": 3.926151761517615e-05,
      "loss": 5.6512,
      "step": 158500
    },
    {
      "epoch": 215.4471544715447,
      "grad_norm": 9.947468757629395,
      "learning_rate": 3.922764227642276e-05,
      "loss": 5.6097,
      "step": 159000
    },
    {
      "epoch": 216.12466124661248,
      "grad_norm": 11.325965881347656,
      "learning_rate": 3.919376693766938e-05,
      "loss": 5.6951,
      "step": 159500
    },
    {
      "epoch": 216.80216802168022,
      "grad_norm": 9.506473541259766,
      "learning_rate": 3.915989159891599e-05,
      "loss": 5.6343,
      "step": 160000
    },
    {
      "epoch": 217.47967479674796,
      "grad_norm": 6.306151866912842,
      "learning_rate": 3.91260162601626e-05,
      "loss": 5.6673,
      "step": 160500
    },
    {
      "epoch": 218.15718157181573,
      "grad_norm": 11.059097290039062,
      "learning_rate": 3.909214092140922e-05,
      "loss": 5.6666,
      "step": 161000
    },
    {
      "epoch": 218.83468834688347,
      "grad_norm": 8.186347961425781,
      "learning_rate": 3.905826558265583e-05,
      "loss": 5.6451,
      "step": 161500
    },
    {
      "epoch": 219.5121951219512,
      "grad_norm": 13.263635635375977,
      "learning_rate": 3.9024390243902444e-05,
      "loss": 5.661,
      "step": 162000
    },
    {
      "epoch": 220.18970189701898,
      "grad_norm": 8.844897270202637,
      "learning_rate": 3.8990514905149054e-05,
      "loss": 5.6398,
      "step": 162500
    },
    {
      "epoch": 220.86720867208672,
      "grad_norm": 8.628641128540039,
      "learning_rate": 3.8956639566395664e-05,
      "loss": 5.6352,
      "step": 163000
    },
    {
      "epoch": 221.54471544715446,
      "grad_norm": 10.12667179107666,
      "learning_rate": 3.892276422764228e-05,
      "loss": 5.6349,
      "step": 163500
    },
    {
      "epoch": 222.22222222222223,
      "grad_norm": 7.523834705352783,
      "learning_rate": 3.888888888888889e-05,
      "loss": 5.6931,
      "step": 164000
    },
    {
      "epoch": 222.89972899728997,
      "grad_norm": 8.079325675964355,
      "learning_rate": 3.885501355013551e-05,
      "loss": 5.6395,
      "step": 164500
    },
    {
      "epoch": 223.5772357723577,
      "grad_norm": 9.81254768371582,
      "learning_rate": 3.882113821138211e-05,
      "loss": 5.6883,
      "step": 165000
    },
    {
      "epoch": 224.25474254742548,
      "grad_norm": 8.419825553894043,
      "learning_rate": 3.878726287262873e-05,
      "loss": 5.6472,
      "step": 165500
    },
    {
      "epoch": 224.93224932249322,
      "grad_norm": 13.789162635803223,
      "learning_rate": 3.875338753387534e-05,
      "loss": 5.6378,
      "step": 166000
    },
    {
      "epoch": 225.609756097561,
      "grad_norm": 8.11507797241211,
      "learning_rate": 3.8719512195121954e-05,
      "loss": 5.652,
      "step": 166500
    },
    {
      "epoch": 226.28726287262873,
      "grad_norm": 7.329874515533447,
      "learning_rate": 3.8685636856368564e-05,
      "loss": 5.6791,
      "step": 167000
    },
    {
      "epoch": 226.96476964769647,
      "grad_norm": 11.427009582519531,
      "learning_rate": 3.8651761517615174e-05,
      "loss": 5.6205,
      "step": 167500
    },
    {
      "epoch": 227.64227642276424,
      "grad_norm": 7.879823207855225,
      "learning_rate": 3.861788617886179e-05,
      "loss": 5.619,
      "step": 168000
    },
    {
      "epoch": 228.31978319783198,
      "grad_norm": 12.941329002380371,
      "learning_rate": 3.85840108401084e-05,
      "loss": 5.672,
      "step": 168500
    },
    {
      "epoch": 228.99728997289972,
      "grad_norm": 10.653696060180664,
      "learning_rate": 3.855013550135502e-05,
      "loss": 5.654,
      "step": 169000
    },
    {
      "epoch": 229.6747967479675,
      "grad_norm": 8.766142845153809,
      "learning_rate": 3.851626016260163e-05,
      "loss": 5.6443,
      "step": 169500
    },
    {
      "epoch": 230.35230352303523,
      "grad_norm": 7.289042949676514,
      "learning_rate": 3.848238482384824e-05,
      "loss": 5.6524,
      "step": 170000
    },
    {
      "epoch": 231.02981029810297,
      "grad_norm": 9.484582901000977,
      "learning_rate": 3.8448509485094855e-05,
      "loss": 5.6491,
      "step": 170500
    },
    {
      "epoch": 231.70731707317074,
      "grad_norm": 11.203863143920898,
      "learning_rate": 3.8414634146341465e-05,
      "loss": 5.6457,
      "step": 171000
    },
    {
      "epoch": 232.38482384823848,
      "grad_norm": 6.91785192489624,
      "learning_rate": 3.838075880758808e-05,
      "loss": 5.6363,
      "step": 171500
    },
    {
      "epoch": 233.06233062330622,
      "grad_norm": 8.789227485656738,
      "learning_rate": 3.8346883468834685e-05,
      "loss": 5.6626,
      "step": 172000
    },
    {
      "epoch": 233.739837398374,
      "grad_norm": 8.564908981323242,
      "learning_rate": 3.83130081300813e-05,
      "loss": 5.6667,
      "step": 172500
    },
    {
      "epoch": 234.41734417344173,
      "grad_norm": 6.6852707862854,
      "learning_rate": 3.827913279132791e-05,
      "loss": 5.6319,
      "step": 173000
    },
    {
      "epoch": 235.09485094850947,
      "grad_norm": 15.418218612670898,
      "learning_rate": 3.824525745257453e-05,
      "loss": 5.6361,
      "step": 173500
    },
    {
      "epoch": 235.77235772357724,
      "grad_norm": 7.55927038192749,
      "learning_rate": 3.8211382113821145e-05,
      "loss": 5.6453,
      "step": 174000
    },
    {
      "epoch": 236.44986449864498,
      "grad_norm": 7.34635591506958,
      "learning_rate": 3.817750677506775e-05,
      "loss": 5.6315,
      "step": 174500
    },
    {
      "epoch": 237.12737127371273,
      "grad_norm": 7.687874794006348,
      "learning_rate": 3.8143631436314366e-05,
      "loss": 5.6566,
      "step": 175000
    },
    {
      "epoch": 237.8048780487805,
      "grad_norm": 7.580517292022705,
      "learning_rate": 3.8109756097560976e-05,
      "loss": 5.6403,
      "step": 175500
    },
    {
      "epoch": 238.48238482384824,
      "grad_norm": 21.376815795898438,
      "learning_rate": 3.807588075880759e-05,
      "loss": 5.6667,
      "step": 176000
    },
    {
      "epoch": 239.15989159891598,
      "grad_norm": 6.582630634307861,
      "learning_rate": 3.80420054200542e-05,
      "loss": 5.6487,
      "step": 176500
    },
    {
      "epoch": 239.83739837398375,
      "grad_norm": 10.589590072631836,
      "learning_rate": 3.800813008130081e-05,
      "loss": 5.6495,
      "step": 177000
    },
    {
      "epoch": 240.5149051490515,
      "grad_norm": 8.026455879211426,
      "learning_rate": 3.797425474254743e-05,
      "loss": 5.6245,
      "step": 177500
    },
    {
      "epoch": 241.19241192411923,
      "grad_norm": 11.354104995727539,
      "learning_rate": 3.794037940379404e-05,
      "loss": 5.6711,
      "step": 178000
    },
    {
      "epoch": 241.869918699187,
      "grad_norm": 8.525466918945312,
      "learning_rate": 3.7906504065040656e-05,
      "loss": 5.6458,
      "step": 178500
    },
    {
      "epoch": 242.54742547425474,
      "grad_norm": 11.042203903198242,
      "learning_rate": 3.787262872628726e-05,
      "loss": 5.6454,
      "step": 179000
    },
    {
      "epoch": 243.2249322493225,
      "grad_norm": 6.108914375305176,
      "learning_rate": 3.7838753387533876e-05,
      "loss": 5.6552,
      "step": 179500
    },
    {
      "epoch": 243.90243902439025,
      "grad_norm": 9.15170955657959,
      "learning_rate": 3.780487804878049e-05,
      "loss": 5.6325,
      "step": 180000
    },
    {
      "epoch": 244.579945799458,
      "grad_norm": 9.118218421936035,
      "learning_rate": 3.77710027100271e-05,
      "loss": 5.6671,
      "step": 180500
    },
    {
      "epoch": 245.25745257452576,
      "grad_norm": 7.162714004516602,
      "learning_rate": 3.773712737127372e-05,
      "loss": 5.6436,
      "step": 181000
    },
    {
      "epoch": 245.9349593495935,
      "grad_norm": 14.006855010986328,
      "learning_rate": 3.770325203252032e-05,
      "loss": 5.6345,
      "step": 181500
    },
    {
      "epoch": 246.61246612466124,
      "grad_norm": 9.820459365844727,
      "learning_rate": 3.766937669376694e-05,
      "loss": 5.6191,
      "step": 182000
    },
    {
      "epoch": 247.289972899729,
      "grad_norm": 7.360935688018799,
      "learning_rate": 3.763550135501355e-05,
      "loss": 5.6888,
      "step": 182500
    },
    {
      "epoch": 247.96747967479675,
      "grad_norm": 10.865755081176758,
      "learning_rate": 3.760162601626017e-05,
      "loss": 5.6179,
      "step": 183000
    },
    {
      "epoch": 248.6449864498645,
      "grad_norm": 9.121543884277344,
      "learning_rate": 3.756775067750678e-05,
      "loss": 5.6372,
      "step": 183500
    },
    {
      "epoch": 249.32249322493226,
      "grad_norm": 10.905919075012207,
      "learning_rate": 3.753387533875339e-05,
      "loss": 5.6463,
      "step": 184000
    },
    {
      "epoch": 250.0,
      "grad_norm": 7.210629463195801,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 5.6479,
      "step": 184500
    },
    {
      "epoch": 250.67750677506774,
      "grad_norm": 7.172852993011475,
      "learning_rate": 3.7466124661246614e-05,
      "loss": 5.6634,
      "step": 185000
    },
    {
      "epoch": 251.3550135501355,
      "grad_norm": 8.932280540466309,
      "learning_rate": 3.743224932249323e-05,
      "loss": 5.6469,
      "step": 185500
    },
    {
      "epoch": 252.03252032520325,
      "grad_norm": 9.475967407226562,
      "learning_rate": 3.739837398373984e-05,
      "loss": 5.6372,
      "step": 186000
    },
    {
      "epoch": 252.710027100271,
      "grad_norm": 8.934803009033203,
      "learning_rate": 3.736449864498645e-05,
      "loss": 5.6115,
      "step": 186500
    },
    {
      "epoch": 253.38753387533876,
      "grad_norm": 10.826531410217285,
      "learning_rate": 3.733062330623307e-05,
      "loss": 5.6811,
      "step": 187000
    },
    {
      "epoch": 254.0650406504065,
      "grad_norm": 6.963500022888184,
      "learning_rate": 3.729674796747968e-05,
      "loss": 5.6258,
      "step": 187500
    },
    {
      "epoch": 254.74254742547424,
      "grad_norm": 8.574974060058594,
      "learning_rate": 3.726287262872629e-05,
      "loss": 5.6465,
      "step": 188000
    },
    {
      "epoch": 255.420054200542,
      "grad_norm": 7.295594215393066,
      "learning_rate": 3.72289972899729e-05,
      "loss": 5.6509,
      "step": 188500
    },
    {
      "epoch": 256.0975609756098,
      "grad_norm": 9.164753913879395,
      "learning_rate": 3.7195121951219514e-05,
      "loss": 5.6258,
      "step": 189000
    },
    {
      "epoch": 256.7750677506775,
      "grad_norm": 8.523214340209961,
      "learning_rate": 3.7161246612466124e-05,
      "loss": 5.6235,
      "step": 189500
    },
    {
      "epoch": 257.45257452574526,
      "grad_norm": 8.026851654052734,
      "learning_rate": 3.712737127371274e-05,
      "loss": 5.6746,
      "step": 190000
    },
    {
      "epoch": 258.130081300813,
      "grad_norm": 7.911179065704346,
      "learning_rate": 3.709349593495935e-05,
      "loss": 5.6463,
      "step": 190500
    },
    {
      "epoch": 258.80758807588074,
      "grad_norm": 8.934882164001465,
      "learning_rate": 3.705962059620596e-05,
      "loss": 5.633,
      "step": 191000
    },
    {
      "epoch": 259.4850948509485,
      "grad_norm": 8.663004875183105,
      "learning_rate": 3.702574525745258e-05,
      "loss": 5.6615,
      "step": 191500
    },
    {
      "epoch": 260.1626016260163,
      "grad_norm": 10.49855899810791,
      "learning_rate": 3.699186991869919e-05,
      "loss": 5.6469,
      "step": 192000
    },
    {
      "epoch": 260.840108401084,
      "grad_norm": 10.633378982543945,
      "learning_rate": 3.6957994579945805e-05,
      "loss": 5.6505,
      "step": 192500
    },
    {
      "epoch": 261.51761517615176,
      "grad_norm": 7.790566921234131,
      "learning_rate": 3.6924119241192415e-05,
      "loss": 5.6356,
      "step": 193000
    },
    {
      "epoch": 262.1951219512195,
      "grad_norm": 8.197284698486328,
      "learning_rate": 3.6890243902439025e-05,
      "loss": 5.6502,
      "step": 193500
    },
    {
      "epoch": 262.87262872628725,
      "grad_norm": 7.55950927734375,
      "learning_rate": 3.685636856368564e-05,
      "loss": 5.639,
      "step": 194000
    },
    {
      "epoch": 263.550135501355,
      "grad_norm": 8.10146713256836,
      "learning_rate": 3.682249322493225e-05,
      "loss": 5.6498,
      "step": 194500
    },
    {
      "epoch": 264.2276422764228,
      "grad_norm": 8.515738487243652,
      "learning_rate": 3.678861788617886e-05,
      "loss": 5.6642,
      "step": 195000
    },
    {
      "epoch": 264.9051490514905,
      "grad_norm": 9.482948303222656,
      "learning_rate": 3.675474254742547e-05,
      "loss": 5.6172,
      "step": 195500
    },
    {
      "epoch": 265.58265582655827,
      "grad_norm": 6.859283447265625,
      "learning_rate": 3.672086720867209e-05,
      "loss": 5.6578,
      "step": 196000
    },
    {
      "epoch": 266.260162601626,
      "grad_norm": 12.24489974975586,
      "learning_rate": 3.6686991869918705e-05,
      "loss": 5.6246,
      "step": 196500
    },
    {
      "epoch": 266.93766937669375,
      "grad_norm": 9.491538047790527,
      "learning_rate": 3.6653116531165315e-05,
      "loss": 5.6504,
      "step": 197000
    },
    {
      "epoch": 267.61517615176155,
      "grad_norm": 9.953487396240234,
      "learning_rate": 3.6619241192411925e-05,
      "loss": 5.6559,
      "step": 197500
    },
    {
      "epoch": 268.2926829268293,
      "grad_norm": 9.916776657104492,
      "learning_rate": 3.6585365853658535e-05,
      "loss": 5.6371,
      "step": 198000
    },
    {
      "epoch": 268.970189701897,
      "grad_norm": 8.83138656616211,
      "learning_rate": 3.655149051490515e-05,
      "loss": 5.6377,
      "step": 198500
    },
    {
      "epoch": 269.64769647696477,
      "grad_norm": 8.935249328613281,
      "learning_rate": 3.651761517615176e-05,
      "loss": 5.6355,
      "step": 199000
    },
    {
      "epoch": 270.3252032520325,
      "grad_norm": 13.528648376464844,
      "learning_rate": 3.648373983739837e-05,
      "loss": 5.6482,
      "step": 199500
    },
    {
      "epoch": 271.00271002710025,
      "grad_norm": 15.816442489624023,
      "learning_rate": 3.644986449864499e-05,
      "loss": 5.6583,
      "step": 200000
    },
    {
      "epoch": 271.68021680216805,
      "grad_norm": 7.723995685577393,
      "learning_rate": 3.64159891598916e-05,
      "loss": 5.6204,
      "step": 200500
    },
    {
      "epoch": 272.3577235772358,
      "grad_norm": 7.0365071296691895,
      "learning_rate": 3.6382113821138216e-05,
      "loss": 5.636,
      "step": 201000
    },
    {
      "epoch": 273.03523035230353,
      "grad_norm": 8.030179977416992,
      "learning_rate": 3.6348238482384826e-05,
      "loss": 5.6637,
      "step": 201500
    },
    {
      "epoch": 273.71273712737127,
      "grad_norm": 5.488485336303711,
      "learning_rate": 3.6314363143631436e-05,
      "loss": 5.6323,
      "step": 202000
    },
    {
      "epoch": 274.390243902439,
      "grad_norm": 10.184182167053223,
      "learning_rate": 3.628048780487805e-05,
      "loss": 5.6544,
      "step": 202500
    },
    {
      "epoch": 275.06775067750675,
      "grad_norm": 11.952322006225586,
      "learning_rate": 3.624661246612466e-05,
      "loss": 5.6358,
      "step": 203000
    },
    {
      "epoch": 275.74525745257455,
      "grad_norm": 11.470171928405762,
      "learning_rate": 3.621273712737128e-05,
      "loss": 5.6406,
      "step": 203500
    },
    {
      "epoch": 276.4227642276423,
      "grad_norm": 7.208085060119629,
      "learning_rate": 3.617886178861789e-05,
      "loss": 5.6567,
      "step": 204000
    },
    {
      "epoch": 277.10027100271003,
      "grad_norm": 15.083871841430664,
      "learning_rate": 3.61449864498645e-05,
      "loss": 5.6287,
      "step": 204500
    },
    {
      "epoch": 277.77777777777777,
      "grad_norm": 13.283829689025879,
      "learning_rate": 3.611111111111111e-05,
      "loss": 5.6675,
      "step": 205000
    },
    {
      "epoch": 278.4552845528455,
      "grad_norm": 8.447561264038086,
      "learning_rate": 3.6077235772357726e-05,
      "loss": 5.6419,
      "step": 205500
    },
    {
      "epoch": 279.13279132791325,
      "grad_norm": 8.673116683959961,
      "learning_rate": 3.6043360433604336e-05,
      "loss": 5.6225,
      "step": 206000
    },
    {
      "epoch": 279.81029810298105,
      "grad_norm": 8.276495933532715,
      "learning_rate": 3.6009485094850946e-05,
      "loss": 5.6273,
      "step": 206500
    },
    {
      "epoch": 280.4878048780488,
      "grad_norm": 22.931875228881836,
      "learning_rate": 3.597560975609756e-05,
      "loss": 5.6311,
      "step": 207000
    },
    {
      "epoch": 281.16531165311653,
      "grad_norm": 5.805251121520996,
      "learning_rate": 3.594173441734417e-05,
      "loss": 5.6582,
      "step": 207500
    },
    {
      "epoch": 281.8428184281843,
      "grad_norm": 7.636198997497559,
      "learning_rate": 3.590785907859079e-05,
      "loss": 5.6428,
      "step": 208000
    },
    {
      "epoch": 282.520325203252,
      "grad_norm": 7.073180675506592,
      "learning_rate": 3.58739837398374e-05,
      "loss": 5.6286,
      "step": 208500
    },
    {
      "epoch": 283.19783197831975,
      "grad_norm": 9.140508651733398,
      "learning_rate": 3.584010840108401e-05,
      "loss": 5.6443,
      "step": 209000
    },
    {
      "epoch": 283.87533875338755,
      "grad_norm": 8.637862205505371,
      "learning_rate": 3.580623306233063e-05,
      "loss": 5.6502,
      "step": 209500
    },
    {
      "epoch": 284.5528455284553,
      "grad_norm": 7.539974689483643,
      "learning_rate": 3.577235772357724e-05,
      "loss": 5.6159,
      "step": 210000
    },
    {
      "epoch": 285.23035230352303,
      "grad_norm": 8.314375877380371,
      "learning_rate": 3.5738482384823854e-05,
      "loss": 5.6294,
      "step": 210500
    },
    {
      "epoch": 285.9078590785908,
      "grad_norm": 10.356468200683594,
      "learning_rate": 3.5704607046070464e-05,
      "loss": 5.6493,
      "step": 211000
    },
    {
      "epoch": 286.5853658536585,
      "grad_norm": 7.9699578285217285,
      "learning_rate": 3.5670731707317074e-05,
      "loss": 5.6586,
      "step": 211500
    },
    {
      "epoch": 287.2628726287263,
      "grad_norm": 9.322332382202148,
      "learning_rate": 3.5636856368563684e-05,
      "loss": 5.6228,
      "step": 212000
    },
    {
      "epoch": 287.94037940379405,
      "grad_norm": 11.088629722595215,
      "learning_rate": 3.56029810298103e-05,
      "loss": 5.6556,
      "step": 212500
    },
    {
      "epoch": 288.6178861788618,
      "grad_norm": 10.046131134033203,
      "learning_rate": 3.556910569105692e-05,
      "loss": 5.6514,
      "step": 213000
    },
    {
      "epoch": 289.29539295392954,
      "grad_norm": 11.453797340393066,
      "learning_rate": 3.553523035230352e-05,
      "loss": 5.613,
      "step": 213500
    },
    {
      "epoch": 289.9728997289973,
      "grad_norm": 19.865585327148438,
      "learning_rate": 3.550135501355014e-05,
      "loss": 5.6413,
      "step": 214000
    },
    {
      "epoch": 290.650406504065,
      "grad_norm": 12.953450202941895,
      "learning_rate": 3.546747967479675e-05,
      "loss": 5.6723,
      "step": 214500
    },
    {
      "epoch": 291.3279132791328,
      "grad_norm": 9.453527450561523,
      "learning_rate": 3.5433604336043364e-05,
      "loss": 5.6318,
      "step": 215000
    },
    {
      "epoch": 292.00542005420056,
      "grad_norm": 9.448775291442871,
      "learning_rate": 3.5399728997289974e-05,
      "loss": 5.6297,
      "step": 215500
    },
    {
      "epoch": 292.6829268292683,
      "grad_norm": 8.053380012512207,
      "learning_rate": 3.5365853658536584e-05,
      "loss": 5.6641,
      "step": 216000
    },
    {
      "epoch": 293.36043360433604,
      "grad_norm": 12.836420059204102,
      "learning_rate": 3.53319783197832e-05,
      "loss": 5.5644,
      "step": 216500
    },
    {
      "epoch": 294.0379403794038,
      "grad_norm": 8.850604057312012,
      "learning_rate": 3.529810298102981e-05,
      "loss": 5.7038,
      "step": 217000
    },
    {
      "epoch": 294.7154471544715,
      "grad_norm": 8.01876163482666,
      "learning_rate": 3.526422764227643e-05,
      "loss": 5.6337,
      "step": 217500
    },
    {
      "epoch": 295.3929539295393,
      "grad_norm": 8.343851089477539,
      "learning_rate": 3.523035230352303e-05,
      "loss": 5.6241,
      "step": 218000
    },
    {
      "epoch": 296.07046070460706,
      "grad_norm": 9.858091354370117,
      "learning_rate": 3.519647696476965e-05,
      "loss": 5.6482,
      "step": 218500
    },
    {
      "epoch": 296.7479674796748,
      "grad_norm": 16.99285316467285,
      "learning_rate": 3.5162601626016265e-05,
      "loss": 5.6396,
      "step": 219000
    },
    {
      "epoch": 297.42547425474254,
      "grad_norm": 7.603085994720459,
      "learning_rate": 3.5128726287262875e-05,
      "loss": 5.636,
      "step": 219500
    },
    {
      "epoch": 298.1029810298103,
      "grad_norm": 7.64933967590332,
      "learning_rate": 3.509485094850949e-05,
      "loss": 5.6192,
      "step": 220000
    },
    {
      "epoch": 298.780487804878,
      "grad_norm": 8.70661735534668,
      "learning_rate": 3.5060975609756095e-05,
      "loss": 5.6611,
      "step": 220500
    },
    {
      "epoch": 299.4579945799458,
      "grad_norm": 8.5040922164917,
      "learning_rate": 3.502710027100271e-05,
      "loss": 5.6302,
      "step": 221000
    },
    {
      "epoch": 300.13550135501356,
      "grad_norm": 8.569077491760254,
      "learning_rate": 3.499322493224932e-05,
      "loss": 5.6538,
      "step": 221500
    },
    {
      "epoch": 300.8130081300813,
      "grad_norm": 7.392378330230713,
      "learning_rate": 3.495934959349594e-05,
      "loss": 5.6441,
      "step": 222000
    },
    {
      "epoch": 301.49051490514904,
      "grad_norm": 9.2504301071167,
      "learning_rate": 3.492547425474255e-05,
      "loss": 5.6374,
      "step": 222500
    },
    {
      "epoch": 302.1680216802168,
      "grad_norm": 8.54338264465332,
      "learning_rate": 3.489159891598916e-05,
      "loss": 5.6412,
      "step": 223000
    },
    {
      "epoch": 302.8455284552846,
      "grad_norm": 10.593342781066895,
      "learning_rate": 3.4857723577235775e-05,
      "loss": 5.6473,
      "step": 223500
    },
    {
      "epoch": 303.5230352303523,
      "grad_norm": 9.018580436706543,
      "learning_rate": 3.4823848238482385e-05,
      "loss": 5.6206,
      "step": 224000
    },
    {
      "epoch": 304.20054200542006,
      "grad_norm": 7.166301727294922,
      "learning_rate": 3.4789972899729e-05,
      "loss": 5.6429,
      "step": 224500
    },
    {
      "epoch": 304.8780487804878,
      "grad_norm": 13.861719131469727,
      "learning_rate": 3.475609756097561e-05,
      "loss": 5.655,
      "step": 225000
    },
    {
      "epoch": 305.55555555555554,
      "grad_norm": 15.176480293273926,
      "learning_rate": 3.472222222222222e-05,
      "loss": 5.6277,
      "step": 225500
    },
    {
      "epoch": 306.2330623306233,
      "grad_norm": 7.312662601470947,
      "learning_rate": 3.468834688346884e-05,
      "loss": 5.6253,
      "step": 226000
    },
    {
      "epoch": 306.9105691056911,
      "grad_norm": 9.807488441467285,
      "learning_rate": 3.465447154471545e-05,
      "loss": 5.6385,
      "step": 226500
    },
    {
      "epoch": 307.5880758807588,
      "grad_norm": 7.384364604949951,
      "learning_rate": 3.4620596205962066e-05,
      "loss": 5.6691,
      "step": 227000
    },
    {
      "epoch": 308.26558265582656,
      "grad_norm": 7.606192111968994,
      "learning_rate": 3.458672086720867e-05,
      "loss": 5.6301,
      "step": 227500
    },
    {
      "epoch": 308.9430894308943,
      "grad_norm": 7.418758392333984,
      "learning_rate": 3.4552845528455286e-05,
      "loss": 5.6331,
      "step": 228000
    },
    {
      "epoch": 309.62059620596204,
      "grad_norm": 9.298274993896484,
      "learning_rate": 3.4518970189701896e-05,
      "loss": 5.6579,
      "step": 228500
    },
    {
      "epoch": 310.2981029810298,
      "grad_norm": 5.825813293457031,
      "learning_rate": 3.448509485094851e-05,
      "loss": 5.6071,
      "step": 229000
    },
    {
      "epoch": 310.9756097560976,
      "grad_norm": 10.774868965148926,
      "learning_rate": 3.445121951219512e-05,
      "loss": 5.6654,
      "step": 229500
    },
    {
      "epoch": 311.6531165311653,
      "grad_norm": 12.535643577575684,
      "learning_rate": 3.441734417344173e-05,
      "loss": 5.6205,
      "step": 230000
    },
    {
      "epoch": 312.33062330623306,
      "grad_norm": 10.739433288574219,
      "learning_rate": 3.438346883468835e-05,
      "loss": 5.6377,
      "step": 230500
    },
    {
      "epoch": 313.0081300813008,
      "grad_norm": 7.974839210510254,
      "learning_rate": 3.434959349593496e-05,
      "loss": 5.65,
      "step": 231000
    },
    {
      "epoch": 313.68563685636855,
      "grad_norm": 22.491466522216797,
      "learning_rate": 3.4315718157181576e-05,
      "loss": 5.6226,
      "step": 231500
    },
    {
      "epoch": 314.3631436314363,
      "grad_norm": 7.048492908477783,
      "learning_rate": 3.4281842818428186e-05,
      "loss": 5.6296,
      "step": 232000
    },
    {
      "epoch": 315.0406504065041,
      "grad_norm": 8.479637145996094,
      "learning_rate": 3.4247967479674796e-05,
      "loss": 5.6526,
      "step": 232500
    },
    {
      "epoch": 315.7181571815718,
      "grad_norm": 9.428337097167969,
      "learning_rate": 3.421409214092141e-05,
      "loss": 5.6214,
      "step": 233000
    },
    {
      "epoch": 316.39566395663957,
      "grad_norm": 7.254103660583496,
      "learning_rate": 3.418021680216802e-05,
      "loss": 5.6373,
      "step": 233500
    },
    {
      "epoch": 317.0731707317073,
      "grad_norm": 10.298555374145508,
      "learning_rate": 3.414634146341464e-05,
      "loss": 5.6503,
      "step": 234000
    },
    {
      "epoch": 317.75067750677505,
      "grad_norm": 9.144974708557129,
      "learning_rate": 3.411246612466124e-05,
      "loss": 5.6273,
      "step": 234500
    },
    {
      "epoch": 318.42818428184285,
      "grad_norm": 7.277905464172363,
      "learning_rate": 3.407859078590786e-05,
      "loss": 5.6228,
      "step": 235000
    },
    {
      "epoch": 319.1056910569106,
      "grad_norm": 10.316221237182617,
      "learning_rate": 3.404471544715448e-05,
      "loss": 5.6544,
      "step": 235500
    },
    {
      "epoch": 319.7831978319783,
      "grad_norm": 9.275721549987793,
      "learning_rate": 3.401084010840109e-05,
      "loss": 5.6346,
      "step": 236000
    },
    {
      "epoch": 320.46070460704607,
      "grad_norm": 9.33399772644043,
      "learning_rate": 3.39769647696477e-05,
      "loss": 5.6361,
      "step": 236500
    },
    {
      "epoch": 321.1382113821138,
      "grad_norm": 8.025078773498535,
      "learning_rate": 3.394308943089431e-05,
      "loss": 5.6452,
      "step": 237000
    },
    {
      "epoch": 321.81571815718155,
      "grad_norm": 8.789301872253418,
      "learning_rate": 3.3909214092140924e-05,
      "loss": 5.6235,
      "step": 237500
    },
    {
      "epoch": 322.49322493224935,
      "grad_norm": 7.047425746917725,
      "learning_rate": 3.3875338753387534e-05,
      "loss": 5.6552,
      "step": 238000
    },
    {
      "epoch": 323.1707317073171,
      "grad_norm": 8.790125846862793,
      "learning_rate": 3.384146341463415e-05,
      "loss": 5.6351,
      "step": 238500
    },
    {
      "epoch": 323.84823848238483,
      "grad_norm": 5.554738521575928,
      "learning_rate": 3.380758807588076e-05,
      "loss": 5.6509,
      "step": 239000
    },
    {
      "epoch": 324.52574525745257,
      "grad_norm": 9.12978744506836,
      "learning_rate": 3.377371273712737e-05,
      "loss": 5.6153,
      "step": 239500
    },
    {
      "epoch": 325.2032520325203,
      "grad_norm": 10.973477363586426,
      "learning_rate": 3.373983739837399e-05,
      "loss": 5.6088,
      "step": 240000
    },
    {
      "epoch": 325.88075880758805,
      "grad_norm": 11.238083839416504,
      "learning_rate": 3.37059620596206e-05,
      "loss": 5.6557,
      "step": 240500
    },
    {
      "epoch": 326.55826558265585,
      "grad_norm": 6.917649745941162,
      "learning_rate": 3.3672086720867214e-05,
      "loss": 5.6573,
      "step": 241000
    },
    {
      "epoch": 327.2357723577236,
      "grad_norm": 13.260126113891602,
      "learning_rate": 3.3638211382113824e-05,
      "loss": 5.6376,
      "step": 241500
    },
    {
      "epoch": 327.91327913279133,
      "grad_norm": 14.946444511413574,
      "learning_rate": 3.3604336043360434e-05,
      "loss": 5.6318,
      "step": 242000
    },
    {
      "epoch": 328.5907859078591,
      "grad_norm": 19.169536590576172,
      "learning_rate": 3.357046070460705e-05,
      "loss": 5.6315,
      "step": 242500
    },
    {
      "epoch": 329.2682926829268,
      "grad_norm": 9.64830493927002,
      "learning_rate": 3.353658536585366e-05,
      "loss": 5.636,
      "step": 243000
    },
    {
      "epoch": 329.94579945799455,
      "grad_norm": 8.478034973144531,
      "learning_rate": 3.350271002710027e-05,
      "loss": 5.6457,
      "step": 243500
    },
    {
      "epoch": 330.62330623306235,
      "grad_norm": 9.721768379211426,
      "learning_rate": 3.346883468834688e-05,
      "loss": 5.6509,
      "step": 244000
    },
    {
      "epoch": 331.3008130081301,
      "grad_norm": 8.703048706054688,
      "learning_rate": 3.34349593495935e-05,
      "loss": 5.6084,
      "step": 244500
    },
    {
      "epoch": 331.97831978319783,
      "grad_norm": 6.074817657470703,
      "learning_rate": 3.340108401084011e-05,
      "loss": 5.6431,
      "step": 245000
    },
    {
      "epoch": 332.6558265582656,
      "grad_norm": 10.028462409973145,
      "learning_rate": 3.3367208672086725e-05,
      "loss": 5.6408,
      "step": 245500
    },
    {
      "epoch": 333.3333333333333,
      "grad_norm": 10.228201866149902,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 5.6431,
      "step": 246000
    },
    {
      "epoch": 334.0108401084011,
      "grad_norm": 6.065526485443115,
      "learning_rate": 3.3299457994579945e-05,
      "loss": 5.6362,
      "step": 246500
    },
    {
      "epoch": 334.68834688346885,
      "grad_norm": 12.137288093566895,
      "learning_rate": 3.326558265582656e-05,
      "loss": 5.6392,
      "step": 247000
    },
    {
      "epoch": 335.3658536585366,
      "grad_norm": 10.260432243347168,
      "learning_rate": 3.323170731707317e-05,
      "loss": 5.6386,
      "step": 247500
    },
    {
      "epoch": 336.04336043360433,
      "grad_norm": 11.00155258178711,
      "learning_rate": 3.319783197831978e-05,
      "loss": 5.6232,
      "step": 248000
    },
    {
      "epoch": 336.7208672086721,
      "grad_norm": 8.80339241027832,
      "learning_rate": 3.31639566395664e-05,
      "loss": 5.6662,
      "step": 248500
    },
    {
      "epoch": 337.3983739837398,
      "grad_norm": 10.507308959960938,
      "learning_rate": 3.313008130081301e-05,
      "loss": 5.5736,
      "step": 249000
    },
    {
      "epoch": 338.0758807588076,
      "grad_norm": 11.039789199829102,
      "learning_rate": 3.3096205962059625e-05,
      "loss": 5.6507,
      "step": 249500
    },
    {
      "epoch": 338.75338753387535,
      "grad_norm": 6.931565761566162,
      "learning_rate": 3.3062330623306235e-05,
      "loss": 5.6313,
      "step": 250000
    },
    {
      "epoch": 339.4308943089431,
      "grad_norm": 13.058575630187988,
      "learning_rate": 3.3028455284552845e-05,
      "loss": 5.6434,
      "step": 250500
    },
    {
      "epoch": 340.10840108401084,
      "grad_norm": 12.30347728729248,
      "learning_rate": 3.2994579945799456e-05,
      "loss": 5.6415,
      "step": 251000
    },
    {
      "epoch": 340.7859078590786,
      "grad_norm": 9.079402923583984,
      "learning_rate": 3.296070460704607e-05,
      "loss": 5.6212,
      "step": 251500
    },
    {
      "epoch": 341.4634146341463,
      "grad_norm": 17.50655746459961,
      "learning_rate": 3.292682926829269e-05,
      "loss": 5.6513,
      "step": 252000
    },
    {
      "epoch": 342.1409214092141,
      "grad_norm": 9.59919261932373,
      "learning_rate": 3.28929539295393e-05,
      "loss": 5.6362,
      "step": 252500
    },
    {
      "epoch": 342.81842818428186,
      "grad_norm": 6.403855800628662,
      "learning_rate": 3.285907859078591e-05,
      "loss": 5.6283,
      "step": 253000
    },
    {
      "epoch": 343.4959349593496,
      "grad_norm": 6.834600448608398,
      "learning_rate": 3.282520325203252e-05,
      "loss": 5.6177,
      "step": 253500
    },
    {
      "epoch": 344.17344173441734,
      "grad_norm": 9.10738468170166,
      "learning_rate": 3.2791327913279136e-05,
      "loss": 5.6338,
      "step": 254000
    },
    {
      "epoch": 344.8509485094851,
      "grad_norm": 8.326763153076172,
      "learning_rate": 3.2757452574525746e-05,
      "loss": 5.6524,
      "step": 254500
    },
    {
      "epoch": 345.5284552845528,
      "grad_norm": 8.131766319274902,
      "learning_rate": 3.2723577235772356e-05,
      "loss": 5.5908,
      "step": 255000
    },
    {
      "epoch": 346.2059620596206,
      "grad_norm": 9.731898307800293,
      "learning_rate": 3.268970189701897e-05,
      "loss": 5.6622,
      "step": 255500
    },
    {
      "epoch": 346.88346883468836,
      "grad_norm": 6.899817943572998,
      "learning_rate": 3.265582655826558e-05,
      "loss": 5.6405,
      "step": 256000
    },
    {
      "epoch": 347.5609756097561,
      "grad_norm": 6.952796936035156,
      "learning_rate": 3.26219512195122e-05,
      "loss": 5.645,
      "step": 256500
    },
    {
      "epoch": 348.23848238482384,
      "grad_norm": 7.54498815536499,
      "learning_rate": 3.258807588075881e-05,
      "loss": 5.6624,
      "step": 257000
    },
    {
      "epoch": 348.9159891598916,
      "grad_norm": 7.000537395477295,
      "learning_rate": 3.255420054200542e-05,
      "loss": 5.607,
      "step": 257500
    },
    {
      "epoch": 349.5934959349593,
      "grad_norm": 9.078072547912598,
      "learning_rate": 3.2520325203252037e-05,
      "loss": 5.6425,
      "step": 258000
    },
    {
      "epoch": 350.2710027100271,
      "grad_norm": 10.158866882324219,
      "learning_rate": 3.2486449864498647e-05,
      "loss": 5.6362,
      "step": 258500
    },
    {
      "epoch": 350.94850948509486,
      "grad_norm": 7.343793869018555,
      "learning_rate": 3.245257452574526e-05,
      "loss": 5.62,
      "step": 259000
    },
    {
      "epoch": 351.6260162601626,
      "grad_norm": 10.157678604125977,
      "learning_rate": 3.241869918699187e-05,
      "loss": 5.6448,
      "step": 259500
    },
    {
      "epoch": 352.30352303523034,
      "grad_norm": 21.466419219970703,
      "learning_rate": 3.2384823848238483e-05,
      "loss": 5.6176,
      "step": 260000
    },
    {
      "epoch": 352.9810298102981,
      "grad_norm": 7.733177661895752,
      "learning_rate": 3.2350948509485093e-05,
      "loss": 5.6507,
      "step": 260500
    },
    {
      "epoch": 353.6585365853659,
      "grad_norm": 12.053696632385254,
      "learning_rate": 3.231707317073171e-05,
      "loss": 5.6248,
      "step": 261000
    },
    {
      "epoch": 354.3360433604336,
      "grad_norm": 7.20143461227417,
      "learning_rate": 3.228319783197832e-05,
      "loss": 5.6388,
      "step": 261500
    },
    {
      "epoch": 355.01355013550136,
      "grad_norm": 8.536059379577637,
      "learning_rate": 3.224932249322493e-05,
      "loss": 5.6295,
      "step": 262000
    },
    {
      "epoch": 355.6910569105691,
      "grad_norm": 10.032732963562012,
      "learning_rate": 3.221544715447155e-05,
      "loss": 5.6423,
      "step": 262500
    },
    {
      "epoch": 356.36856368563684,
      "grad_norm": 17.669389724731445,
      "learning_rate": 3.218157181571816e-05,
      "loss": 5.6073,
      "step": 263000
    },
    {
      "epoch": 357.0460704607046,
      "grad_norm": 10.581758499145508,
      "learning_rate": 3.2147696476964774e-05,
      "loss": 5.6459,
      "step": 263500
    },
    {
      "epoch": 357.7235772357724,
      "grad_norm": 8.224884986877441,
      "learning_rate": 3.2113821138211384e-05,
      "loss": 5.6236,
      "step": 264000
    },
    {
      "epoch": 358.4010840108401,
      "grad_norm": 8.530622482299805,
      "learning_rate": 3.2079945799457994e-05,
      "loss": 5.6237,
      "step": 264500
    },
    {
      "epoch": 359.07859078590786,
      "grad_norm": 8.433812141418457,
      "learning_rate": 3.204607046070461e-05,
      "loss": 5.6516,
      "step": 265000
    },
    {
      "epoch": 359.7560975609756,
      "grad_norm": 10.720961570739746,
      "learning_rate": 3.201219512195122e-05,
      "loss": 5.6479,
      "step": 265500
    },
    {
      "epoch": 360.43360433604335,
      "grad_norm": 8.914022445678711,
      "learning_rate": 3.197831978319784e-05,
      "loss": 5.6036,
      "step": 266000
    },
    {
      "epoch": 361.1111111111111,
      "grad_norm": 9.036809921264648,
      "learning_rate": 3.194444444444444e-05,
      "loss": 5.659,
      "step": 266500
    },
    {
      "epoch": 361.7886178861789,
      "grad_norm": 11.355944633483887,
      "learning_rate": 3.191056910569106e-05,
      "loss": 5.632,
      "step": 267000
    },
    {
      "epoch": 362.4661246612466,
      "grad_norm": 8.82209300994873,
      "learning_rate": 3.187669376693767e-05,
      "loss": 5.6325,
      "step": 267500
    },
    {
      "epoch": 363.14363143631437,
      "grad_norm": 9.35318660736084,
      "learning_rate": 3.1842818428184285e-05,
      "loss": 5.6377,
      "step": 268000
    },
    {
      "epoch": 363.8211382113821,
      "grad_norm": 8.12077808380127,
      "learning_rate": 3.18089430894309e-05,
      "loss": 5.6318,
      "step": 268500
    },
    {
      "epoch": 364.49864498644985,
      "grad_norm": 8.098655700683594,
      "learning_rate": 3.1775067750677505e-05,
      "loss": 5.6309,
      "step": 269000
    },
    {
      "epoch": 365.1761517615176,
      "grad_norm": 5.7750325202941895,
      "learning_rate": 3.174119241192412e-05,
      "loss": 5.6317,
      "step": 269500
    },
    {
      "epoch": 365.8536585365854,
      "grad_norm": 9.858672142028809,
      "learning_rate": 3.170731707317073e-05,
      "loss": 5.6392,
      "step": 270000
    },
    {
      "epoch": 366.5311653116531,
      "grad_norm": 9.507521629333496,
      "learning_rate": 3.167344173441735e-05,
      "loss": 5.6344,
      "step": 270500
    },
    {
      "epoch": 367.20867208672087,
      "grad_norm": 15.29931926727295,
      "learning_rate": 3.163956639566396e-05,
      "loss": 5.6182,
      "step": 271000
    },
    {
      "epoch": 367.8861788617886,
      "grad_norm": 10.747172355651855,
      "learning_rate": 3.160569105691057e-05,
      "loss": 5.6513,
      "step": 271500
    },
    {
      "epoch": 368.56368563685635,
      "grad_norm": 13.830399513244629,
      "learning_rate": 3.1571815718157185e-05,
      "loss": 5.6422,
      "step": 272000
    },
    {
      "epoch": 369.24119241192415,
      "grad_norm": 8.30903434753418,
      "learning_rate": 3.1537940379403795e-05,
      "loss": 5.6163,
      "step": 272500
    },
    {
      "epoch": 369.9186991869919,
      "grad_norm": 7.160650730133057,
      "learning_rate": 3.150406504065041e-05,
      "loss": 5.6479,
      "step": 273000
    },
    {
      "epoch": 370.5962059620596,
      "grad_norm": 8.375289916992188,
      "learning_rate": 3.1470189701897015e-05,
      "loss": 5.6606,
      "step": 273500
    },
    {
      "epoch": 371.27371273712737,
      "grad_norm": 10.299137115478516,
      "learning_rate": 3.143631436314363e-05,
      "loss": 5.599,
      "step": 274000
    },
    {
      "epoch": 371.9512195121951,
      "grad_norm": 10.528438568115234,
      "learning_rate": 3.140243902439025e-05,
      "loss": 5.6342,
      "step": 274500
    },
    {
      "epoch": 372.62872628726285,
      "grad_norm": 7.8246917724609375,
      "learning_rate": 3.136856368563686e-05,
      "loss": 5.6059,
      "step": 275000
    },
    {
      "epoch": 373.30623306233065,
      "grad_norm": 11.204476356506348,
      "learning_rate": 3.1334688346883476e-05,
      "loss": 5.6472,
      "step": 275500
    },
    {
      "epoch": 373.9837398373984,
      "grad_norm": 11.783681869506836,
      "learning_rate": 3.130081300813008e-05,
      "loss": 5.6188,
      "step": 276000
    },
    {
      "epoch": 374.66124661246613,
      "grad_norm": 7.432352542877197,
      "learning_rate": 3.1266937669376696e-05,
      "loss": 5.6603,
      "step": 276500
    },
    {
      "epoch": 375.33875338753387,
      "grad_norm": 7.837210178375244,
      "learning_rate": 3.1233062330623306e-05,
      "loss": 5.5971,
      "step": 277000
    },
    {
      "epoch": 376.0162601626016,
      "grad_norm": 8.424173355102539,
      "learning_rate": 3.119918699186992e-05,
      "loss": 5.6416,
      "step": 277500
    }
  ],
  "logging_steps": 500,
  "max_steps": 738000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 9250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.561073569792e+16,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
