{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 37.60162601626016,
  "eval_steps": 500,
  "global_step": 27750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 5.091018199920654,
      "learning_rate": 4.996612466124662e-05,
      "loss": 6.803,
      "step": 500
    },
    {
      "epoch": 1.3550135501355014,
      "grad_norm": 3.3020758628845215,
      "learning_rate": 4.993224932249323e-05,
      "loss": 6.1589,
      "step": 1000
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 8.262969017028809,
      "learning_rate": 4.989837398373984e-05,
      "loss": 6.1256,
      "step": 1500
    },
    {
      "epoch": 2.710027100271003,
      "grad_norm": 7.3213653564453125,
      "learning_rate": 4.986449864498645e-05,
      "loss": 6.0501,
      "step": 2000
    },
    {
      "epoch": 3.3875338753387534,
      "grad_norm": 15.888947486877441,
      "learning_rate": 4.9830623306233066e-05,
      "loss": 5.9678,
      "step": 2500
    },
    {
      "epoch": 4.065040650406504,
      "grad_norm": 8.027318954467773,
      "learning_rate": 4.9796747967479676e-05,
      "loss": 5.9816,
      "step": 3000
    },
    {
      "epoch": 4.742547425474255,
      "grad_norm": 9.33156681060791,
      "learning_rate": 4.9762872628726286e-05,
      "loss": 5.9411,
      "step": 3500
    },
    {
      "epoch": 5.420054200542006,
      "grad_norm": 10.087059020996094,
      "learning_rate": 4.97289972899729e-05,
      "loss": 5.8978,
      "step": 4000
    },
    {
      "epoch": 6.097560975609756,
      "grad_norm": 9.301751136779785,
      "learning_rate": 4.969512195121951e-05,
      "loss": 5.9311,
      "step": 4500
    },
    {
      "epoch": 6.775067750677507,
      "grad_norm": 11.422779083251953,
      "learning_rate": 4.966124661246613e-05,
      "loss": 5.8857,
      "step": 5000
    },
    {
      "epoch": 7.452574525745257,
      "grad_norm": 6.880033493041992,
      "learning_rate": 4.962737127371274e-05,
      "loss": 5.8476,
      "step": 5500
    },
    {
      "epoch": 8.130081300813009,
      "grad_norm": 7.027878284454346,
      "learning_rate": 4.959349593495935e-05,
      "loss": 5.8935,
      "step": 6000
    },
    {
      "epoch": 8.807588075880759,
      "grad_norm": 7.722722053527832,
      "learning_rate": 4.955962059620597e-05,
      "loss": 5.8373,
      "step": 6500
    },
    {
      "epoch": 9.48509485094851,
      "grad_norm": 9.376630783081055,
      "learning_rate": 4.952574525745258e-05,
      "loss": 5.8788,
      "step": 7000
    },
    {
      "epoch": 10.16260162601626,
      "grad_norm": 11.910229682922363,
      "learning_rate": 4.9491869918699193e-05,
      "loss": 5.8272,
      "step": 7500
    },
    {
      "epoch": 10.840108401084011,
      "grad_norm": 10.906257629394531,
      "learning_rate": 4.9457994579945803e-05,
      "loss": 5.8068,
      "step": 8000
    },
    {
      "epoch": 11.517615176151761,
      "grad_norm": 14.655856132507324,
      "learning_rate": 4.9424119241192414e-05,
      "loss": 5.8539,
      "step": 8500
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 7.715482234954834,
      "learning_rate": 4.9390243902439024e-05,
      "loss": 5.7983,
      "step": 9000
    },
    {
      "epoch": 12.872628726287262,
      "grad_norm": 11.57014274597168,
      "learning_rate": 4.935636856368564e-05,
      "loss": 5.8107,
      "step": 9500
    },
    {
      "epoch": 13.550135501355014,
      "grad_norm": 8.040351867675781,
      "learning_rate": 4.932249322493225e-05,
      "loss": 5.8246,
      "step": 10000
    },
    {
      "epoch": 14.227642276422765,
      "grad_norm": 13.104251861572266,
      "learning_rate": 4.928861788617886e-05,
      "loss": 5.8106,
      "step": 10500
    },
    {
      "epoch": 14.905149051490515,
      "grad_norm": 9.188155174255371,
      "learning_rate": 4.925474254742548e-05,
      "loss": 5.8132,
      "step": 11000
    },
    {
      "epoch": 15.582655826558266,
      "grad_norm": 11.850305557250977,
      "learning_rate": 4.922086720867209e-05,
      "loss": 5.8105,
      "step": 11500
    },
    {
      "epoch": 16.260162601626018,
      "grad_norm": 11.092033386230469,
      "learning_rate": 4.9186991869918704e-05,
      "loss": 5.784,
      "step": 12000
    },
    {
      "epoch": 16.937669376693766,
      "grad_norm": 8.578161239624023,
      "learning_rate": 4.9153116531165314e-05,
      "loss": 5.7848,
      "step": 12500
    },
    {
      "epoch": 17.615176151761517,
      "grad_norm": 10.56583309173584,
      "learning_rate": 4.9119241192411924e-05,
      "loss": 5.8027,
      "step": 13000
    },
    {
      "epoch": 18.29268292682927,
      "grad_norm": 7.775592803955078,
      "learning_rate": 4.908536585365854e-05,
      "loss": 5.7688,
      "step": 13500
    },
    {
      "epoch": 18.97018970189702,
      "grad_norm": 12.018457412719727,
      "learning_rate": 4.905149051490515e-05,
      "loss": 5.7836,
      "step": 14000
    },
    {
      "epoch": 19.647696476964768,
      "grad_norm": 8.249943733215332,
      "learning_rate": 4.901761517615177e-05,
      "loss": 5.7988,
      "step": 14500
    },
    {
      "epoch": 20.32520325203252,
      "grad_norm": 7.565089702606201,
      "learning_rate": 4.898373983739837e-05,
      "loss": 5.7566,
      "step": 15000
    },
    {
      "epoch": 21.00271002710027,
      "grad_norm": 9.527578353881836,
      "learning_rate": 4.894986449864499e-05,
      "loss": 5.7912,
      "step": 15500
    },
    {
      "epoch": 21.680216802168022,
      "grad_norm": 6.278652667999268,
      "learning_rate": 4.89159891598916e-05,
      "loss": 5.7818,
      "step": 16000
    },
    {
      "epoch": 22.357723577235774,
      "grad_norm": 9.314443588256836,
      "learning_rate": 4.8882113821138215e-05,
      "loss": 5.7763,
      "step": 16500
    },
    {
      "epoch": 23.035230352303522,
      "grad_norm": 14.77385425567627,
      "learning_rate": 4.884823848238483e-05,
      "loss": 5.7497,
      "step": 17000
    },
    {
      "epoch": 23.712737127371273,
      "grad_norm": 9.693492889404297,
      "learning_rate": 4.8814363143631435e-05,
      "loss": 5.775,
      "step": 17500
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 15.656332969665527,
      "learning_rate": 4.878048780487805e-05,
      "loss": 5.7446,
      "step": 18000
    },
    {
      "epoch": 25.067750677506776,
      "grad_norm": 9.059091567993164,
      "learning_rate": 4.874661246612466e-05,
      "loss": 5.797,
      "step": 18500
    },
    {
      "epoch": 25.745257452574524,
      "grad_norm": 9.663070678710938,
      "learning_rate": 4.871273712737128e-05,
      "loss": 5.7604,
      "step": 19000
    },
    {
      "epoch": 26.422764227642276,
      "grad_norm": 14.631203651428223,
      "learning_rate": 4.867886178861789e-05,
      "loss": 5.7375,
      "step": 19500
    },
    {
      "epoch": 27.100271002710027,
      "grad_norm": 7.689953804016113,
      "learning_rate": 4.86449864498645e-05,
      "loss": 5.7543,
      "step": 20000
    },
    {
      "epoch": 27.77777777777778,
      "grad_norm": 12.156087875366211,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 5.7679,
      "step": 20500
    },
    {
      "epoch": 28.45528455284553,
      "grad_norm": 8.73508071899414,
      "learning_rate": 4.8577235772357725e-05,
      "loss": 5.754,
      "step": 21000
    },
    {
      "epoch": 29.132791327913278,
      "grad_norm": 6.654566287994385,
      "learning_rate": 4.854336043360434e-05,
      "loss": 5.7533,
      "step": 21500
    },
    {
      "epoch": 29.81029810298103,
      "grad_norm": 13.123746871948242,
      "learning_rate": 4.8509485094850945e-05,
      "loss": 5.7613,
      "step": 22000
    },
    {
      "epoch": 30.48780487804878,
      "grad_norm": 10.495832443237305,
      "learning_rate": 4.847560975609756e-05,
      "loss": 5.7233,
      "step": 22500
    },
    {
      "epoch": 31.165311653116532,
      "grad_norm": 7.0927605628967285,
      "learning_rate": 4.844173441734418e-05,
      "loss": 5.7641,
      "step": 23000
    },
    {
      "epoch": 31.84281842818428,
      "grad_norm": 8.636788368225098,
      "learning_rate": 4.840785907859079e-05,
      "loss": 5.7313,
      "step": 23500
    },
    {
      "epoch": 32.520325203252035,
      "grad_norm": 9.791983604431152,
      "learning_rate": 4.8373983739837406e-05,
      "loss": 5.7351,
      "step": 24000
    },
    {
      "epoch": 33.19783197831978,
      "grad_norm": 24.067975997924805,
      "learning_rate": 4.834010840108401e-05,
      "loss": 5.7762,
      "step": 24500
    },
    {
      "epoch": 33.87533875338753,
      "grad_norm": 8.568990707397461,
      "learning_rate": 4.8306233062330626e-05,
      "loss": 5.7436,
      "step": 25000
    },
    {
      "epoch": 34.552845528455286,
      "grad_norm": 9.561119079589844,
      "learning_rate": 4.8272357723577236e-05,
      "loss": 5.734,
      "step": 25500
    },
    {
      "epoch": 35.230352303523034,
      "grad_norm": 9.91040325164795,
      "learning_rate": 4.823848238482385e-05,
      "loss": 5.742,
      "step": 26000
    },
    {
      "epoch": 35.90785907859079,
      "grad_norm": 11.39449405670166,
      "learning_rate": 4.820460704607046e-05,
      "loss": 5.7499,
      "step": 26500
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 9.793062210083008,
      "learning_rate": 4.817073170731707e-05,
      "loss": 5.7413,
      "step": 27000
    },
    {
      "epoch": 37.262872628726285,
      "grad_norm": 7.896596431732178,
      "learning_rate": 4.813685636856369e-05,
      "loss": 5.7286,
      "step": 27500
    }
  ],
  "logging_steps": 500,
  "max_steps": 738000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 9250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3561073569792000.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
