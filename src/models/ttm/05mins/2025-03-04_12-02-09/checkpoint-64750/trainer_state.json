{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 87.73712737127371,
  "eval_steps": 500,
  "global_step": 64750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 5.091018199920654,
      "learning_rate": 4.996612466124662e-05,
      "loss": 6.803,
      "step": 500
    },
    {
      "epoch": 1.3550135501355014,
      "grad_norm": 3.3020758628845215,
      "learning_rate": 4.993224932249323e-05,
      "loss": 6.1589,
      "step": 1000
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 8.262969017028809,
      "learning_rate": 4.989837398373984e-05,
      "loss": 6.1256,
      "step": 1500
    },
    {
      "epoch": 2.710027100271003,
      "grad_norm": 7.3213653564453125,
      "learning_rate": 4.986449864498645e-05,
      "loss": 6.0501,
      "step": 2000
    },
    {
      "epoch": 3.3875338753387534,
      "grad_norm": 15.888947486877441,
      "learning_rate": 4.9830623306233066e-05,
      "loss": 5.9678,
      "step": 2500
    },
    {
      "epoch": 4.065040650406504,
      "grad_norm": 8.027318954467773,
      "learning_rate": 4.9796747967479676e-05,
      "loss": 5.9816,
      "step": 3000
    },
    {
      "epoch": 4.742547425474255,
      "grad_norm": 9.33156681060791,
      "learning_rate": 4.9762872628726286e-05,
      "loss": 5.9411,
      "step": 3500
    },
    {
      "epoch": 5.420054200542006,
      "grad_norm": 10.087059020996094,
      "learning_rate": 4.97289972899729e-05,
      "loss": 5.8978,
      "step": 4000
    },
    {
      "epoch": 6.097560975609756,
      "grad_norm": 9.301751136779785,
      "learning_rate": 4.969512195121951e-05,
      "loss": 5.9311,
      "step": 4500
    },
    {
      "epoch": 6.775067750677507,
      "grad_norm": 11.422779083251953,
      "learning_rate": 4.966124661246613e-05,
      "loss": 5.8857,
      "step": 5000
    },
    {
      "epoch": 7.452574525745257,
      "grad_norm": 6.880033493041992,
      "learning_rate": 4.962737127371274e-05,
      "loss": 5.8476,
      "step": 5500
    },
    {
      "epoch": 8.130081300813009,
      "grad_norm": 7.027878284454346,
      "learning_rate": 4.959349593495935e-05,
      "loss": 5.8935,
      "step": 6000
    },
    {
      "epoch": 8.807588075880759,
      "grad_norm": 7.722722053527832,
      "learning_rate": 4.955962059620597e-05,
      "loss": 5.8373,
      "step": 6500
    },
    {
      "epoch": 9.48509485094851,
      "grad_norm": 9.376630783081055,
      "learning_rate": 4.952574525745258e-05,
      "loss": 5.8788,
      "step": 7000
    },
    {
      "epoch": 10.16260162601626,
      "grad_norm": 11.910229682922363,
      "learning_rate": 4.9491869918699193e-05,
      "loss": 5.8272,
      "step": 7500
    },
    {
      "epoch": 10.840108401084011,
      "grad_norm": 10.906257629394531,
      "learning_rate": 4.9457994579945803e-05,
      "loss": 5.8068,
      "step": 8000
    },
    {
      "epoch": 11.517615176151761,
      "grad_norm": 14.655856132507324,
      "learning_rate": 4.9424119241192414e-05,
      "loss": 5.8539,
      "step": 8500
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 7.715482234954834,
      "learning_rate": 4.9390243902439024e-05,
      "loss": 5.7983,
      "step": 9000
    },
    {
      "epoch": 12.872628726287262,
      "grad_norm": 11.57014274597168,
      "learning_rate": 4.935636856368564e-05,
      "loss": 5.8107,
      "step": 9500
    },
    {
      "epoch": 13.550135501355014,
      "grad_norm": 8.040351867675781,
      "learning_rate": 4.932249322493225e-05,
      "loss": 5.8246,
      "step": 10000
    },
    {
      "epoch": 14.227642276422765,
      "grad_norm": 13.104251861572266,
      "learning_rate": 4.928861788617886e-05,
      "loss": 5.8106,
      "step": 10500
    },
    {
      "epoch": 14.905149051490515,
      "grad_norm": 9.188155174255371,
      "learning_rate": 4.925474254742548e-05,
      "loss": 5.8132,
      "step": 11000
    },
    {
      "epoch": 15.582655826558266,
      "grad_norm": 11.850305557250977,
      "learning_rate": 4.922086720867209e-05,
      "loss": 5.8105,
      "step": 11500
    },
    {
      "epoch": 16.260162601626018,
      "grad_norm": 11.092033386230469,
      "learning_rate": 4.9186991869918704e-05,
      "loss": 5.784,
      "step": 12000
    },
    {
      "epoch": 16.937669376693766,
      "grad_norm": 8.578161239624023,
      "learning_rate": 4.9153116531165314e-05,
      "loss": 5.7848,
      "step": 12500
    },
    {
      "epoch": 17.615176151761517,
      "grad_norm": 10.56583309173584,
      "learning_rate": 4.9119241192411924e-05,
      "loss": 5.8027,
      "step": 13000
    },
    {
      "epoch": 18.29268292682927,
      "grad_norm": 7.775592803955078,
      "learning_rate": 4.908536585365854e-05,
      "loss": 5.7688,
      "step": 13500
    },
    {
      "epoch": 18.97018970189702,
      "grad_norm": 12.018457412719727,
      "learning_rate": 4.905149051490515e-05,
      "loss": 5.7836,
      "step": 14000
    },
    {
      "epoch": 19.647696476964768,
      "grad_norm": 8.249943733215332,
      "learning_rate": 4.901761517615177e-05,
      "loss": 5.7988,
      "step": 14500
    },
    {
      "epoch": 20.32520325203252,
      "grad_norm": 7.565089702606201,
      "learning_rate": 4.898373983739837e-05,
      "loss": 5.7566,
      "step": 15000
    },
    {
      "epoch": 21.00271002710027,
      "grad_norm": 9.527578353881836,
      "learning_rate": 4.894986449864499e-05,
      "loss": 5.7912,
      "step": 15500
    },
    {
      "epoch": 21.680216802168022,
      "grad_norm": 6.278652667999268,
      "learning_rate": 4.89159891598916e-05,
      "loss": 5.7818,
      "step": 16000
    },
    {
      "epoch": 22.357723577235774,
      "grad_norm": 9.314443588256836,
      "learning_rate": 4.8882113821138215e-05,
      "loss": 5.7763,
      "step": 16500
    },
    {
      "epoch": 23.035230352303522,
      "grad_norm": 14.77385425567627,
      "learning_rate": 4.884823848238483e-05,
      "loss": 5.7497,
      "step": 17000
    },
    {
      "epoch": 23.712737127371273,
      "grad_norm": 9.693492889404297,
      "learning_rate": 4.8814363143631435e-05,
      "loss": 5.775,
      "step": 17500
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 15.656332969665527,
      "learning_rate": 4.878048780487805e-05,
      "loss": 5.7446,
      "step": 18000
    },
    {
      "epoch": 25.067750677506776,
      "grad_norm": 9.059091567993164,
      "learning_rate": 4.874661246612466e-05,
      "loss": 5.797,
      "step": 18500
    },
    {
      "epoch": 25.745257452574524,
      "grad_norm": 9.663070678710938,
      "learning_rate": 4.871273712737128e-05,
      "loss": 5.7604,
      "step": 19000
    },
    {
      "epoch": 26.422764227642276,
      "grad_norm": 14.631203651428223,
      "learning_rate": 4.867886178861789e-05,
      "loss": 5.7375,
      "step": 19500
    },
    {
      "epoch": 27.100271002710027,
      "grad_norm": 7.689953804016113,
      "learning_rate": 4.86449864498645e-05,
      "loss": 5.7543,
      "step": 20000
    },
    {
      "epoch": 27.77777777777778,
      "grad_norm": 12.156087875366211,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 5.7679,
      "step": 20500
    },
    {
      "epoch": 28.45528455284553,
      "grad_norm": 8.73508071899414,
      "learning_rate": 4.8577235772357725e-05,
      "loss": 5.754,
      "step": 21000
    },
    {
      "epoch": 29.132791327913278,
      "grad_norm": 6.654566287994385,
      "learning_rate": 4.854336043360434e-05,
      "loss": 5.7533,
      "step": 21500
    },
    {
      "epoch": 29.81029810298103,
      "grad_norm": 13.123746871948242,
      "learning_rate": 4.8509485094850945e-05,
      "loss": 5.7613,
      "step": 22000
    },
    {
      "epoch": 30.48780487804878,
      "grad_norm": 10.495832443237305,
      "learning_rate": 4.847560975609756e-05,
      "loss": 5.7233,
      "step": 22500
    },
    {
      "epoch": 31.165311653116532,
      "grad_norm": 7.0927605628967285,
      "learning_rate": 4.844173441734418e-05,
      "loss": 5.7641,
      "step": 23000
    },
    {
      "epoch": 31.84281842818428,
      "grad_norm": 8.636788368225098,
      "learning_rate": 4.840785907859079e-05,
      "loss": 5.7313,
      "step": 23500
    },
    {
      "epoch": 32.520325203252035,
      "grad_norm": 9.791983604431152,
      "learning_rate": 4.8373983739837406e-05,
      "loss": 5.7351,
      "step": 24000
    },
    {
      "epoch": 33.19783197831978,
      "grad_norm": 24.067975997924805,
      "learning_rate": 4.834010840108401e-05,
      "loss": 5.7762,
      "step": 24500
    },
    {
      "epoch": 33.87533875338753,
      "grad_norm": 8.568990707397461,
      "learning_rate": 4.8306233062330626e-05,
      "loss": 5.7436,
      "step": 25000
    },
    {
      "epoch": 34.552845528455286,
      "grad_norm": 9.561119079589844,
      "learning_rate": 4.8272357723577236e-05,
      "loss": 5.734,
      "step": 25500
    },
    {
      "epoch": 35.230352303523034,
      "grad_norm": 9.91040325164795,
      "learning_rate": 4.823848238482385e-05,
      "loss": 5.742,
      "step": 26000
    },
    {
      "epoch": 35.90785907859079,
      "grad_norm": 11.39449405670166,
      "learning_rate": 4.820460704607046e-05,
      "loss": 5.7499,
      "step": 26500
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 9.793062210083008,
      "learning_rate": 4.817073170731707e-05,
      "loss": 5.7413,
      "step": 27000
    },
    {
      "epoch": 37.262872628726285,
      "grad_norm": 7.896596431732178,
      "learning_rate": 4.813685636856369e-05,
      "loss": 5.7286,
      "step": 27500
    },
    {
      "epoch": 37.94037940379404,
      "grad_norm": 6.42507791519165,
      "learning_rate": 4.81029810298103e-05,
      "loss": 5.7345,
      "step": 28000
    },
    {
      "epoch": 38.61788617886179,
      "grad_norm": 11.094392776489258,
      "learning_rate": 4.8069105691056916e-05,
      "loss": 5.7163,
      "step": 28500
    },
    {
      "epoch": 39.295392953929536,
      "grad_norm": 9.148066520690918,
      "learning_rate": 4.8035230352303526e-05,
      "loss": 5.7792,
      "step": 29000
    },
    {
      "epoch": 39.97289972899729,
      "grad_norm": 10.703146934509277,
      "learning_rate": 4.8001355013550136e-05,
      "loss": 5.7311,
      "step": 29500
    },
    {
      "epoch": 40.65040650406504,
      "grad_norm": 11.24628734588623,
      "learning_rate": 4.796747967479675e-05,
      "loss": 5.7382,
      "step": 30000
    },
    {
      "epoch": 41.327913279132794,
      "grad_norm": 12.71223258972168,
      "learning_rate": 4.793360433604336e-05,
      "loss": 5.713,
      "step": 30500
    },
    {
      "epoch": 42.00542005420054,
      "grad_norm": 7.349093914031982,
      "learning_rate": 4.789972899728998e-05,
      "loss": 5.7301,
      "step": 31000
    },
    {
      "epoch": 42.68292682926829,
      "grad_norm": 8.91130256652832,
      "learning_rate": 4.786585365853658e-05,
      "loss": 5.7398,
      "step": 31500
    },
    {
      "epoch": 43.360433604336045,
      "grad_norm": 5.983088493347168,
      "learning_rate": 4.78319783197832e-05,
      "loss": 5.7064,
      "step": 32000
    },
    {
      "epoch": 44.03794037940379,
      "grad_norm": 9.94255256652832,
      "learning_rate": 4.779810298102981e-05,
      "loss": 5.7605,
      "step": 32500
    },
    {
      "epoch": 44.71544715447155,
      "grad_norm": 11.870882987976074,
      "learning_rate": 4.776422764227643e-05,
      "loss": 5.7491,
      "step": 33000
    },
    {
      "epoch": 45.392953929539296,
      "grad_norm": 11.326759338378906,
      "learning_rate": 4.773035230352304e-05,
      "loss": 5.696,
      "step": 33500
    },
    {
      "epoch": 46.070460704607044,
      "grad_norm": 11.065422058105469,
      "learning_rate": 4.769647696476965e-05,
      "loss": 5.7135,
      "step": 34000
    },
    {
      "epoch": 46.7479674796748,
      "grad_norm": 11.42674446105957,
      "learning_rate": 4.7662601626016264e-05,
      "loss": 5.7237,
      "step": 34500
    },
    {
      "epoch": 47.42547425474255,
      "grad_norm": 10.969059944152832,
      "learning_rate": 4.7628726287262874e-05,
      "loss": 5.7174,
      "step": 35000
    },
    {
      "epoch": 48.1029810298103,
      "grad_norm": 11.1267671585083,
      "learning_rate": 4.759485094850949e-05,
      "loss": 5.7379,
      "step": 35500
    },
    {
      "epoch": 48.78048780487805,
      "grad_norm": 7.527477741241455,
      "learning_rate": 4.75609756097561e-05,
      "loss": 5.7232,
      "step": 36000
    },
    {
      "epoch": 49.4579945799458,
      "grad_norm": 11.792120933532715,
      "learning_rate": 4.752710027100271e-05,
      "loss": 5.7007,
      "step": 36500
    },
    {
      "epoch": 50.13550135501355,
      "grad_norm": 12.336548805236816,
      "learning_rate": 4.749322493224933e-05,
      "loss": 5.7497,
      "step": 37000
    },
    {
      "epoch": 50.8130081300813,
      "grad_norm": 8.872333526611328,
      "learning_rate": 4.745934959349594e-05,
      "loss": 5.7216,
      "step": 37500
    },
    {
      "epoch": 51.49051490514905,
      "grad_norm": 14.99206256866455,
      "learning_rate": 4.7425474254742554e-05,
      "loss": 5.7078,
      "step": 38000
    },
    {
      "epoch": 52.1680216802168,
      "grad_norm": 14.154269218444824,
      "learning_rate": 4.739159891598916e-05,
      "loss": 5.7151,
      "step": 38500
    },
    {
      "epoch": 52.84552845528455,
      "grad_norm": 16.802804946899414,
      "learning_rate": 4.7357723577235774e-05,
      "loss": 5.7296,
      "step": 39000
    },
    {
      "epoch": 53.523035230352306,
      "grad_norm": 8.198492050170898,
      "learning_rate": 4.732384823848239e-05,
      "loss": 5.6997,
      "step": 39500
    },
    {
      "epoch": 54.200542005420054,
      "grad_norm": 8.03692626953125,
      "learning_rate": 4.7289972899729e-05,
      "loss": 5.7388,
      "step": 40000
    },
    {
      "epoch": 54.8780487804878,
      "grad_norm": 9.592068672180176,
      "learning_rate": 4.725609756097561e-05,
      "loss": 5.703,
      "step": 40500
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 8.117247581481934,
      "learning_rate": 4.722222222222222e-05,
      "loss": 5.7057,
      "step": 41000
    },
    {
      "epoch": 56.233062330623305,
      "grad_norm": 9.578608512878418,
      "learning_rate": 4.718834688346884e-05,
      "loss": 5.7093,
      "step": 41500
    },
    {
      "epoch": 56.91056910569106,
      "grad_norm": 20.542922973632812,
      "learning_rate": 4.715447154471545e-05,
      "loss": 5.7104,
      "step": 42000
    },
    {
      "epoch": 57.58807588075881,
      "grad_norm": 10.634708404541016,
      "learning_rate": 4.7120596205962065e-05,
      "loss": 5.6843,
      "step": 42500
    },
    {
      "epoch": 58.265582655826556,
      "grad_norm": 13.108489036560059,
      "learning_rate": 4.7086720867208675e-05,
      "loss": 5.7408,
      "step": 43000
    },
    {
      "epoch": 58.94308943089431,
      "grad_norm": 7.956690311431885,
      "learning_rate": 4.7052845528455285e-05,
      "loss": 5.7162,
      "step": 43500
    },
    {
      "epoch": 59.62059620596206,
      "grad_norm": 10.43460750579834,
      "learning_rate": 4.70189701897019e-05,
      "loss": 5.7095,
      "step": 44000
    },
    {
      "epoch": 60.29810298102981,
      "grad_norm": 8.457387924194336,
      "learning_rate": 4.698509485094851e-05,
      "loss": 5.7415,
      "step": 44500
    },
    {
      "epoch": 60.97560975609756,
      "grad_norm": 8.040995597839355,
      "learning_rate": 4.695121951219512e-05,
      "loss": 5.6758,
      "step": 45000
    },
    {
      "epoch": 61.65311653116531,
      "grad_norm": 8.13553524017334,
      "learning_rate": 4.691734417344174e-05,
      "loss": 5.6884,
      "step": 45500
    },
    {
      "epoch": 62.330623306233065,
      "grad_norm": 10.497289657592773,
      "learning_rate": 4.688346883468835e-05,
      "loss": 5.7133,
      "step": 46000
    },
    {
      "epoch": 63.00813008130081,
      "grad_norm": 15.844855308532715,
      "learning_rate": 4.6849593495934965e-05,
      "loss": 5.7167,
      "step": 46500
    },
    {
      "epoch": 63.68563685636856,
      "grad_norm": 9.074761390686035,
      "learning_rate": 4.6815718157181575e-05,
      "loss": 5.69,
      "step": 47000
    },
    {
      "epoch": 64.36314363143632,
      "grad_norm": 9.115644454956055,
      "learning_rate": 4.6781842818428185e-05,
      "loss": 5.7144,
      "step": 47500
    },
    {
      "epoch": 65.04065040650407,
      "grad_norm": 7.438839912414551,
      "learning_rate": 4.6747967479674795e-05,
      "loss": 5.7096,
      "step": 48000
    },
    {
      "epoch": 65.71815718157181,
      "grad_norm": 13.010701179504395,
      "learning_rate": 4.671409214092141e-05,
      "loss": 5.7045,
      "step": 48500
    },
    {
      "epoch": 66.39566395663957,
      "grad_norm": 8.549729347229004,
      "learning_rate": 4.668021680216802e-05,
      "loss": 5.7061,
      "step": 49000
    },
    {
      "epoch": 67.07317073170732,
      "grad_norm": 7.454829216003418,
      "learning_rate": 4.664634146341464e-05,
      "loss": 5.6976,
      "step": 49500
    },
    {
      "epoch": 67.75067750677506,
      "grad_norm": 11.433731079101562,
      "learning_rate": 4.661246612466125e-05,
      "loss": 5.7339,
      "step": 50000
    },
    {
      "epoch": 68.42818428184282,
      "grad_norm": 9.491683959960938,
      "learning_rate": 4.657859078590786e-05,
      "loss": 5.6896,
      "step": 50500
    },
    {
      "epoch": 69.10569105691057,
      "grad_norm": 18.037641525268555,
      "learning_rate": 4.6544715447154476e-05,
      "loss": 5.7133,
      "step": 51000
    },
    {
      "epoch": 69.78319783197831,
      "grad_norm": 10.448026657104492,
      "learning_rate": 4.6510840108401086e-05,
      "loss": 5.6725,
      "step": 51500
    },
    {
      "epoch": 70.46070460704607,
      "grad_norm": 8.89246940612793,
      "learning_rate": 4.6476964769647696e-05,
      "loss": 5.701,
      "step": 52000
    },
    {
      "epoch": 71.13821138211382,
      "grad_norm": 9.766633987426758,
      "learning_rate": 4.644308943089431e-05,
      "loss": 5.7397,
      "step": 52500
    },
    {
      "epoch": 71.81571815718158,
      "grad_norm": 7.610794544219971,
      "learning_rate": 4.640921409214092e-05,
      "loss": 5.6946,
      "step": 53000
    },
    {
      "epoch": 72.49322493224932,
      "grad_norm": 11.947199821472168,
      "learning_rate": 4.637533875338754e-05,
      "loss": 5.6909,
      "step": 53500
    },
    {
      "epoch": 73.17073170731707,
      "grad_norm": 9.854961395263672,
      "learning_rate": 4.634146341463415e-05,
      "loss": 5.6739,
      "step": 54000
    },
    {
      "epoch": 73.84823848238483,
      "grad_norm": 6.153323173522949,
      "learning_rate": 4.630758807588076e-05,
      "loss": 5.6826,
      "step": 54500
    },
    {
      "epoch": 74.52574525745257,
      "grad_norm": 9.705251693725586,
      "learning_rate": 4.627371273712737e-05,
      "loss": 5.7062,
      "step": 55000
    },
    {
      "epoch": 75.20325203252033,
      "grad_norm": 8.191561698913574,
      "learning_rate": 4.6239837398373986e-05,
      "loss": 5.6917,
      "step": 55500
    },
    {
      "epoch": 75.88075880758808,
      "grad_norm": 8.579245567321777,
      "learning_rate": 4.62059620596206e-05,
      "loss": 5.6945,
      "step": 56000
    },
    {
      "epoch": 76.55826558265582,
      "grad_norm": 13.465226173400879,
      "learning_rate": 4.617208672086721e-05,
      "loss": 5.7235,
      "step": 56500
    },
    {
      "epoch": 77.23577235772358,
      "grad_norm": 7.795341968536377,
      "learning_rate": 4.613821138211382e-05,
      "loss": 5.6659,
      "step": 57000
    },
    {
      "epoch": 77.91327913279133,
      "grad_norm": 8.340959548950195,
      "learning_rate": 4.610433604336043e-05,
      "loss": 5.7015,
      "step": 57500
    },
    {
      "epoch": 78.59078590785907,
      "grad_norm": 9.505107879638672,
      "learning_rate": 4.607046070460705e-05,
      "loss": 5.7274,
      "step": 58000
    },
    {
      "epoch": 79.26829268292683,
      "grad_norm": 17.29327392578125,
      "learning_rate": 4.603658536585366e-05,
      "loss": 5.6947,
      "step": 58500
    },
    {
      "epoch": 79.94579945799458,
      "grad_norm": 12.023770332336426,
      "learning_rate": 4.600271002710027e-05,
      "loss": 5.6738,
      "step": 59000
    },
    {
      "epoch": 80.62330623306234,
      "grad_norm": 10.241198539733887,
      "learning_rate": 4.596883468834689e-05,
      "loss": 5.7083,
      "step": 59500
    },
    {
      "epoch": 81.30081300813008,
      "grad_norm": 18.282018661499023,
      "learning_rate": 4.59349593495935e-05,
      "loss": 5.6599,
      "step": 60000
    },
    {
      "epoch": 81.97831978319783,
      "grad_norm": 10.7692289352417,
      "learning_rate": 4.5901084010840114e-05,
      "loss": 5.7053,
      "step": 60500
    },
    {
      "epoch": 82.65582655826559,
      "grad_norm": 11.36131763458252,
      "learning_rate": 4.5867208672086724e-05,
      "loss": 5.7152,
      "step": 61000
    },
    {
      "epoch": 83.33333333333333,
      "grad_norm": 12.090702056884766,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 5.6963,
      "step": 61500
    },
    {
      "epoch": 84.01084010840108,
      "grad_norm": 11.715343475341797,
      "learning_rate": 4.579945799457995e-05,
      "loss": 5.6697,
      "step": 62000
    },
    {
      "epoch": 84.68834688346884,
      "grad_norm": 10.861991882324219,
      "learning_rate": 4.576558265582656e-05,
      "loss": 5.7116,
      "step": 62500
    },
    {
      "epoch": 85.36585365853658,
      "grad_norm": 7.252991199493408,
      "learning_rate": 4.573170731707318e-05,
      "loss": 5.686,
      "step": 63000
    },
    {
      "epoch": 86.04336043360433,
      "grad_norm": 7.696922779083252,
      "learning_rate": 4.569783197831978e-05,
      "loss": 5.669,
      "step": 63500
    },
    {
      "epoch": 86.72086720867209,
      "grad_norm": 8.799280166625977,
      "learning_rate": 4.56639566395664e-05,
      "loss": 5.684,
      "step": 64000
    },
    {
      "epoch": 87.39837398373983,
      "grad_norm": 7.576893329620361,
      "learning_rate": 4.563008130081301e-05,
      "loss": 5.7047,
      "step": 64500
    }
  ],
  "logging_steps": 500,
  "max_steps": 738000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 9250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8309171662848000.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
