"""
⚠️  IMPORTANT NOTICE - INTERNAL USE ONLY ⚠️
This dataset is for INTERNAL USE ONLY and will NOT be released to the public.
"""

import logging
import re
from itertools import chain
from typing import Iterable
from pathlib import Path

import pandas as pd
from datasets import IterableDataset, load_dataset
import pyarrow.compute as pc
import pyarrow as pa
import pyarrow.dataset as ds
from sqlalchemy import create_engine, text
import torch
from src.data.models import ColumnNames
from src.data.cache_manager import get_cache_manager
from src.data.diabetes_datasets.dataset_base import DatasetBase
from src.data.diabetes_datasets.gluroo.data_cleaner import data_translation
from src.data.dataset_configs import get_dataset_config
from src.data.preprocessing.pipeline import preprocessing_pipeline
from src.data.preprocessing.time_processing import (
    get_train_validation_split_by_percentage,
    iter_daily_context_forecast_splits,
)
from concurrent.futures import ProcessPoolExecutor, as_completed

logger = logging.getLogger(__name__)

"""
TODO: Tune the the params. patients_per_partition, patients_per_batch, target_file_size_mb, etc.
TODO: Add some meta data as a json file maybe
TODO: Double check p_num refracotring logic.
TODO: Create a streaming dataset. Hmmm maybe like a IterableDataset but it seems to have some issues. -> FIGURE OUT HOW TO DO THIS
This need to work with huggingface trainer.

"""


class GlurooDataLoader(DatasetBase):
    """
    Loader for Gluroo diabetes dataset with preprocessing and feature engineering.

    This class loads Gluroo diabetes data from TimescaleDB, processes each patient
    separately, and prepares it for training machine learning models. It follows
    the same pattern as AleppoDataLoader, storing processed data as one CSV per
    patient in the cache directory.

    Note:
    - gid is the base64 encoded which is hard to work with and contains special characters.
    - p_num is now a string identifier (e.g., gluroo_1) ordered by gid (generated by the script add_patient_id.sql).

    Attributes:
        keep_columns (list[str] | None): Columns to keep from the raw data.
        train_percentage (float): Percentage of data to use for training.
        config (dict | None): Configuration for data processing.
        use_cached (bool): Whether to use previously cached processed data.
        processed_data (dict[str, pd.DataFrame]): Processed data by patient ID.
        train_data (dict[str, pd.DataFrame]): Training data by patient ID.
        validation_data (dict[str, pd.DataFrame]): Validation data by patient ID.
    """

    def __init__(
        self,
        keep_columns: list[str] | None = None,
        use_cached: bool = True,
        train_percentage: float = 0.9,
        config: dict | None = None,
        max_workers: int = 10,
        parquet_batch_size: int = 100000,
    ):
        """
        Initialize the Gluroo data loader.

        Args:
            keep_columns (list[str] | None): Columns to retain from the raw data.
                If None, all columns are kept.
            use_cached (bool): If True, load previously processed data from cache
                instead of processing raw data again. Default is True.
            train_percentage (float): Percentage of data to use for training (0-1).
                Default is 0.9.
            config (dict | None): Configuration dictionary for data processing steps.
            max_workers (int): Maximum number of worker processes for parallel processing.
            parquet_batch_size (int): Number of rows to process per batch when loading
                from Parquet files. Default is 100000. Can be tuned for memory/performance.
        """
        self.keep_columns = keep_columns
        self.train_percentage = train_percentage
        self.cache_manager = get_cache_manager()
        self.dataset_config = get_dataset_config(self.dataset_name)
        self.use_cached = use_cached
        self.max_workers = max_workers
        self.config = config
        self.parquet_batch_size = parquet_batch_size
        # Hardcoded for now
        self.db_connection_string = (
            "postgresql://postgres:password@127.0.0.1:5433/gluroo_datasets"
        )

        self.processed_data: dict[str, pd.DataFrame] = {}
        self.train_data: dict[str, pd.DataFrame] = {}
        self.validation_data: dict[str, pd.DataFrame] | None = None
        self.total_skipped_patients: int = 0

        logger.info(f"Initializing GlurooDataLoader with use_cached={use_cached}")
        self.load_data()

    @property
    def dataset_name(self):
        return "gluroo"

    @property
    def description(self):
        return """
        Gluroo diabetes dataset loaded from TimescaleDB.
        Contains continuous glucose monitoring data, meal announcements, and insulin dosing
        information for multiple patients.
        This is for internal use only. The dataset will NOT be released to the public as they are collected within Gluroo and therefore confidential.
        """

    def load_data(self):
        """
        Load processed data from cache or process raw data from database.

        If use_cached is True, loads previously processed data from Parquet cache.
        Otherwise, loads raw data from TimescaleDB, processes each patient,
        and saves processed data to cache as partitioned Parquet.
        Then splits the processed data into training and validation sets.

        Note: processed_data will remain empty to avoid loading all data into memory.
        Use get_parquet_dataset() for streaming access to the Parquet files.
        """
        need_to_process_data = True
        if self.use_cached:
            processed_path = self.cache_manager.get_absolute_path_by_type(
                self.dataset_name, "processed"
            )
            parquet_path = processed_path / "parquet"
            if parquet_path.exists() and any(parquet_path.glob("**/*.parquet")):
                need_to_process_data = False
                logger.info("Found cached Parquet data; skipping processing.")
            else:
                logger.warning(
                    "use_cached=True but no Parquet found. Will process from source."
                )

        if need_to_process_data:
            logger.warning(
                "This is a very long running operation. Make sure to run it as a job."
            )
            # Ideally, we wouldn't want to process the data when trying to load the data for training.
            # We would submit a job to get data to process then load the data later.
            # so if the code reaches here, it means this is a batch job not training so probably doesn't need to load the data when done.
            self._process_and_cache_data(batch_size=20)

        # TODO: Maybe this is not needed because Chris is doing its own things?
        # self.train_data, self.validation_data = self._split_train_validation()

    def _get_all_patient_ids(self) -> list[tuple[str, str]]:
        """
        Get all unique patient IDs sorted by gid from the database. A patient is a group.

        Returns:
            list[tuple[str, str]]: List of patient tuples (gid, p_num) sorted by gid (each gid is mapped to one string p_num)
        """
        engine = create_engine(self.db_connection_string)
        with engine.connect() as conn:
            # TODO: Might need to change this for now because the example dataset doesn't even satisfy the foreign key constraint....
            # TODO: Might make sense to just make up a group in groups table for the test data.
            result = conn.execute(
                text("SELECT DISTINCT gid, p_num FROM groups ORDER BY gid")
            )
            gids_p_nums = [(row[0], str(row[1])) for row in result.fetchall()]
        return gids_p_nums

    def _get_patient_id_mapping(self) -> dict[str, str]:
        """
        Get mapping from gid to integer p_num from the database.

        Returns:
            dict[str, str]: Dictionary mapping gid to p_num (string, ordered) (each gid is mapped to one p_num)
        """
        engine = create_engine(self.db_connection_string)
        with engine.connect() as conn:
            # Get gid and p_num from groups table, ordered by p_num for consistency
            result = conn.execute(text("SELECT gid, p_num FROM groups ORDER BY p_num"))
            mapping = {row[0]: str(row[1]) for row in result.fetchall()}
        return mapping

    def load_raw(self, patient_ids: list[str] | None = None) -> dict[str, pd.DataFrame]:
        """
        Load raw data from TimescaleDB for specified patients.

        Args:
            patient_ids: List of patient IDs to load. If None, loads all patients.

        Returns:
            dict[str, pd.DataFrame]: Dictionary mapping patient IDs (gid) to raw DataFrames
        """
        if patient_ids is None:
            # id is the base64 encoded
            patient_ids_p_nums = self._get_all_patient_ids()
            logger.info(
                f"Loading raw data for all {len(patient_ids_p_nums)} groups from TimescaleDB..."
            )
            # Extract just the gids for iteration
            patient_ids = [gid for gid, _p_num in patient_ids_p_nums]
        else:
            logger.info(
                f"Loading raw data for {len(patient_ids)} patients from TimescaleDB..."
            )

        engine = create_engine(self.db_connection_string)
        raw_data = {}

        for gid in patient_ids:
            # TODO: This will be a bottleneck. Should query all patients at once.
            df = self._load_patient_data_from_db(engine, gid)
            if df is not None and not df.empty:
                raw_data[gid] = df
            else:
                self.total_skipped_patients += 1

        logger.info(f"Loaded raw data for {len(raw_data)} patients")
        return raw_data

    def _load_patient_data_from_db(self, engine, gid: str) -> pd.DataFrame | None:
        """
        Load raw data for a single patient from TimescaleDB.

        Combines readings and messages tables for a specific patient (gid).

        Args:
            engine: SQLAlchemy engine
            gid: Patient group ID

        Returns:
            pd.DataFrame: Combined raw data for the patient, or None if no data
        """
        # Use UNION instead of FULL OUTER JOIN to speed up the query while maintaining chronological order
        # Join with groups table to get string p_num (This is generated by the script add_patient_id.sql)
        query = text("""
            SELECT
                r.date,
                r.bgl,
                r.trend,
                r.source,
                NULL::TEXT as msg_type,
                NULL::FLOAT as food_g,
                NULL::FLOAT as dose_units,
                NULL::TEXT as dose_type,
                NULL::INT as exercise_mins,
                NULL::TEXT as exercise_level,
                g.p_num
            FROM readings r
            JOIN groups g ON r.gid = g.gid
            WHERE r.gid = :gid
            UNION ALL
            SELECT
                m.date,
                NULL::INT as bgl,
                NULL::trend_type as trend,
                NULL::TEXT as source,
                m.type as msg_type,
                m.food_g,
                m.dose_units,
                m.dose_type,
                m.exercise_mins,
                m.exercise_level,
                g.p_num
            FROM messages m
            JOIN groups g ON m.gid = g.gid
            WHERE m.gid = :gid
            ORDER BY date
        """)

        try:
            with engine.connect() as conn:
                df = pd.read_sql(query, conn, params={"gid": gid})
                if df.empty:
                    logger.warning(f"No data found for patient {gid}. Skipping...")
                    return None

                # Add patient ID column (string p_num)
                df[ColumnNames.P_NUM.value] = df["p_num"]
                # Convert 'date' column to datetime (will be renamed to 'datetime' in data_translation)
                df[ColumnNames.DATETIME.value] = pd.to_datetime(df["date"])

                return df
        except Exception as e:
            logger.warning(f"Error loading data for patient {gid}: {e}")
            return None

    def _process_and_cache_data(self, batch_size: int = 10):
        """
        Process raw data and save to cache in batches.

        Loads raw data from database in batches, processes each batch,
        and saves processed data to cache as partitioned Parquet.
        This avoids loading all patients' raw data into memory at once.

        Args:
            batch_size: Number of patients to process in each batch. Default is 100. This can be tuned to optimize for memory usage and processing speed.
        """
        # Get all patient IDs with p_nums first (sorted by gid)
        all_patient_ids_p_nums = self._get_all_patient_ids()
        total_patients = len(all_patient_ids_p_nums)
        logger.info(f"Processing {total_patients} patients in batches of {batch_size}")

        processed_path = self.cache_manager.get_absolute_path_by_type(
            self.dataset_name, "processed"
        )
        processed_path.parent.mkdir(parents=True, exist_ok=True)
        processed_path.mkdir(parents=True, exist_ok=True)

        # Process patients in batches and save incrementally
        parquet_path = processed_path / "parquet"
        parquet_path.mkdir(parents=True, exist_ok=True)

        for batch_start in range(0, total_patients, batch_size):
            batch_end = min(batch_start + batch_size, total_patients)
            batch_patient_ids_p_nums = all_patient_ids_p_nums[batch_start:batch_end]
            # Extract just the gids for load_raw
            batch_patient_ids = [gid for gid, _p_num in batch_patient_ids_p_nums]
            batch_num = (batch_start // batch_size) + 1
            total_batches = (total_patients + batch_size - 1) // batch_size

            logger.info(
                f"Processing batch {batch_num}/{total_batches}: "
                f"patients {batch_start+1}-{batch_end} of {total_patients}"
            )

            # Load raw data for this batch (returns dict with gid as keys)
            batch_raw_data_gid = self.load_raw(batch_patient_ids)
            logger.info(f"Total skipped patients so far: {self.total_skipped_patients}")

            #### From this point on, we only use p_num to identify patients. gid is not used anymore. #####
            # Convert from {gid: df} to {p_num: df} for processing
            batch_raw_data = {}
            gid_to_pnum_map = {}  # Keep mapping for error messages
            for gid, df in batch_raw_data_gid.items():
                if "p_num" not in df.columns or df.empty:
                    logger.warning(
                        f"Patient {gid} missing p_num column or empty DataFrame. Skipping..."
                    )
                    continue
                p_num = str(df["p_num"].iloc[0])
                batch_raw_data[p_num] = df
                gid_to_pnum_map[p_num] = gid

            # Process this batch with parallel processing (now uses p_num as keys)
            batch_processed_data = self._process_raw_data_batch(
                batch_raw_data, processed_path, gid_to_pnum_map
            )

            # Save this batch immediately to free up memory
            logger.info(f"Saving batch {batch_num}/{total_batches} to Parquet...")
            self._save_batch_incremental(
                batch_processed_data,
                batch_raw_data,  # Pass the p_num-keyed dict for reference
                parquet_path,
                batch_num,
            )

            # Clear batch data from memory after saving
            del batch_raw_data

            # Clear batch processed data from memory
            del batch_processed_data

            logger.info(
                f"Completed processing and saving batch {batch_num}/{total_batches}"
            )

    def _process_raw_data(
        self, raw_data: pd.DataFrame, gid: str
    ) -> pd.DataFrame | None:
        pass

    def _process_raw_data_batch(
        self,
        raw_data: dict[str, pd.DataFrame],
        processed_path: Path,
        gid_to_pnum_map: dict[str, str],
    ) -> dict[str, pd.DataFrame]:
        """
        Process a batch of raw data for patients.

        Args:
            raw_data: Dictionary mapping p_num (string) to raw DataFrames
            processed_path: Path to processed data directory (not used here, but kept for compatibility)
            gid_to_pnum_map: Dictionary mapping p_num to gid for error messages

        Returns:
            dict[str, pd.DataFrame]: Dictionary mapping p_num to processed DataFrames
        """
        processed_data = {}

        # Prepare patient data tuples for parallel processing
        # Sort by p_num to ensure deterministic ordering
        patient_tuples = sorted(
            [(p_num, df, processed_path) for p_num, df in raw_data.items()],
            key=lambda x: self._p_num_to_int(x[0]),  # Sort by numeric portion of p_num
        )

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks and track them by p_num
            future_to_pnum = {
                executor.submit(
                    process_single_patient,
                    patient_tuple,
                    # Data: (p_num, df, processed_path)
                    # Return: (p_num, df)
                ): patient_tuple[0]
                for patient_tuple in patient_tuples
            }

            # Collect results as they complete (order may vary)
            results = {}
            for index, future in enumerate(as_completed(future_to_pnum), 1):
                p_num = future_to_pnum[future]
                gid = gid_to_pnum_map.get(p_num, "unknown")
                progress = f"({index}/{len(raw_data)})"
                try:
                    result_pnum, df = future.result()
                    if df.empty:
                        logger.warning(
                            f"Processed data is empty for patient p_num={result_pnum} (gid={gid})"
                        )
                        continue
                    results[result_pnum] = df
                    logger.debug(
                        f"Successfully processed patient p_num={result_pnum} (gid={gid}) {progress}"
                    )
                except Exception as exc:
                    logger.error(
                        f"Patient p_num={p_num} (gid={gid}) generated an exception: {exc} {progress}"
                    )

            # Reconstruct processed_data in deterministic order (sorted by p_num)
            for p_num, _df, _processed_path in patient_tuples:
                if p_num in results:
                    processed_data[p_num] = results[p_num]

        return processed_data

    def _process_one_patient(
        self, df_raw: pd.DataFrame, p_num: str, gid: str = "unknown"
    ) -> pd.DataFrame | None:
        """
        Process raw data for a single patient.

        Args:
            df_raw: Raw DataFrame for the patient (must contain p_num column from database)
            p_num: Patient number (string identifier)
            gid: Patient group ID (used for error messages only)

        Returns:
            pd.DataFrame: Processed DataFrame, or None if processing fails
        """
        try:
            # Verify p_num matches DataFrame
            if "p_num" not in df_raw.columns:
                raise ValueError(
                    f"DataFrame missing p_num column for patient p_num={p_num} (gid={gid})"
                )
            df_p_num = str(df_raw["p_num"].iloc[0])
            if df_p_num != p_num:
                logger.warning(
                    f"p_num mismatch: expected {p_num}, got {df_p_num} for patient gid={gid}"
                )

            # Translate to standardized format
            df = data_translation(df_raw)

            # Apply preprocessing pipeline using p_num
            df = preprocessing_pipeline(str(p_num), df, use_aggregation=True)

            return df
        except Exception as e:
            logger.error(f"Error processing patient p_num={p_num} (gid={gid}): {e}")
            return None

    # TODO: Not sure if this is needed.
    def _split_train_validation(
        self,
    ) -> tuple[dict[str, pd.DataFrame], dict[str, pd.DataFrame]]:
        """
        Split processed data into train and validation dicts per patient.

        Uses train_percentage to split each patient's data chronologically.

        Returns:
            tuple: (train_dict, val_dict) where each is a dict mapping patient IDs to DataFrames
        """
        train_dict: dict[str, pd.DataFrame] = {}
        val_dict: dict[str, pd.DataFrame] = {}

        for patient_id, df in self.processed_data.items():
            try:
                patient_df = df.copy()

                # Ensure DatetimeIndex
                if not isinstance(patient_df.index, pd.DatetimeIndex):
                    if "datetime" in patient_df.columns:
                        patient_df = patient_df.sort_values("datetime").set_index(
                            "datetime"
                        )
                    else:
                        logger.warning(
                            f"Patient {patient_id} skipped: missing 'datetime' column"
                        )
                        continue

                patient_df = patient_df.sort_index()

                # Split by percentage
                train_df, val_df, _ = get_train_validation_split_by_percentage(
                    patient_df, train_percentage=self.train_percentage
                )

                train_dict[patient_id] = train_df
                val_dict[patient_id] = val_df

            except Exception as e:
                logger.warning(f"Patient {patient_id} skipped due to error: {e}")
                continue

        return train_dict, val_dict

    # TODO: Not sure if this is needed.
    def get_validation_day_splits(self, patient_id: str):
        """
        Generate day-by-day training and testing periods for a specific patient.

        For each day in the validation data, yields:
        - Current day's data from 6am-12am (training period)
        - Next day's data from 12am-6am (testing/prediction period)

        Args:
            patient_id (str): Identifier for the patient whose data to split.

        Yields:
            tuple: (patient_id, train_period_data, test_period_data)
        """
        if self.validation_data is None:
            raise ValueError("Validation data is not loaded")

        if patient_id not in self.validation_data:
            raise ValueError(f"Patient {patient_id} not found in validation data")

        patient_data = self.validation_data[patient_id]
        for train_period, test_period in self._get_day_splits(patient_data):
            yield patient_id, train_period, test_period

    # TODO: Not sure if this is needed.
    def _get_day_splits(
        self,
        patient_data: pd.DataFrame,
        context_period: tuple[int, int] = (6, 24),
        forecast_horizon: tuple[int, int] = (0, 6),
    ):
        """
        Split each day's data into context period and forecast horizon.
        TODO: Not sure if this is needed.

        Args:
            patient_data: Data for a single patient with DatetimeIndex
            context_period: Start and end hours for context period (default: 6am-midnight)
            forecast_horizon: Start and end hours for forecast period (default: midnight-6am)

        Yields:
            tuple: (context_data, forecast_data)
        """
        yield from iter_daily_context_forecast_splits(
            patient_data,
            context_period=context_period,
            forecast_horizon=forecast_horizon,
        )

    def _p_num_to_int(self, p_num: int | str) -> int:
        """
        Extract integer component from p_num string (e.g., gluroo_123 -> 123).
        Falls back to direct integer if already numeric.
        """
        if isinstance(p_num, int):
            return p_num

        match = re.search(r"(\d+)$", str(p_num))
        if match:
            return int(match.group(1))
        raise ValueError(f"Invalid p_num format: {p_num}")

    def _get_partition_number(
        self, patient_index: int | str, patients_per_partition: int = 400
    ) -> int:
        """
        Get partition number for a patient based on sequential ordering.
        Accepts either numeric index or p_num string (e.g., gluroo_123).
        """
        numeric_index = self._p_num_to_int(patient_index)
        return numeric_index // patients_per_partition

    def _save_batch_incremental(
        self,
        batch_processed_data: dict[str, pd.DataFrame],
        batch_raw_data: dict[str, pd.DataFrame],
        parquet_path: Path,
        batch_num: int,
        patients_per_partition: int = 400,
    ):
        """
        Save a batch of processed data incrementally to Parquet files.

        This method saves each batch immediately after processing to free up memory.
        It determines partition numbers based on string p_num from the database.

        Args:
            batch_processed_data: Dictionary mapping p_num (str) to processed DataFrames for this batch
            batch_raw_data: Dictionary mapping p_num (str) to raw DataFrames (used to extract p_num for patient_id column)
            parquet_path: Path to Parquet directory
            batch_num: Batch number for unique file naming
            patients_per_partition: Number of patients per partition (default: 400)
            TODO: patients_per_partition will need to be tuned too. We don't want too many files
        """
        # Group batch patients by partition using numeric component of p_num
        partition_groups: dict[int, list[tuple[str, pd.DataFrame]]] = {}
        for p_num in batch_processed_data.keys():
            if p_num not in batch_processed_data:
                continue  # Skip if processing failed

            partition_num = self._get_partition_number(p_num, patients_per_partition)

            if partition_num not in partition_groups:
                partition_groups[partition_num] = []
            partition_groups[partition_num].append((p_num, batch_processed_data[p_num]))

        # Save each partition group
        for partition_num in sorted(partition_groups.keys()):
            partition_dir = parquet_path / f"partition={partition_num:03d}"
            partition_dir.mkdir(parents=True, exist_ok=True)

            # Prepare data for this partition
            partition_data = []
            for p_num, df in partition_groups[partition_num]:
                df_with_id = df.copy()
                df_with_id["patient_id"] = p_num  # Use p_num as patient_id
                df_with_id = df_with_id.reset_index()  # Convert index to column
                partition_data.append(df_with_id)

            if not partition_data:
                continue

            # Concatenate all patients in this partition for this batch
            batch_df = pd.concat(partition_data, ignore_index=True)

            # Save to a unique file for this batch and partition
            # Using batch_num ensures uniqueness across batches
            batch_file = partition_dir / f"batch_{batch_num:06d}.parquet"
            batch_df.to_parquet(
                batch_file,
                engine="pyarrow",
                compression="snappy",  # Doesn't compress as much as gzip, but it's faster at decompressing.
                index=False,
            )

            logger.debug(
                f"Saved batch {batch_num}, partition {partition_num}: "
                f"{len(partition_groups[partition_num])} patients to {batch_file}"
            )

            # Clear from memory
            del batch_df
            del partition_data

    def _save_as_partitioned_parquet(
        self,
        processed_data: dict[str, pd.DataFrame],
        processed_path: Path,
        patients_per_partition: int = 400,
        patients_per_batch: int = 100,
        target_file_size_mb: float = 500.0,
    ):
        """
        Save processed data as partitioned Parquet files.

        Partitions patients sequentially (partition 0 = first N patients, partition 1 = next N, etc.)
        and batches multiple patients per file for efficient storage and streaming.

        Args:
            processed_data: Dictionary mapping patient IDs to processed DataFrames
            processed_path: Path to processed data directory
            patients_per_partition: Number of patients per partition (default: 400)
            patients_per_batch: Target number of patients per batch file
            target_file_size_mb: Target file size in MB (approximate)
        """
        parquet_path = processed_path / "parquet"
        parquet_path.mkdir(parents=True, exist_ok=True)

        # Convert dict to list and sort by patient_id for deterministic ordering
        # This ensures consistent partitioning across runs
        patients_list = sorted(processed_data.items(), key=lambda x: x[0])
        total_patients = len(patients_list)

        # Group patients by sequential partition
        partition_groups: dict[int, list[tuple[str, pd.DataFrame]]] = {}
        for patient_index, (patient_id, df) in enumerate(patients_list):
            partition_num = self._get_partition_number(
                patient_index, patients_per_partition
            )
            if partition_num not in partition_groups:
                partition_groups[partition_num] = []
            partition_groups[partition_num].append((patient_id, df))

        num_partitions = len(partition_groups)
        logger.info(
            f"Saving {total_patients} patients to {num_partitions} sequential partitions "
            f"(~{patients_per_partition} patients per partition)"
        )

        # Save each partition
        total_files = 0
        for partition_num in sorted(partition_groups.keys()):
            patients = partition_groups[partition_num]
            partition_dir = parquet_path / f"partition={partition_num:03d}"
            partition_dir.mkdir(parents=True, exist_ok=True)

            # Batch patients within each partition
            batch_num = 0
            current_batch: list[pd.DataFrame] = []
            current_patient_ids: list[str] = []
            current_size_mb = 0.0

            for patient_id, df in patients:
                # Add patient_id column to DataFrame
                df_with_id = df.copy()
                df_with_id["patient_id"] = patient_id
                df_with_id = df_with_id.reset_index()  # Convert index to column

                # Estimate size (rough approximation)
                df_size_mb = df_with_id.memory_usage(deep=True).sum() / (1024**2)

                # Check if we should start a new batch
                if len(current_batch) >= patients_per_batch or (
                    current_size_mb > 0
                    and current_size_mb + df_size_mb > target_file_size_mb
                ):
                    # Save current batch
                    batch_df = pd.concat(current_batch, ignore_index=True)
                    batch_file = partition_dir / f"batch_{batch_num:04d}.parquet"
                    batch_df.to_parquet(
                        batch_file,
                        engine="pyarrow",
                        compression="snappy",
                        index=False,
                    )
                    logger.debug(
                        f"Saved batch {batch_num} to {batch_file} "
                        f"({len(current_batch)} patients, ~{current_size_mb:.1f}MB)"
                    )
                    total_files += 1
                    batch_num += 1
                    current_batch = []
                    current_patient_ids = []
                    current_size_mb = 0.0

                current_batch.append(df_with_id)
                current_patient_ids.append(patient_id)
                current_size_mb += df_size_mb

            # Save remaining batch
            if current_batch:
                batch_df = pd.concat(current_batch, ignore_index=True)
                batch_file = partition_dir / f"batch_{batch_num:04d}.parquet"
                batch_df.to_parquet(
                    batch_file,
                    engine="pyarrow",
                    compression="snappy",
                    index=False,
                )
                logger.debug(
                    f"Saved batch {batch_num} to {batch_file} "
                    f"({len(current_batch)} patients, ~{current_size_mb:.1f}MB)"
                )
                total_files += 1

        logger.info(
            f"Saved {total_files} Parquet batch files across {len(partition_groups)} partitions"
        )

    # TODO: THIS SHOULD BE REMOVED TOO
    def _load_from_parquet(
        self, batch_size: int = 100000
    ) -> dict[str, pd.DataFrame] | None:
        """
        Load processed data from partitioned Parquet files.

        WARNING: This loads the ENTIRE dataset into memory. For large datasets,
        use get_parquet_dataset() instead for true streaming access.

        Reads data in batches for I/O efficiency, but still accumulates all data in memory.

        Args:
            batch_size (int): Number of rows to process per batch. Default is 100000.
                Can be tuned for memory/performance optimization.

        Returns:
            Dictionary mapping patient IDs to DataFrames, or None if Parquet files don't exist
        """
        processed_path = self.cache_manager.get_absolute_path_by_type(
            self.dataset_name, "processed"
        )
        parquet_path = processed_path / "parquet"

        if not parquet_path.exists():
            return None

        # Check if parquet directory has partition subdirectories
        # List of all partition=* directories: like [partition=000, partition=001, ...]
        partition_dirs = [
            d
            for d in parquet_path.iterdir()
            if d.is_dir() and d.name.startswith("partition=")
        ]
        if not partition_dirs:
            return None

        logger.info(
            f"Loading data from {len(partition_dirs)} Parquet partitions (lazy loading)..."
        )

        # Use PyArrow Dataset for efficient lazy reading
        try:
            dataset = ds.dataset(str(parquet_path), format="parquet")

            # Use scanner for lazy batch reading
            # https://arrow.apache.org/docs/python/dataset.html#writing-large-amounts-of-data
            scanner = dataset.scanner()

            # Dictionary to accumulate data by patient_id
            processed_data: dict[str, list[pd.DataFrame]] = {}

            # Read data in batches
            for batch in scanner.to_batches(max_rows=batch_size):
                # Convert batch to pandas DataFrame
                df_batch = batch.to_pandas()

                # Group by patient_id and accumulate
                for patient_id, group_df in df_batch.groupby("patient_id"):
                    if patient_id not in processed_data:
                        processed_data[patient_id] = []
                    processed_data[patient_id].append(group_df)

            # Combine all batches for each patient and format
            result = {}
            for patient_id, df_list in processed_data.items():
                # Concatenate all batches for this patient
                patient_df = pd.concat(df_list, ignore_index=True)

                # Set datetime as index if it exists
                if "datetime" in patient_df.columns:
                    patient_df = patient_df.sort_values("datetime").set_index(
                        "datetime"
                    )
                    patient_df.index = pd.to_datetime(patient_df.index)
                else:
                    # If no datetime column, sort by index
                    patient_df = patient_df.sort_index()

                # Remove patient_id column (it was added for grouping)
                patient_df = patient_df.drop(columns=["patient_id"], errors="ignore")
                result[patient_id] = patient_df

            logger.info(f"Loaded {len(result)} patients from Parquet (lazy loading)")
            return result

        except Exception as e:
            logger.warning(f"Error loading from Parquet: {e}")
            return None

    def get_parquet_dataset(self) -> ds.Dataset:
        """
        Get PyArrow Dataset for streaming access to Parquet files. The entire processed data can be treated as a single logical dataset.

        This is useful for training with PyTorch DataLoader where you want
        to stream data efficiently without loading everything into memory.

        Returns:
            PyArrow Dataset object, or None if Parquet files don't exist
        """
        # this does not load the data. If needed, it only crawls the dir to find all the files
        processed_path = self.cache_manager.get_absolute_path_by_type(
            self.dataset_name, "processed"
        )
        parquet_path = processed_path / "parquet"

        if not parquet_path.exists():
            raise FileNotFoundError(f"Parquet files not found at {parquet_path}")

        try:
            dataset = ds.dataset(str(parquet_path), format="parquet")
            return dataset
        except Exception as e:
            logger.warning(f"Error creating PyArrow Dataset: {e}")
            raise

    def get_hf_streaming_dataset(
        self,
        columns: list[str] | None = None,
        patient_ids: Iterable[str] | None = None,
        batch_size: int | None = None,
        validate_non_empty: bool = True,
    ):
        """
        Return a Hugging Face IterableDataset that streams local Parquet files with streaming=True.

        Args:
            columns: Optional subset of columns to load (passed to load_dataset).
            patient_ids: Optional iterable of patient_ids (string p_num identifiers) to filter.
            batch_size: Optional on-the-fly batching for downstream consumers.
            validate_non_empty: If True, peek one element to surface empty dataset errors early.

        Returns:
            datasets.iterable_dataset.IterableDataset configured for streaming.
        """
        processed_path = self.cache_manager.get_absolute_path_by_type(
            self.dataset_name, "processed"
        )
        parquet_path = processed_path / "parquet"
        if not parquet_path.exists():
            raise FileNotFoundError(f"Parquet files not found at {parquet_path}")

        data_files = str(parquet_path / "**" / "*.parquet")
        load_kwargs = {
            "path": "parquet",
            "data_files": data_files,
            "split": "train",
            "streaming": True,
        }
        if columns:
            load_kwargs["columns"] = columns

        dataset = load_dataset(**load_kwargs)

        if isinstance(dataset, dict):
            dataset = dataset["train"]

        if patient_ids:
            patient_ids_set = {str(pid) for pid in patient_ids}
            dataset = dataset.filter(
                lambda example: str(example.get("patient_id", "")) in patient_ids_set
            )

        if batch_size:
            dataset = dataset.batch(batch_size=batch_size)

        if validate_non_empty:
            try:
                # Peek to fail fast when filters/paths produce an empty stream.
                # Prepend the peeked element back so no data is lost.
                first_item = next(iter(dataset))
                dataset = IterableDataset.from_generator(
                    lambda: chain([first_item], dataset)
                )
            except StopIteration as e:
                raise ValueError(
                    f"Hugging Face streaming dataset is empty. "
                    f"Parquet path: {parquet_path}. "
                    f"Filters -> columns={columns}, patient_ids={patient_ids}."
                ) from e
            except Exception as e:  # pragma: no cover - defensive
                raise ValueError(
                    f"Failed to initialize streaming dataset from {parquet_path}: {e}"
                ) from e

        return dataset

    def get_torch_iterable(
        self,
        columns: list[str] | None = None,
        batch_size: int = 8192,
        patient_ids: Iterable[str] | None = None,
    ) -> torch.utils.data.IterableDataset:
        """
        Create a torch IterableDataset that streams batches from the Parquet dataset.

        Args:
            columns: Columns to project. If None, all columns are used (not recommended for large data).
            batch_size: Arrow batch size for streaming (separate from Trainer per-device batch).
            patient_ids: Optional iterable of patient_ids to filter (expects string p_num identifiers).

        Returns:
            torch.utils.data.IterableDataset suitable for Hugging Face Trainer.
        """

        base_ds = self.get_parquet_dataset()
        if patient_ids:
            patient_ids_set = set(patient_ids)
            base_ds = base_ds.filter(pc.field("patient_id").isin(patient_ids_set))

        projected_cols = columns

        class GlurooIterable(torch.utils.data.IterableDataset):
            def __iter__(self_inner):
                for batch in base_ds.to_batches(
                    columns=projected_cols, batch_size=batch_size
                ):
                    data = {}
                    for col_name in (
                        batch.schema.names if projected_cols is None else projected_cols
                    ):
                        col = batch.column(col_name)
                        if pa.types.is_integer(col.type):
                            tensor = torch.tensor(
                                col.to_numpy(zero_copy_only=False), dtype=torch.int64
                            )
                        elif pa.types.is_floating(col.type):
                            tensor = torch.tensor(
                                col.to_numpy(zero_copy_only=False), dtype=torch.float32
                            )
                        else:
                            # Fallback: keep as Python objects (e.g., strings); users can post-process in collate_fn
                            tensor = col.to_pylist()
                        data[col_name] = tensor

                    # Yield whole batch (tensors where possible, python lists for strings)
                    # Downstream DataLoader/Trainer can use a collate_fn to handle list->tensor or keep strings as-is.
                    yield data

        return GlurooIterable()

    def peek_parquet(
        self,
        columns: list[str] | None = None,
        n_rows: int = 20,
    ) -> pd.DataFrame:
        """
        Return a small Pandas sample from the Parquet dataset without loading everything.

        Args:
            columns: Columns to project. If None, all columns are returned.
            n_rows: Number of rows to peek (pulled from the first batch).
        """
        dataset = self.get_parquet_dataset()
        try:
            batch = next(dataset.to_batches(columns=columns, batch_size=n_rows))
        except StopIteration:
            return pd.DataFrame()
        return batch.to_pandas().head(n_rows)


# Standalone function for parallel processing
def process_single_patient(patient_tuple: tuple[str, pd.DataFrame, Path]) -> tuple:
    """
    Process a single patient file for parallel execution.

    Note: This function no longer saves individual CSV files.
    All patients are batched and saved as partitioned Parquet after processing.

    Args:
        patient_tuple: Tuple containing (p_num, df_raw, processed_path)

    Returns:
        tuple: (p_num, processed_df) - returns p_num (str) for mapping
    """
    p_num, df_raw, _processed_path = patient_tuple

    if df_raw.empty:
        # This shouldn't really happen. But just in case.
        logger.warning(f"Raw data is empty for patient p_num={p_num}. Skipping...")
        return (p_num, None)

    # Extract p_num from DataFrame to verify it matches
    if "p_num" not in df_raw.columns:
        logger.error(
            f"DataFrame missing p_num column for patient p_num={p_num}. Skipping..."
        )
        return (p_num, None)

    df_p_num = str(df_raw["p_num"].iloc[0])
    if df_p_num != p_num:
        logger.warning(f"p_num mismatch: expected {p_num}, got {df_p_num}")

    logger.info(f"Processing patient p_num={p_num}")

    # Process the patient
    try:
        df = data_translation(df_raw)
        df = preprocessing_pipeline(str(p_num), df, use_aggregation=True)
        # Debugging only: use p_num for filename (cleaner than gid)
        df.to_csv(f"processed_{p_num}.csv", index=True)  # Debugging only

        if df.empty:
            raise ValueError(f"Processed data is empty for patient p_num={p_num}")

        logger.debug(f"Done processing patient p_num={p_num}")

        return (p_num, df)
    except Exception as e:
        logger.error(f"Error processing patient p_num={p_num}: {e}")
        raise
