# Copyright (c) 2025 Blood-Glucose-Control
# Licensed under Custom Research License (see LICENSE file)
# For commercial licensing, contact: christopher/cjrisi AT gluroo/uwaterloo DOT com/ca

"""
⚠️  IMPORTANT NOTICE - INTERNAL USE ONLY ⚠️
This dataset is for INTERNAL USE ONLY and will NOT be released to the public.
"""

import json
import logging
import re
from datetime import datetime, timezone
from itertools import chain
from typing import Iterable, TypedDict
from pathlib import Path

import pandas as pd
from datasets import IterableDataset, load_dataset
from sqlalchemy import bindparam, create_engine, text
from src.data.models import ColumnNames
from src.data.cache_manager import get_cache_manager
from src.data.diabetes_datasets.dataset_base import DatasetBase
from src.data.diabetes_datasets.gluroo.data_cleaner import data_translation
from src.data.dataset_configs import get_dataset_config
from src.data.preprocessing.pipeline import preprocessing_pipeline
from concurrent.futures import ProcessPoolExecutor, as_completed

logger = logging.getLogger(__name__)

PROCESSING_CHECKPOINT_FILENAME = "processing_checkpoint.json"

"""
TODO: Tune the the params. patients_per_partition, patients_per_batch, etc.
TODO: Create a streaming dataset. Hmmm maybe like a IterableDataset but it seems to have some issues with huggingface Trainer.
This need to work with huggingface trainer.
TODO: Add a way to know if the data processing failed and keep going?
TODO: Add a way to log the time taken to process for each batch or each run.
"""

class Gluroo2026DataLoader(DatasetBase):
    """
    Loader for Gluroo diabetes dataset with preprocessing and feature engineering.

    This class loads Gluroo diabetes data from TimescaleDB, processes each patient
    separately, and prepares it for training machine learning models. It follows
    the same pattern as AleppoDataLoader, storing processed data as one CSV per
    patient in the cache directory.

    Note:
    - gid is the base64 encoded which is hard to work with and contains special characters.
    - p_num (the internal id) the patient now a string identifier (e.g., gluroo_1) ordered by gid (generated by the script add_patient_id.sql).
    - p_num is the patient_id

    Attributes:
        keep_columns (list[str] | None): Columns to keep from the raw data.
        config (dict | None): Configuration for data processing.
        use_cached (bool): Whether to use previously cached processed data.
        processed_data (dict[str, pd.DataFrame]): Processed data by patient ID.
        train_data (dict[str, pd.DataFrame]): Training data by patient ID.
        validation_data (dict[str, pd.DataFrame]): Validation data by patient ID.
    """

    def __init__(
        self,
        keep_columns: list[str] | None = None,
        use_cached: bool = True,
        max_workers: int = 10,

        patients_per_batch: int = 100,
        # TODO: This will need to be tuned. Can we fit 100 patients per batch in memory?
        # 100 patients size can also vary depending on the data.

        patients_per_file: int = 400,
        # TODO: This will need to be tuned too
        # Higher: less files, larger reads (better IO performance)
        # Lower: more files, better for dataloader workers
        # Total files = 100k / patients_per_file
        # so 100k / 400 = 250 files?

        number_of_patients_to_process: int = 100,
        min_date_span_days: int = 30,
        load_all: bool = False,
    ):
        """
        Initialize the Gluroo data loader.

        Args:
            keep_columns (list[str] | None): Columns to retain from the raw data.
                If None, all columns are kept.
            use_cached (bool): If True, load previously processed data from cache
                instead of processing raw data again. Default is True.
                IMPORTANT: This should never be false
            patients_per_batch: Number of patients per batch. This is the number of patients we query from the database to process at a time.
            patients_per_file: Number of patients per output Parquet file. Batches fill files in order; a batch may write to 0, 1, or 2+ files when it straddles the fill boundary.
            number_of_patients_to_process: Number of patients to process for this run starting from the checkpoint. This has to a scalar multiple of patients_per_batch.
            max_workers: Maximum number of workers to use for parallel processing within a batch.
            min_date_span_days: Minimum date span in days for a patient to be considered valid.
            load_all: Load all data into processed_data for testing.
        """
        self.keep_columns = keep_columns
        self.cache_manager = get_cache_manager()
        self.dataset_config = get_dataset_config(self.dataset_name)
        self.use_cached = use_cached
        self.max_workers = max_workers
        self.patients_per_batch = patients_per_batch
        self.patients_per_file = patients_per_file
        self.number_of_patients_to_process = number_of_patients_to_process
        self.min_date_span_days = min_date_span_days
        self.load_all = load_all
        self.db_connection_string = (
            "postgresql://postgres:password@127.0.0.1:5432/gluroo_datasets"
        )

        self.processed_data: dict[str, pd.DataFrame] = {}
        self.train_data: dict[str, pd.DataFrame] = {}
        self.validation_data: dict[str, pd.DataFrame] | None = None
        self.total_skipped_patients: int = 0

        logger.info(f"Initializing GlurooDataLoader with use_cached={use_cached}")
        self.load_data()

    @property
    def dataset_name(self):
        return "gluroo"

    @property
    def description(self):
        return """
        Gluroo diabetes dataset loaded from TimescaleDB.
        Contains continuous glucose monitoring data, meal announcements, and insulin dosing
        information for multiple patients.
        This is for internal use only. The dataset will NOT be released to the public as they are collected within Gluroo and therefore confidential.
        """

    def load_data(self):
        """
        Load processed data from cache or process raw data from database.
        TODO: Need this clean up this function.

        If use_cached is True, loads previously processed data from Parquet cache.
        Otherwise, loads raw data from TimescaleDB, processes each patient,
        and saves processed data to cache as partitioned Parquet.
        Then splits the processed data into training and validation sets.

        Note: By default processed_data remains empty to avoid loading all data into memory.
        Use load_all=True to populate processed_data from cache (e.g. for testing).
        Use get_hf_streaming_dataset() for streaming access to the Parquet files.
        """
        need_to_process_data = True
        if self.use_cached:
            processed_path = self.cache_manager.get_absolute_path_by_type(
                self.dataset_name, "processed"
            )
            parquet_path = processed_path / "parquet"

            if parquet_path.exists() and any(parquet_path.glob("**/*.parquet")):
                need_to_process_data = False
                logger.info("Found cached Parquet data; skipping processing.")
                if self.load_all:
                    logging.warning(
                        "WARNING: Loading all data into processed_data for testing."
                    )
                    self._load_all_into_processed_data()
                    need_to_process_data = False
            else:
                logger.warning(
                    "use_cached=True but no Parquet found. Will process from source."
                )

        if need_to_process_data:
            logger.warning(
                "This is a very long running operation. Make sure to run it as a job."
            )
            self._process_and_cache_data(batch_size=self.patients_per_batch)
            logger.info("Data processing completed. Call the loader with use_cached=True to load the data into processed_data.")
            '''
            Ideally, we wouldn't want to process the data when trying to load the data for training.
            We would submit a job to get data to process then load the data later.
            So if the code reaches here, it means this is a batch job not training so probably doesn't need to load the data when done.
            '''
            if self.load_all:
                logging.warning(
                    "WARNING: Loading all data into processed_data for testing."
                )
                # Load the data again
                self._load_all_into_processed_data()

    def _load_all_into_processed_data(self) -> None:
        """
        Load all patients from Parquet cache into processed_data (for testing).
        Reads all *.parquet under the cache parquet path and groups by p_num.
        """
        processed_path = self.cache_manager.get_absolute_path_by_type(
            self.dataset_name, "processed"
        )
        parquet_path = processed_path / "parquet"
        if not parquet_path.exists():
            raise FileNotFoundError(f"Parquet path not found: {parquet_path}")

        parquet_files = sorted(parquet_path.glob("**/*.parquet"))
        if not parquet_files:
            logger.warning("No Parquet files found; processed_data remains empty.")
            return

        logger.info(
            f"Loading all patients from {len(parquet_files)} Parquet file(s) into processed_data..."
        )
        for path in parquet_files:
            df = pd.read_parquet(path)
            if ColumnNames.P_NUM.value not in df.columns:
                logger.warning(f"File {path} missing p_num column; skipping.")
                continue
            for p_num, group in df.groupby(ColumnNames.P_NUM.value, sort=False):
                if p_num not in self.processed_data:
                    self.processed_data[str(p_num)] = group.copy()
                else:
                    self.processed_data[str(p_num)] = pd.concat(
                        [self.processed_data[str(p_num)], group], ignore_index=True
                    )

        logger.info(f"Loaded {len(self.processed_data)} patients into processed_data.")

    def _get_all_patient_ids(self) -> list[tuple[str, str]]:
        """
        Get all unique patient IDs sorted by gid from the database (p_num will be ordered too like gluroo_0, gluroo_1, etc). A patient is a group.
        gids: base64 encoded id. This is the primary key of the groups table and referenced by the readings and messages tables.
        p_num (or patient_id): loader internal string identifier (e.g., gluroo_0) ordered by gid (generated by the script add_patient_id.sql).

        Returns:
            list[tuple[str, str]]: List of patient tuples (gid, p_num) sorted by gid (each gid is mapped to one string p_num)
        """
        engine = create_engine(self.db_connection_string)
        with engine.connect() as conn:
            result = conn.execute(
                # Order by gid to ensure deterministic ordering.
                # p_num is a string so order by that will give us something wrong like
                # gluroo_1, gluroo_10, gluroo_100, etc
                # p_num is produced by the order of gid so order by gid here is correct.
                text("SELECT DISTINCT gid, p_num FROM groups ORDER BY gid")
            )
            gids_p_nums = [(row[0], str(row[1])) for row in result.fetchall()]
        return gids_p_nums

    def load_raw(self, gids_p_nums: list[tuple[str, str]] | None = None) -> dict[str, pd.DataFrame]:
        """
        Load raw data from TimescaleDB for specified patients and filters out patients with less than min_date_span_days.

        Args:
            gids_p_nums: List of patient IDs to load. If None, loads all patients.

        Returns:
            A dictionary mapping patient IDs (p_num) to raw DataFrames (note that no longer gids)
        """
        engine = create_engine(self.db_connection_string)
        raw_data = {}
        if not gids_p_nums:
            logger.info("No gids_p_nums to load")
            return raw_data

        try:
            gids = [gid for gid, _p_num in gids_p_nums]
            all_df = self._load_all_patients_data_from_db(engine, gids)
            logger.info(f"Loaded {len(all_df)} rows from database.")
        except Exception as e:
            logger.warning(f"Error loading batch data from database: {e}")
            return raw_data

        if all_df.empty:
            self.total_skipped_patients += len(gids_p_nums)
            logger.info("No data found for any patient")
            return raw_data

        for gid, p_num in gids_p_nums:
            df = all_df.loc[all_df["gid"] == gid].copy()
            if not df.empty:
                date_span = df["date"].max() - df["date"].min()
                # date may be string, try to parse if needed (TODO: check if this is needed)
                if not pd.api.types.is_datetime64_any_dtype(df["date"]):
                    df["date"] = pd.to_datetime(df["date"], errors="coerce")
                    date_span = df["date"].max() - df["date"].min()
                if pd.isnull(date_span) or date_span.days < self.min_date_span_days:
                    self.total_skipped_patients += 1
                    continue

            # Remove the gids
            df = df.drop(columns=["gid"])
            if df.empty:
                self.total_skipped_patients += 1
                continue
            df[ColumnNames.P_NUM.value] = df["p_num"]
            raw_data[p_num] = df

        logger.info(f"Loaded raw data for {len(raw_data)} patients")
        return raw_data

    def _load_all_patients_data_from_db(
        self, engine, gids: list[str]
    ) -> pd.DataFrame:
        """
        Load raw data for multiple patients from TimescaleDB in a single query.

        Combines readings and messages for all given gids. Caller is responsible
        for splitting the result by gid and handling empty groups.

        Args:
            engine: SQLAlchemy engine
            patient_ids: List of patient group IDs (gids)

        Returns:
            pd.DataFrame: Combined raw data with a 'gid' column for splitting.
                Rows are ordered by date, then gid; callers may assume this order.
        """
        query = text("""
            SELECT
                r.gid,
                r.date,
                r.bgl,
                r.trend,
                r.source,
                NULL::TEXT as msg_type,
                NULL::FLOAT as food_g,
                NULL::FLOAT as dose_units,
                NULL::TEXT as dose_type,
                NULL::INT as exercise_mins,
                NULL::TEXT as exercise_level,
                g.p_num
            FROM readings r
            JOIN groups g ON r.gid = g.gid
            WHERE r.gid IN :gids
            UNION ALL
            SELECT
                m.gid,
                m.date,
                NULL::INT,
                NULL::TEXT as trend,
                NULL::TEXT as source,
                m.type as msg_type,
                m.food_g,
                m.dose_units,
                m.dose_type,
                m.exercise_mins,
                m.exercise_level,
                g.p_num
            FROM messages m
            JOIN groups g ON m.gid = g.gid
            WHERE m.gid IN :gids
            -- Order by date and gid to ensure deterministic ordering.
            ORDER BY date, gid
        """).bindparams(bindparam("gids", expanding=True))

        with engine.connect() as conn:
            return pd.read_sql(query, conn, params={"gids": gids})

    def _process_and_cache_data(self, batch_size: int = 10):
        """
        Process raw data and save to cache in batches.
        Progress is checkpointed so runs can be resumed. Each run processes the
        *next* number_of_patients_to_process patients (not a global cap).

        For each batch:
            1. Loads checkpoint (if any) to get next_batch_start and patients_processed_count; validates patients_per_file matches (error if mismatch),
            2. Loads raw data from database for the batch,
            3. Processes each batch in parallel with max_workers workers,
            4. Saves processed data to cache as partitioned Parquet,
            5. Saves checkpoint (next_batch_start, patients_processed_count, patients_per_file).
        This avoids loading all patients' raw data into memory at once.

        Args:
            batch_size: Number of patients to process in each batch. Default is 100. This can be tuned to optimize for memory usage and processing speed.
        """
        if self.number_of_patients_to_process % batch_size != 0:
            raise ValueError(
                f"number_of_patients_to_process {self.number_of_patients_to_process} must be a scalar multiple of batch_size {batch_size}"
            )
        # Get all patient IDs with p_nums first (sorted by gid)
        full_gids_p_nums: list[tuple[str, str]] = self._get_all_patient_ids()
        total_patients = len(full_gids_p_nums)
        logger.info(f"Processing {total_patients} patients in batches of {batch_size}")

        processed_path = self.cache_manager.get_absolute_path_by_type(
            self.dataset_name, "processed"
        )
        logger.info(f"Processed path: {processed_path}")
        processed_path.parent.mkdir(parents=True, exist_ok=True)
        processed_path.mkdir(parents=True, exist_ok=True)

        # Process patients in batches and save incrementally.
        # Stateless: each batch determines target file from global index, then read-if-exists → concat → write.
        parquet_path = processed_path / "parquet"
        parquet_path.mkdir(parents=True, exist_ok=True)

        # Load checkpoint to resume from previous run (next batch index). This run processes the next number_of_patients_to_process patients.
        checkpoint = self._load_checkpoint(processed_path)
        logger.info(f"Checkpoint: {checkpoint}")
        next_batch_start = checkpoint["next_batch_start"]
        total_processed_so_far = checkpoint["patients_processed_count"]

        # preserved across runs; set on first save
        first_run_date = checkpoint.get("first_run_date")
        saved_patients_per_file = checkpoint.get("patients_per_file")

        # This is pretty important because we want to ensure that the patients_per_file is consistent across runs.
        if saved_patients_per_file is not None and saved_patients_per_file != self.patients_per_file:
            raise ValueError(
                f"patients_per_file mismatch: checkpoint has {saved_patients_per_file}, "
                f"current loader has {self.patients_per_file}. "
                "patients_per_file must be constant across runs; use the same value or remove the checkpoint to start fresh."
            )
        
        # This is the n-th run (incremented each time _process_and_cache_data is invoked).
        current_run_number = checkpoint.get("run_number", 0) + 1
        run_start_date = datetime.now(timezone.utc).isoformat()
        logger.info(
            f"This is run {current_run_number} (started {run_start_date}); "
            f"will process up to {self.number_of_patients_to_process} patients."
        )

        # We will get whole number of batches to do this run due to the constraint
        batches_to_do_this_run = self.number_of_patients_to_process // batch_size
        patients_processed_this_run = 0
        if next_batch_start > 0 or total_processed_so_far > 0:
            logger.info(
                f"Resuming from checkpoint: next_batch_start={next_batch_start}, "
                f"total_processed_so_far={total_processed_so_far}; will process next {self.number_of_patients_to_process} patients ({batches_to_do_this_run} batches)."
            )

        for i in range(batches_to_do_this_run):
            batch_start = next_batch_start + i * batch_size
            if batch_start >= total_patients:
                break
            batch_end = min(batch_start + batch_size, total_patients)
            batch_gids_p_nums = full_gids_p_nums[batch_start:batch_end]
            batch_num = (batch_start // batch_size) + 1
            total_batches = (total_patients + batch_size - 1) // batch_size

            logger.info(
                f"Processing batch {batch_num}/{total_batches}: "
                f"patients {batch_start+1}-{batch_end} of {total_patients}"
            )

            ##### Load raw data for this batch (returns dict with p_num as keys) #####
            # From this point on, we no longer need to use gids as we are out of database phase.
            batch_raw_data = self.load_raw(batch_gids_p_nums)
            logger.info(f"Total skipped patients so far: {self.total_skipped_patients}")

            # p_num -> gid for error messages in _process_raw_data_batch
            gid_to_pnum_map = {p_num: gid for gid, p_num in batch_gids_p_nums}

            # Process this batch with parallel processing (uses p_num as keys)
            batch_processed_data = self._process_raw_data_batch(
                batch_raw_data, processed_path, gid_to_pnum_map
            )

            # Save this batch to Parquet
            logger.info(f"Saving batch {batch_num}/{total_batches} to Parquet...")
            self._save_batch_incremental(
                batch_processed_data,
                parquet_path,
                self.patients_per_file,
            )

            # Update checkpoint: next batch index, total processed, run number and date.
            patients_processed_this_run += len(batch_processed_data)
            total_processed_so_far += len(batch_processed_data)
            next_batch_start = batch_end

            # TODO: If any of the code above fails, we need to rollback the checkpoint? or need a way to know this batch failed and keep going?
            self._save_checkpoint(
                processed_path,
                next_batch_start,
                total_processed_so_far,
                self.patients_per_file,
                first_run_date=first_run_date,
                run_number=current_run_number,
                last_run_date=run_start_date,
            )
            if first_run_date is None:
                first_run_date = datetime.now(timezone.utc).isoformat()

            # Clear batch data from memory after saving
            del batch_raw_data

            # Clear batch processed data from memory
            del batch_processed_data

            logger.info(
                f"Completed processing and saving batch {batch_num}/{total_batches} "
                f"(this run: {patients_processed_this_run}, total: {total_processed_so_far})"
            )

    def _process_raw_data(self):
        """
        Required by DatasetBase. This is a WIP maybe?

        Gluroo processing is handled via `_process_and_cache_data()` and
        `_process_one_patient()` in a batched/streaming workflow.
        """
        raise NotImplementedError(
            "GlurooDataLoader does not implement a single in-memory _process_raw_data() step. "
            "Use `_process_and_cache_data()` to build Parquet cache and `get_hf_streaming_dataset()` to stream it."
        )

    def _process_raw_data_batch(
        self,
        raw_data: dict[str, pd.DataFrame],
        processed_path: Path,
        gid_to_pnum_map: dict[str, str],
    ) -> dict[str, pd.DataFrame]:
        """
        Process a batch of raw data for patients.

        Args:
            raw_data: Dictionary mapping p_num (string) to raw DataFrames
            processed_path: Path to processed data directory (not used here, but kept for compatibility)
            gid_to_pnum_map: Dictionary mapping p_num to gid for error messages

        Returns:
            dict[str, pd.DataFrame]: Dictionary mapping p_num to processed DataFrames
        """
        processed_data = {}

        # Prepare patient data tuples for parallel processing
        # Sort by p_num to ensure deterministic ordering (probably not needed because data should already be sorted by p_num)
        patient_tuples = sorted(
            [(p_num, df, processed_path) for p_num, df in raw_data.items()],
            key=lambda x: self._p_num_to_int(x[0]),  # Sort by numeric portion of p_num
        )

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks and track them by p_num
            future_to_pnum = {
                executor.submit(
                    process_single_patient,
                    patient_tuple,
                    # Data: (p_num, df, processed_path)
                    # Return: (p_num, df)
                ): patient_tuple[0]
                for patient_tuple in patient_tuples
            }

            # Collect results as they complete (order may vary)
            results = {}
            for index, future in enumerate(as_completed(future_to_pnum), 1):
                p_num = future_to_pnum[future]
                gid = gid_to_pnum_map.get(p_num, "unknown")
                progress = f"({index}/{len(raw_data)})"
                try:
                    result_pnum, df = future.result()
                    if df.empty:
                        logger.warning(
                            f"Processed data is empty for patient p_num={result_pnum} (gid={gid})"
                        )
                        continue
                    results[result_pnum] = df
                    logger.debug(
                        f"Successfully processed patient p_num={result_pnum} (gid={gid}) {progress}"
                    )
                except Exception as exc:
                    logger.error(
                        f"Patient p_num={p_num} (gid={gid}) generated an exception: {exc} {progress}"
                    )

            # Reconstruct processed_data in deterministic order (sorted by p_num)
            for p_num, _df, _processed_path in patient_tuples:
                if p_num in results:
                    processed_data[p_num] = results[p_num]

        return processed_data

    def _process_one_patient(
        self, df_raw: pd.DataFrame, p_num: str, gid: str = "unknown"
    ) -> pd.DataFrame | None:
        """
        Process raw data for a single patient.

        Args:
            df_raw: Raw DataFrame for the patient (must contain p_num column from database)
            p_num: Patient number (string identifier)
            gid: Patient group ID (used for error messages only)

        Returns:
            pd.DataFrame: Processed DataFrame, or None if processing fails
        """
        try:
            # Verify p_num matches DataFrame
            if "p_num" not in df_raw.columns:
                raise ValueError(
                    f"DataFrame missing p_num column for patient p_num={p_num} (gid={gid})"
                )
            df_p_num = str(df_raw["p_num"].iloc[0])
            if df_p_num != p_num:
                logger.warning(
                    f"p_num mismatch: expected {p_num}, got {df_p_num} for patient gid={gid}"
                )

            # Translate to standardized format
            df = data_translation(df_raw)

            # Apply preprocessing pipeline using p_num
            df = preprocessing_pipeline(str(p_num), df, use_aggregation=True)

            # df.to_csv(f"processed_{p_num}.csv")

            return df
        except Exception as e:
            logger.error(f"Error processing patient p_num={p_num} (gid={gid}): {e}")
            return None

    def _p_num_to_int(self, p_num: int | str) -> int:
        """
        Extract integer component from p_num string (e.g., gluroo_123 -> 123).
        Falls back to direct integer if already numeric.
        """
        if isinstance(p_num, int):
            return p_num

        match = re.search(r"(\d+)$", str(p_num))
        if match:
            return int(match.group(1))
        raise ValueError(f"Invalid p_num format: {p_num}")

    def _get_checkpoint_path(self, processed_path: Path) -> Path:
        """Path to the processing checkpoint file (resume state)."""
        return processed_path / PROCESSING_CHECKPOINT_FILENAME

    def _load_checkpoint(self, processed_path: Path) -> dict:
        """
        Load checkpoint for resumable processing.
        Returns dict with next_batch_start (int), patients_processed_count (int),
        patients_per_file (int | None), first_run_date (str | None), run_number (int),
        and last_run_date (str | None); None if missing (old checkpoint).
        """
        path = self._get_checkpoint_path(processed_path)
        if not path.exists():
            return {
                "next_batch_start": 0,
                "patients_processed_count": 0,
                "patients_per_file": None,
                "first_run_date": None,
                "run_number": 0,
                "last_run_date": None,
            }
        try:
            with open(path) as f:
                data = json.load(f)
            return {
                "next_batch_start": int(data.get("next_batch_start", 0)),
                "patients_processed_count": int(data.get("patients_processed_count", 0)),
                "patients_per_file": data.get("patients_per_file"),  # None if missing
                "first_run_date": data.get("first_run_date"),  # None if missing (old checkpoint)
                "run_number": int(data.get("run_number", 0)),
                "last_run_date": data.get("last_run_date"),  # None if missing (old checkpoint)
            }
        except (json.JSONDecodeError, OSError) as e:
            logger.warning(f"Could not load checkpoint from {path}: {e}; starting from 0.")
            return {
                "next_batch_start": 0,
                "patients_processed_count": 0,
                "patients_per_file": None,
                "first_run_date": None,
                "run_number": 0,
                "last_run_date": None,
            }

    def _save_checkpoint(
        self,
        processed_path: Path,
        next_batch_start: int,
        patients_processed_count: int,
        patients_per_file: int,
        first_run_date: str | None = None,
        run_number: int = 1,
        last_run_date: str | None = None,
    ) -> None:
        """Persist checkpoint after saving a batch. patients_per_file is stored to enforce consistency; first_run_date is set on first save and preserved thereafter. run_number and last_run_date identify this run."""
        if first_run_date is None:
            first_run_date = datetime.now(timezone.utc).isoformat()
        path = self._get_checkpoint_path(processed_path)
        try:
            with open(path, "w") as f:
                json.dump(
                    {
                        "next_batch_start": next_batch_start,
                        "patients_processed_count": patients_processed_count,
                        "patients_per_file": patients_per_file,
                        "first_run_date": first_run_date,
                        "run_number": run_number,
                        "last_run_date": last_run_date,
                    },
                    f,
                    indent=2,
                )
        except OSError as e:
            logger.warning(f"Could not save checkpoint to {path}: {e}")

    def _save_batch_incremental(
        self,
        batch_processed_data: dict[str, pd.DataFrame],
        parquet_path: Path,
        patients_per_file: int,
    ) -> None:
        """
        There are around 100k patients. If 100 per files, there will be 1000 files which is not too bad I guess
        We can probably do a 900 / 50 / 50 split for train/val/test or higher ratio even.

        Save a batch to Parquet in a stateless way: for each target file, read
        existing if present, concat with new data, write.

        File is determined by p_num: patients in the same numeric range go to the same file
        (e.g. gluroo_0..gluroo_399 -> file_000000, gluroo_400..gluroo_799 -> file_000001).
        Parquet does not support true append so we read existing file → concat → write.
        Peak memory per file write is one file's worth of data (at most
        patients_per_file patients) plus this batch's contribution.

        Args:
            batch_processed_data: p_num -> processed DataFrame for this batch
            parquet_path: Directory for file_000000.parquet, file_000001.parquet, ...
            patients_per_file: Number of patients per output file (partition size by p_num)
        """
        # Group this batch by target file: file_id = p_num numeric value // patients_per_file
        file_groups: dict[int, list[pd.DataFrame]] = {}
        for p_num, df in batch_processed_data.items():
            file_id = self._p_num_to_int(p_num) // patients_per_file
            df_with_id = df.copy()
            # We use datetime
            if "date" in df_with_id.columns:
                df_with_id = df_with_id.drop(columns=["date"])
            df_with_id[ColumnNames.P_NUM.value] = p_num
            df_with_id = df_with_id.reset_index()
            if file_id not in file_groups:
                file_groups[file_id] = []
            file_groups[file_id].append(df_with_id)

        for file_id in sorted(file_groups.keys()):
            new_df = pd.concat(file_groups[file_id], ignore_index=True)
            file_path = parquet_path / f"file_{file_id:06d}.parquet"
            if file_path.exists():
                # Read and append to existing parquet file
                # It is not ideal but probably fast enough
                existing_df = pd.read_parquet(file_path)
                combined = pd.concat([existing_df, new_df], ignore_index=True)
                del existing_df
            else:
                combined = new_df
            combined.to_parquet(
                file_path,
                engine="pyarrow",
                compression="snappy",
                index=False,
            )
            del combined
            del new_df
        del file_groups


    # WIP: Needs to get this work with huggingface Trainer and different kind of data processing module like ttm's.
    def get_hf_streaming_dataset(
        self,
        columns: list[str] | None = None,
        patient_ids: Iterable[str] | None = None,
        batch_size: int | None = None,
        validate_non_empty: bool = True,
    ):
        """
        Return a Hugging Face IterableDataset that streams local Parquet files with streaming=True.

        Args:
            columns: Optional subset of columns to load (passed to load_dataset).
            patient_ids: Optional iterable of p_num identifiers to filter.
            batch_size: Optional on-the-fly batching for downstream consumers.
            validate_non_empty: If True, peek one element to surface empty dataset errors early.

        Returns:
            datasets.iterable_dataset.IterableDataset configured for streaming.
        """
        processed_path = self.cache_manager.get_absolute_path_by_type(
            self.dataset_name, "processed"
        )
        parquet_path = processed_path / "parquet"
        if not parquet_path.exists():
            raise FileNotFoundError(f"Parquet files not found at {parquet_path}")

        data_files = str(parquet_path / "**" / "*.parquet")
        load_kwargs = {
            "path": "parquet",
            "data_files": data_files,
            "split": "train",
            "streaming": True,
        }
        if columns:
            load_kwargs["columns"] = columns

        dataset = load_dataset(**load_kwargs)

        if isinstance(dataset, dict):
            dataset = dataset["train"]

        if patient_ids:
            patient_ids_set = {str(pid) for pid in patient_ids}
            dataset = dataset.filter(
                lambda example: str(example.get(ColumnNames.P_NUM.value, ""))
                in patient_ids_set
            )

        if batch_size:
            dataset = dataset.batch(batch_size=batch_size)

        if validate_non_empty:
            try:
                # Peek to fail fast when filters/paths produce an empty stream.
                # Prepend the peeked element back so no data is lost.
                first_item = next(iter(dataset))
                dataset = IterableDataset.from_generator(
                    lambda: chain([first_item], dataset)
                )
            except StopIteration as e:
                raise ValueError(
                    f"Hugging Face streaming dataset is empty. "
                    f"Parquet path: {parquet_path}. "
                    f"Filters -> columns={columns}, p_num filter={patient_ids}."
                ) from e
            except Exception as e:  # pragma: no cover - defensive
                raise ValueError(
                    f"Failed to initialize streaming dataset from {parquet_path}: {e}"
                ) from e

        return dataset

# Standalone function for parallel processing
def process_single_patient(patient_tuple: tuple[str, pd.DataFrame, Path]) -> tuple:
    """
    Process a single patient file for parallel execution.

    Note: This function no longer saves individual CSV files.
    All patients are batched and saved as partitioned Parquet after processing.

    Args:
        patient_tuple: Tuple containing (p_num, df_raw, processed_path)

    Returns:
        tuple: (p_num, processed_df) - returns p_num (str) for mapping
    """
    p_num, df_raw, _processed_path = patient_tuple

    if df_raw.empty:
        # This shouldn't really happen. But just in case.
        logger.warning(f"Raw data is empty for patient p_num={p_num}. Skipping...")
        return (p_num, None)

    # Extract p_num from DataFrame to verify it matches
    if "p_num" not in df_raw.columns:
        logger.error(
            f"DataFrame missing p_num column for patient p_num={p_num}. Skipping..."
        )
        return (p_num, None)

    df_p_num = str(df_raw["p_num"].iloc[0])
    if df_p_num != p_num:
        logger.warning(f"p_num mismatch: expected {p_num}, got {df_p_num}")

    logger.info(f"Processing patient p_num={p_num}")

    # Process the patient
    try:
        df = data_translation(df_raw)
        df = preprocessing_pipeline(str(p_num), df, use_aggregation=True)
        # Debugging only: use p_num for filename (cleaner than gid)
        # df.to_csv(f"processed_{p_num}.csv", index=True)  # Debugging only

        if df.empty:
            raise ValueError(f"Processed data is empty for patient p_num={p_num}")

        logger.debug(f"Done processing patient p_num={p_num}")

        return (p_num, df)
    except Exception as e:
        logger.error(f"Error processing patient p_num={p_num}: {e}")
        raise
