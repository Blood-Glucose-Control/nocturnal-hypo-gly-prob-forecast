{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2782.905982905983,
  "eval_steps": 500,
  "global_step": 325600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.273504273504273,
      "grad_norm": 2.1290321350097656,
      "learning_rate": 4.995726495726496e-05,
      "loss": 5.1623,
      "step": 500
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 3.515603542327881,
      "learning_rate": 4.991452991452992e-05,
      "loss": 4.7907,
      "step": 1000
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 5.302112102508545,
      "learning_rate": 4.9871794871794874e-05,
      "loss": 4.6422,
      "step": 1500
    },
    {
      "epoch": 17.094017094017094,
      "grad_norm": 4.725725173950195,
      "learning_rate": 4.982905982905983e-05,
      "loss": 4.5607,
      "step": 2000
    },
    {
      "epoch": 21.367521367521366,
      "grad_norm": 5.004260063171387,
      "learning_rate": 4.978632478632479e-05,
      "loss": 4.5122,
      "step": 2500
    },
    {
      "epoch": 25.641025641025642,
      "grad_norm": 4.929769039154053,
      "learning_rate": 4.9743589743589746e-05,
      "loss": 4.4729,
      "step": 3000
    },
    {
      "epoch": 29.914529914529915,
      "grad_norm": 4.510822296142578,
      "learning_rate": 4.97008547008547e-05,
      "loss": 4.456,
      "step": 3500
    },
    {
      "epoch": 34.18803418803419,
      "grad_norm": 7.050263404846191,
      "learning_rate": 4.965811965811966e-05,
      "loss": 4.4189,
      "step": 4000
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 7.3173418045043945,
      "learning_rate": 4.961538461538462e-05,
      "loss": 4.3964,
      "step": 4500
    },
    {
      "epoch": 42.73504273504273,
      "grad_norm": 8.408453941345215,
      "learning_rate": 4.9572649572649575e-05,
      "loss": 4.3934,
      "step": 5000
    },
    {
      "epoch": 47.00854700854701,
      "grad_norm": 6.575353145599365,
      "learning_rate": 4.952991452991453e-05,
      "loss": 4.3655,
      "step": 5500
    },
    {
      "epoch": 51.282051282051285,
      "grad_norm": 5.145970344543457,
      "learning_rate": 4.948717948717949e-05,
      "loss": 4.3427,
      "step": 6000
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 5.29254150390625,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 4.3254,
      "step": 6500
    },
    {
      "epoch": 59.82905982905983,
      "grad_norm": 13.340841293334961,
      "learning_rate": 4.94017094017094e-05,
      "loss": 4.3108,
      "step": 7000
    },
    {
      "epoch": 64.1025641025641,
      "grad_norm": 7.039699554443359,
      "learning_rate": 4.935897435897436e-05,
      "loss": 4.3092,
      "step": 7500
    },
    {
      "epoch": 68.37606837606837,
      "grad_norm": 8.254450798034668,
      "learning_rate": 4.931623931623932e-05,
      "loss": 4.2853,
      "step": 8000
    },
    {
      "epoch": 72.64957264957265,
      "grad_norm": 10.41873836517334,
      "learning_rate": 4.927350427350428e-05,
      "loss": 4.2847,
      "step": 8500
    },
    {
      "epoch": 76.92307692307692,
      "grad_norm": 6.928525447845459,
      "learning_rate": 4.923076923076924e-05,
      "loss": 4.2722,
      "step": 9000
    },
    {
      "epoch": 81.19658119658119,
      "grad_norm": 7.024401664733887,
      "learning_rate": 4.918803418803419e-05,
      "loss": 4.2597,
      "step": 9500
    },
    {
      "epoch": 85.47008547008546,
      "grad_norm": 7.552577018737793,
      "learning_rate": 4.9145299145299147e-05,
      "loss": 4.2508,
      "step": 10000
    },
    {
      "epoch": 89.74358974358974,
      "grad_norm": 8.843289375305176,
      "learning_rate": 4.9102564102564104e-05,
      "loss": 4.2525,
      "step": 10500
    },
    {
      "epoch": 94.01709401709402,
      "grad_norm": 7.617059230804443,
      "learning_rate": 4.905982905982906e-05,
      "loss": 4.235,
      "step": 11000
    },
    {
      "epoch": 98.2905982905983,
      "grad_norm": 5.512274265289307,
      "learning_rate": 4.901709401709402e-05,
      "loss": 4.2337,
      "step": 11500
    },
    {
      "epoch": 102.56410256410257,
      "grad_norm": 7.132622241973877,
      "learning_rate": 4.8974358974358975e-05,
      "loss": 4.2151,
      "step": 12000
    },
    {
      "epoch": 106.83760683760684,
      "grad_norm": 6.0566534996032715,
      "learning_rate": 4.893162393162393e-05,
      "loss": 4.2214,
      "step": 12500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 12.124021530151367,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.2068,
      "step": 13000
    },
    {
      "epoch": 115.38461538461539,
      "grad_norm": 11.0662202835083,
      "learning_rate": 4.884615384615385e-05,
      "loss": 4.2076,
      "step": 13500
    },
    {
      "epoch": 119.65811965811966,
      "grad_norm": 7.243983745574951,
      "learning_rate": 4.8803418803418804e-05,
      "loss": 4.1974,
      "step": 14000
    },
    {
      "epoch": 123.93162393162393,
      "grad_norm": 7.888822555541992,
      "learning_rate": 4.876068376068376e-05,
      "loss": 4.1813,
      "step": 14500
    },
    {
      "epoch": 128.2051282051282,
      "grad_norm": 7.011981010437012,
      "learning_rate": 4.871794871794872e-05,
      "loss": 4.1851,
      "step": 15000
    },
    {
      "epoch": 132.47863247863248,
      "grad_norm": 6.351913928985596,
      "learning_rate": 4.8675213675213676e-05,
      "loss": 4.1697,
      "step": 15500
    },
    {
      "epoch": 136.75213675213675,
      "grad_norm": 7.328882694244385,
      "learning_rate": 4.863247863247863e-05,
      "loss": 4.1742,
      "step": 16000
    },
    {
      "epoch": 141.02564102564102,
      "grad_norm": 6.786298751831055,
      "learning_rate": 4.858974358974359e-05,
      "loss": 4.1609,
      "step": 16500
    },
    {
      "epoch": 145.2991452991453,
      "grad_norm": 6.474168300628662,
      "learning_rate": 4.854700854700855e-05,
      "loss": 4.1605,
      "step": 17000
    },
    {
      "epoch": 149.57264957264957,
      "grad_norm": 8.576292037963867,
      "learning_rate": 4.8504273504273505e-05,
      "loss": 4.1724,
      "step": 17500
    },
    {
      "epoch": 153.84615384615384,
      "grad_norm": 7.031434059143066,
      "learning_rate": 4.846153846153846e-05,
      "loss": 4.1451,
      "step": 18000
    },
    {
      "epoch": 158.1196581196581,
      "grad_norm": 6.402857780456543,
      "learning_rate": 4.841880341880342e-05,
      "loss": 4.1451,
      "step": 18500
    },
    {
      "epoch": 162.39316239316238,
      "grad_norm": 9.402010917663574,
      "learning_rate": 4.8376068376068376e-05,
      "loss": 4.1437,
      "step": 19000
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 6.044010639190674,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 4.1404,
      "step": 19500
    },
    {
      "epoch": 170.94017094017093,
      "grad_norm": 6.289105415344238,
      "learning_rate": 4.829059829059829e-05,
      "loss": 4.1316,
      "step": 20000
    },
    {
      "epoch": 175.2136752136752,
      "grad_norm": 8.11441421508789,
      "learning_rate": 4.824786324786325e-05,
      "loss": 4.1322,
      "step": 20500
    },
    {
      "epoch": 179.48717948717947,
      "grad_norm": 6.081551551818848,
      "learning_rate": 4.8205128205128205e-05,
      "loss": 4.1295,
      "step": 21000
    },
    {
      "epoch": 183.76068376068375,
      "grad_norm": 11.564292907714844,
      "learning_rate": 4.816239316239316e-05,
      "loss": 4.1294,
      "step": 21500
    },
    {
      "epoch": 188.03418803418805,
      "grad_norm": 6.247604846954346,
      "learning_rate": 4.8119658119658126e-05,
      "loss": 4.109,
      "step": 22000
    },
    {
      "epoch": 192.30769230769232,
      "grad_norm": 6.9817376136779785,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 4.127,
      "step": 22500
    },
    {
      "epoch": 196.5811965811966,
      "grad_norm": 8.155413627624512,
      "learning_rate": 4.803418803418804e-05,
      "loss": 4.1126,
      "step": 23000
    },
    {
      "epoch": 200.85470085470087,
      "grad_norm": 6.863190650939941,
      "learning_rate": 4.7991452991453e-05,
      "loss": 4.1093,
      "step": 23500
    },
    {
      "epoch": 205.12820512820514,
      "grad_norm": 11.453500747680664,
      "learning_rate": 4.7948717948717955e-05,
      "loss": 4.1132,
      "step": 24000
    },
    {
      "epoch": 209.4017094017094,
      "grad_norm": 6.0719475746154785,
      "learning_rate": 4.790598290598291e-05,
      "loss": 4.0981,
      "step": 24500
    },
    {
      "epoch": 213.67521367521368,
      "grad_norm": 7.080769062042236,
      "learning_rate": 4.786324786324787e-05,
      "loss": 4.1045,
      "step": 25000
    },
    {
      "epoch": 217.94871794871796,
      "grad_norm": 7.436689376831055,
      "learning_rate": 4.782051282051283e-05,
      "loss": 4.0937,
      "step": 25500
    },
    {
      "epoch": 222.22222222222223,
      "grad_norm": 5.551352024078369,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 4.0836,
      "step": 26000
    },
    {
      "epoch": 226.4957264957265,
      "grad_norm": 9.403289794921875,
      "learning_rate": 4.773504273504274e-05,
      "loss": 4.0886,
      "step": 26500
    },
    {
      "epoch": 230.76923076923077,
      "grad_norm": 6.150911808013916,
      "learning_rate": 4.76923076923077e-05,
      "loss": 4.0906,
      "step": 27000
    },
    {
      "epoch": 235.04273504273505,
      "grad_norm": 4.876595497131348,
      "learning_rate": 4.764957264957265e-05,
      "loss": 4.0751,
      "step": 27500
    },
    {
      "epoch": 239.31623931623932,
      "grad_norm": 8.254741668701172,
      "learning_rate": 4.7606837606837606e-05,
      "loss": 4.0774,
      "step": 28000
    },
    {
      "epoch": 243.5897435897436,
      "grad_norm": 7.1539177894592285,
      "learning_rate": 4.7564102564102563e-05,
      "loss": 4.0839,
      "step": 28500
    },
    {
      "epoch": 247.86324786324786,
      "grad_norm": 5.6175537109375,
      "learning_rate": 4.752136752136752e-05,
      "loss": 4.0715,
      "step": 29000
    },
    {
      "epoch": 252.13675213675214,
      "grad_norm": 5.4050822257995605,
      "learning_rate": 4.747863247863248e-05,
      "loss": 4.0777,
      "step": 29500
    },
    {
      "epoch": 256.4102564102564,
      "grad_norm": 9.442435264587402,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 4.0747,
      "step": 30000
    },
    {
      "epoch": 260.6837606837607,
      "grad_norm": 8.601881980895996,
      "learning_rate": 4.739316239316239e-05,
      "loss": 4.0625,
      "step": 30500
    },
    {
      "epoch": 264.95726495726495,
      "grad_norm": 7.6944756507873535,
      "learning_rate": 4.735042735042735e-05,
      "loss": 4.0687,
      "step": 31000
    },
    {
      "epoch": 269.2307692307692,
      "grad_norm": 6.2381978034973145,
      "learning_rate": 4.730769230769231e-05,
      "loss": 4.0706,
      "step": 31500
    },
    {
      "epoch": 273.5042735042735,
      "grad_norm": 8.458966255187988,
      "learning_rate": 4.7264957264957264e-05,
      "loss": 4.0671,
      "step": 32000
    },
    {
      "epoch": 277.77777777777777,
      "grad_norm": 6.759262561798096,
      "learning_rate": 4.722222222222222e-05,
      "loss": 4.0666,
      "step": 32500
    },
    {
      "epoch": 282.05128205128204,
      "grad_norm": 6.305822849273682,
      "learning_rate": 4.717948717948718e-05,
      "loss": 4.057,
      "step": 33000
    },
    {
      "epoch": 286.3247863247863,
      "grad_norm": 7.897851943969727,
      "learning_rate": 4.7136752136752136e-05,
      "loss": 4.0568,
      "step": 33500
    },
    {
      "epoch": 290.5982905982906,
      "grad_norm": 6.220949649810791,
      "learning_rate": 4.709401709401709e-05,
      "loss": 4.0512,
      "step": 34000
    },
    {
      "epoch": 294.87179487179486,
      "grad_norm": 6.10500431060791,
      "learning_rate": 4.705128205128205e-05,
      "loss": 4.0621,
      "step": 34500
    },
    {
      "epoch": 299.14529914529913,
      "grad_norm": 5.766659259796143,
      "learning_rate": 4.700854700854701e-05,
      "loss": 4.0478,
      "step": 35000
    },
    {
      "epoch": 303.4188034188034,
      "grad_norm": 6.696651935577393,
      "learning_rate": 4.6965811965811964e-05,
      "loss": 4.0495,
      "step": 35500
    },
    {
      "epoch": 307.6923076923077,
      "grad_norm": 5.952033519744873,
      "learning_rate": 4.692307692307693e-05,
      "loss": 4.0417,
      "step": 36000
    },
    {
      "epoch": 311.96581196581195,
      "grad_norm": 6.345422267913818,
      "learning_rate": 4.6880341880341886e-05,
      "loss": 4.0546,
      "step": 36500
    },
    {
      "epoch": 316.2393162393162,
      "grad_norm": 7.947479248046875,
      "learning_rate": 4.683760683760684e-05,
      "loss": 4.0455,
      "step": 37000
    },
    {
      "epoch": 320.5128205128205,
      "grad_norm": 8.901809692382812,
      "learning_rate": 4.67948717948718e-05,
      "loss": 4.0476,
      "step": 37500
    },
    {
      "epoch": 324.78632478632477,
      "grad_norm": 7.532938480377197,
      "learning_rate": 4.675213675213676e-05,
      "loss": 4.0307,
      "step": 38000
    },
    {
      "epoch": 329.05982905982904,
      "grad_norm": 16.986122131347656,
      "learning_rate": 4.6709401709401714e-05,
      "loss": 4.0339,
      "step": 38500
    },
    {
      "epoch": 333.3333333333333,
      "grad_norm": 5.276901721954346,
      "learning_rate": 4.666666666666667e-05,
      "loss": 4.0361,
      "step": 39000
    },
    {
      "epoch": 337.6068376068376,
      "grad_norm": 7.788668155670166,
      "learning_rate": 4.662393162393163e-05,
      "loss": 4.0336,
      "step": 39500
    },
    {
      "epoch": 341.88034188034186,
      "grad_norm": 6.863504886627197,
      "learning_rate": 4.6581196581196586e-05,
      "loss": 4.0262,
      "step": 40000
    },
    {
      "epoch": 346.15384615384613,
      "grad_norm": 7.218404769897461,
      "learning_rate": 4.653846153846154e-05,
      "loss": 4.0359,
      "step": 40500
    },
    {
      "epoch": 350.4273504273504,
      "grad_norm": 6.467072010040283,
      "learning_rate": 4.64957264957265e-05,
      "loss": 4.0221,
      "step": 41000
    },
    {
      "epoch": 354.7008547008547,
      "grad_norm": 8.331160545349121,
      "learning_rate": 4.645299145299146e-05,
      "loss": 4.0312,
      "step": 41500
    },
    {
      "epoch": 358.97435897435895,
      "grad_norm": 6.1337971687316895,
      "learning_rate": 4.6410256410256415e-05,
      "loss": 4.0274,
      "step": 42000
    },
    {
      "epoch": 363.2478632478632,
      "grad_norm": 5.521655082702637,
      "learning_rate": 4.636752136752137e-05,
      "loss": 4.0245,
      "step": 42500
    },
    {
      "epoch": 367.5213675213675,
      "grad_norm": 6.372340679168701,
      "learning_rate": 4.632478632478633e-05,
      "loss": 4.0253,
      "step": 43000
    },
    {
      "epoch": 371.79487179487177,
      "grad_norm": 7.783965587615967,
      "learning_rate": 4.6282051282051287e-05,
      "loss": 4.026,
      "step": 43500
    },
    {
      "epoch": 376.0683760683761,
      "grad_norm": 6.777711868286133,
      "learning_rate": 4.6239316239316244e-05,
      "loss": 4.0222,
      "step": 44000
    },
    {
      "epoch": 380.34188034188037,
      "grad_norm": 7.369783401489258,
      "learning_rate": 4.61965811965812e-05,
      "loss": 4.0202,
      "step": 44500
    },
    {
      "epoch": 384.61538461538464,
      "grad_norm": 7.54788875579834,
      "learning_rate": 4.615384615384616e-05,
      "loss": 4.0173,
      "step": 45000
    },
    {
      "epoch": 388.8888888888889,
      "grad_norm": 9.773552894592285,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 4.025,
      "step": 45500
    },
    {
      "epoch": 393.1623931623932,
      "grad_norm": 12.026010513305664,
      "learning_rate": 4.6068376068376066e-05,
      "loss": 4.0097,
      "step": 46000
    },
    {
      "epoch": 397.43589743589746,
      "grad_norm": 6.430022716522217,
      "learning_rate": 4.602564102564102e-05,
      "loss": 4.0132,
      "step": 46500
    },
    {
      "epoch": 401.70940170940173,
      "grad_norm": 7.782326698303223,
      "learning_rate": 4.598290598290598e-05,
      "loss": 4.0038,
      "step": 47000
    },
    {
      "epoch": 405.982905982906,
      "grad_norm": 6.288475036621094,
      "learning_rate": 4.594017094017094e-05,
      "loss": 4.0144,
      "step": 47500
    },
    {
      "epoch": 410.2564102564103,
      "grad_norm": 8.162703514099121,
      "learning_rate": 4.5897435897435895e-05,
      "loss": 4.0118,
      "step": 48000
    },
    {
      "epoch": 414.52991452991455,
      "grad_norm": 7.063260555267334,
      "learning_rate": 4.585470085470085e-05,
      "loss": 4.0035,
      "step": 48500
    },
    {
      "epoch": 418.8034188034188,
      "grad_norm": 7.308281421661377,
      "learning_rate": 4.581196581196581e-05,
      "loss": 4.0019,
      "step": 49000
    },
    {
      "epoch": 423.0769230769231,
      "grad_norm": 7.16190767288208,
      "learning_rate": 4.576923076923077e-05,
      "loss": 4.0122,
      "step": 49500
    },
    {
      "epoch": 427.35042735042737,
      "grad_norm": 6.485702991485596,
      "learning_rate": 4.572649572649573e-05,
      "loss": 4.0015,
      "step": 50000
    },
    {
      "epoch": 431.62393162393164,
      "grad_norm": 7.548284530639648,
      "learning_rate": 4.568376068376069e-05,
      "loss": 3.9942,
      "step": 50500
    },
    {
      "epoch": 435.8974358974359,
      "grad_norm": 7.711718559265137,
      "learning_rate": 4.5641025641025645e-05,
      "loss": 3.9966,
      "step": 51000
    },
    {
      "epoch": 440.1709401709402,
      "grad_norm": 8.694419860839844,
      "learning_rate": 4.55982905982906e-05,
      "loss": 4.0033,
      "step": 51500
    },
    {
      "epoch": 444.44444444444446,
      "grad_norm": 9.156994819641113,
      "learning_rate": 4.555555555555556e-05,
      "loss": 3.9963,
      "step": 52000
    },
    {
      "epoch": 448.71794871794873,
      "grad_norm": 6.439909934997559,
      "learning_rate": 4.5512820512820516e-05,
      "loss": 4.0051,
      "step": 52500
    },
    {
      "epoch": 452.991452991453,
      "grad_norm": 6.06203031539917,
      "learning_rate": 4.5470085470085474e-05,
      "loss": 3.9946,
      "step": 53000
    },
    {
      "epoch": 457.2649572649573,
      "grad_norm": 6.1918253898620605,
      "learning_rate": 4.542735042735043e-05,
      "loss": 3.9983,
      "step": 53500
    },
    {
      "epoch": 461.53846153846155,
      "grad_norm": 7.51957368850708,
      "learning_rate": 4.538461538461539e-05,
      "loss": 3.9958,
      "step": 54000
    },
    {
      "epoch": 465.8119658119658,
      "grad_norm": 6.095141887664795,
      "learning_rate": 4.5341880341880345e-05,
      "loss": 3.9925,
      "step": 54500
    },
    {
      "epoch": 470.0854700854701,
      "grad_norm": 8.821395874023438,
      "learning_rate": 4.52991452991453e-05,
      "loss": 4.0011,
      "step": 55000
    },
    {
      "epoch": 474.35897435897436,
      "grad_norm": 8.34216594696045,
      "learning_rate": 4.525641025641026e-05,
      "loss": 3.9966,
      "step": 55500
    },
    {
      "epoch": 478.63247863247864,
      "grad_norm": 6.039234638214111,
      "learning_rate": 4.521367521367522e-05,
      "loss": 3.9946,
      "step": 56000
    },
    {
      "epoch": 482.9059829059829,
      "grad_norm": 6.080371379852295,
      "learning_rate": 4.5170940170940174e-05,
      "loss": 3.995,
      "step": 56500
    },
    {
      "epoch": 487.1794871794872,
      "grad_norm": 8.233863830566406,
      "learning_rate": 4.512820512820513e-05,
      "loss": 3.9836,
      "step": 57000
    },
    {
      "epoch": 491.45299145299145,
      "grad_norm": 7.048008441925049,
      "learning_rate": 4.508547008547009e-05,
      "loss": 3.9871,
      "step": 57500
    },
    {
      "epoch": 495.7264957264957,
      "grad_norm": 5.639457702636719,
      "learning_rate": 4.5042735042735046e-05,
      "loss": 3.9761,
      "step": 58000
    },
    {
      "epoch": 500.0,
      "grad_norm": 9.386116981506348,
      "learning_rate": 4.5e-05,
      "loss": 3.9891,
      "step": 58500
    },
    {
      "epoch": 504.2735042735043,
      "grad_norm": 8.15343189239502,
      "learning_rate": 4.495726495726496e-05,
      "loss": 3.9897,
      "step": 59000
    },
    {
      "epoch": 508.54700854700855,
      "grad_norm": 9.558808326721191,
      "learning_rate": 4.491452991452992e-05,
      "loss": 3.9873,
      "step": 59500
    },
    {
      "epoch": 512.8205128205128,
      "grad_norm": 10.98904037475586,
      "learning_rate": 4.4871794871794874e-05,
      "loss": 3.9831,
      "step": 60000
    },
    {
      "epoch": 517.0940170940171,
      "grad_norm": 7.508718490600586,
      "learning_rate": 4.482905982905983e-05,
      "loss": 3.9728,
      "step": 60500
    },
    {
      "epoch": 521.3675213675214,
      "grad_norm": 8.752313613891602,
      "learning_rate": 4.478632478632479e-05,
      "loss": 3.9839,
      "step": 61000
    },
    {
      "epoch": 525.6410256410256,
      "grad_norm": 6.115170478820801,
      "learning_rate": 4.4743589743589746e-05,
      "loss": 3.9807,
      "step": 61500
    },
    {
      "epoch": 529.9145299145299,
      "grad_norm": 6.219410419464111,
      "learning_rate": 4.47008547008547e-05,
      "loss": 3.9856,
      "step": 62000
    },
    {
      "epoch": 534.1880341880342,
      "grad_norm": 6.511816501617432,
      "learning_rate": 4.465811965811966e-05,
      "loss": 3.9728,
      "step": 62500
    },
    {
      "epoch": 538.4615384615385,
      "grad_norm": 10.061970710754395,
      "learning_rate": 4.461538461538462e-05,
      "loss": 3.9772,
      "step": 63000
    },
    {
      "epoch": 542.7350427350427,
      "grad_norm": 10.08215045928955,
      "learning_rate": 4.4572649572649575e-05,
      "loss": 3.9691,
      "step": 63500
    },
    {
      "epoch": 547.008547008547,
      "grad_norm": 6.054327964782715,
      "learning_rate": 4.452991452991453e-05,
      "loss": 3.9782,
      "step": 64000
    },
    {
      "epoch": 551.2820512820513,
      "grad_norm": 9.000885963439941,
      "learning_rate": 4.448717948717949e-05,
      "loss": 3.9757,
      "step": 64500
    },
    {
      "epoch": 555.5555555555555,
      "grad_norm": 6.924674034118652,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 3.9634,
      "step": 65000
    },
    {
      "epoch": 559.8290598290598,
      "grad_norm": 7.483743190765381,
      "learning_rate": 4.4401709401709404e-05,
      "loss": 3.9728,
      "step": 65500
    },
    {
      "epoch": 564.1025641025641,
      "grad_norm": 5.2609052658081055,
      "learning_rate": 4.435897435897436e-05,
      "loss": 3.9697,
      "step": 66000
    },
    {
      "epoch": 568.3760683760684,
      "grad_norm": 6.079471588134766,
      "learning_rate": 4.431623931623932e-05,
      "loss": 3.9639,
      "step": 66500
    },
    {
      "epoch": 572.6495726495726,
      "grad_norm": 6.238564491271973,
      "learning_rate": 4.4273504273504275e-05,
      "loss": 3.9693,
      "step": 67000
    },
    {
      "epoch": 576.9230769230769,
      "grad_norm": 6.779789447784424,
      "learning_rate": 4.423076923076923e-05,
      "loss": 3.967,
      "step": 67500
    },
    {
      "epoch": 581.1965811965812,
      "grad_norm": 8.94941520690918,
      "learning_rate": 4.418803418803419e-05,
      "loss": 3.9738,
      "step": 68000
    },
    {
      "epoch": 585.4700854700855,
      "grad_norm": 6.445152282714844,
      "learning_rate": 4.414529914529915e-05,
      "loss": 3.9677,
      "step": 68500
    },
    {
      "epoch": 589.7435897435897,
      "grad_norm": 9.103602409362793,
      "learning_rate": 4.4102564102564104e-05,
      "loss": 3.9722,
      "step": 69000
    },
    {
      "epoch": 594.017094017094,
      "grad_norm": 6.753680229187012,
      "learning_rate": 4.405982905982906e-05,
      "loss": 3.9565,
      "step": 69500
    },
    {
      "epoch": 598.2905982905983,
      "grad_norm": 6.498974800109863,
      "learning_rate": 4.401709401709402e-05,
      "loss": 3.9653,
      "step": 70000
    },
    {
      "epoch": 602.5641025641025,
      "grad_norm": 6.302903652191162,
      "learning_rate": 4.3974358974358976e-05,
      "loss": 3.9772,
      "step": 70500
    },
    {
      "epoch": 606.8376068376068,
      "grad_norm": 8.50960922241211,
      "learning_rate": 4.393162393162393e-05,
      "loss": 3.969,
      "step": 71000
    },
    {
      "epoch": 611.1111111111111,
      "grad_norm": 11.961792945861816,
      "learning_rate": 4.388888888888889e-05,
      "loss": 3.9646,
      "step": 71500
    },
    {
      "epoch": 615.3846153846154,
      "grad_norm": 7.538358211517334,
      "learning_rate": 4.384615384615385e-05,
      "loss": 3.9644,
      "step": 72000
    },
    {
      "epoch": 619.6581196581196,
      "grad_norm": 6.098117351531982,
      "learning_rate": 4.3803418803418805e-05,
      "loss": 3.9676,
      "step": 72500
    },
    {
      "epoch": 623.9316239316239,
      "grad_norm": 6.95728063583374,
      "learning_rate": 4.376068376068376e-05,
      "loss": 3.9575,
      "step": 73000
    },
    {
      "epoch": 628.2051282051282,
      "grad_norm": 7.169442653656006,
      "learning_rate": 4.371794871794872e-05,
      "loss": 3.965,
      "step": 73500
    },
    {
      "epoch": 632.4786324786324,
      "grad_norm": 7.487086772918701,
      "learning_rate": 4.3675213675213676e-05,
      "loss": 3.9607,
      "step": 74000
    },
    {
      "epoch": 636.7521367521367,
      "grad_norm": 8.206954002380371,
      "learning_rate": 4.3632478632478634e-05,
      "loss": 3.9575,
      "step": 74500
    },
    {
      "epoch": 641.025641025641,
      "grad_norm": 8.36374568939209,
      "learning_rate": 4.358974358974359e-05,
      "loss": 3.9674,
      "step": 75000
    },
    {
      "epoch": 645.2991452991453,
      "grad_norm": 8.200075149536133,
      "learning_rate": 4.354700854700855e-05,
      "loss": 3.9613,
      "step": 75500
    },
    {
      "epoch": 649.5726495726495,
      "grad_norm": 7.155755519866943,
      "learning_rate": 4.3504273504273505e-05,
      "loss": 3.9597,
      "step": 76000
    },
    {
      "epoch": 653.8461538461538,
      "grad_norm": 8.682790756225586,
      "learning_rate": 4.346153846153846e-05,
      "loss": 3.9581,
      "step": 76500
    },
    {
      "epoch": 658.1196581196581,
      "grad_norm": 10.735983848571777,
      "learning_rate": 4.341880341880342e-05,
      "loss": 3.9679,
      "step": 77000
    },
    {
      "epoch": 662.3931623931624,
      "grad_norm": 7.385895252227783,
      "learning_rate": 4.337606837606838e-05,
      "loss": 3.9498,
      "step": 77500
    },
    {
      "epoch": 666.6666666666666,
      "grad_norm": 5.647291660308838,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 3.9591,
      "step": 78000
    },
    {
      "epoch": 670.9401709401709,
      "grad_norm": 5.892566680908203,
      "learning_rate": 4.329059829059829e-05,
      "loss": 3.9534,
      "step": 78500
    },
    {
      "epoch": 675.2136752136752,
      "grad_norm": 6.375795841217041,
      "learning_rate": 4.324786324786325e-05,
      "loss": 3.9556,
      "step": 79000
    },
    {
      "epoch": 679.4871794871794,
      "grad_norm": 7.242469787597656,
      "learning_rate": 4.320512820512821e-05,
      "loss": 3.9609,
      "step": 79500
    },
    {
      "epoch": 683.7606837606837,
      "grad_norm": 10.83326244354248,
      "learning_rate": 4.316239316239317e-05,
      "loss": 3.9667,
      "step": 80000
    },
    {
      "epoch": 688.034188034188,
      "grad_norm": 8.191494941711426,
      "learning_rate": 4.311965811965813e-05,
      "loss": 3.9398,
      "step": 80500
    },
    {
      "epoch": 692.3076923076923,
      "grad_norm": 8.680912971496582,
      "learning_rate": 4.3076923076923084e-05,
      "loss": 3.9568,
      "step": 81000
    },
    {
      "epoch": 696.5811965811965,
      "grad_norm": 8.823606491088867,
      "learning_rate": 4.303418803418804e-05,
      "loss": 3.9554,
      "step": 81500
    },
    {
      "epoch": 700.8547008547008,
      "grad_norm": 7.222215175628662,
      "learning_rate": 4.2991452991453e-05,
      "loss": 3.9546,
      "step": 82000
    },
    {
      "epoch": 705.1282051282051,
      "grad_norm": 5.584237098693848,
      "learning_rate": 4.294871794871795e-05,
      "loss": 3.9513,
      "step": 82500
    },
    {
      "epoch": 709.4017094017094,
      "grad_norm": 7.259988784790039,
      "learning_rate": 4.2905982905982906e-05,
      "loss": 3.938,
      "step": 83000
    },
    {
      "epoch": 713.6752136752136,
      "grad_norm": 5.676225662231445,
      "learning_rate": 4.286324786324786e-05,
      "loss": 3.9578,
      "step": 83500
    },
    {
      "epoch": 717.9487179487179,
      "grad_norm": 7.094985485076904,
      "learning_rate": 4.282051282051282e-05,
      "loss": 3.9449,
      "step": 84000
    },
    {
      "epoch": 722.2222222222222,
      "grad_norm": 10.262001991271973,
      "learning_rate": 4.277777777777778e-05,
      "loss": 3.9453,
      "step": 84500
    },
    {
      "epoch": 726.4957264957264,
      "grad_norm": 5.974347114562988,
      "learning_rate": 4.2735042735042735e-05,
      "loss": 3.9486,
      "step": 85000
    },
    {
      "epoch": 730.7692307692307,
      "grad_norm": 9.638751983642578,
      "learning_rate": 4.269230769230769e-05,
      "loss": 3.9465,
      "step": 85500
    },
    {
      "epoch": 735.042735042735,
      "grad_norm": 6.363375663757324,
      "learning_rate": 4.264957264957265e-05,
      "loss": 3.9397,
      "step": 86000
    },
    {
      "epoch": 739.3162393162393,
      "grad_norm": 7.853148937225342,
      "learning_rate": 4.260683760683761e-05,
      "loss": 3.9521,
      "step": 86500
    },
    {
      "epoch": 743.5897435897435,
      "grad_norm": 6.903509140014648,
      "learning_rate": 4.2564102564102564e-05,
      "loss": 3.9447,
      "step": 87000
    },
    {
      "epoch": 747.8632478632478,
      "grad_norm": 7.012662887573242,
      "learning_rate": 4.252136752136752e-05,
      "loss": 3.9438,
      "step": 87500
    },
    {
      "epoch": 752.1367521367522,
      "grad_norm": 6.141195297241211,
      "learning_rate": 4.247863247863248e-05,
      "loss": 3.9458,
      "step": 88000
    },
    {
      "epoch": 756.4102564102565,
      "grad_norm": 6.218745231628418,
      "learning_rate": 4.2435897435897435e-05,
      "loss": 3.9381,
      "step": 88500
    },
    {
      "epoch": 760.6837606837607,
      "grad_norm": 5.906060218811035,
      "learning_rate": 4.239316239316239e-05,
      "loss": 3.949,
      "step": 89000
    },
    {
      "epoch": 764.957264957265,
      "grad_norm": 5.518407821655273,
      "learning_rate": 4.235042735042735e-05,
      "loss": 3.9394,
      "step": 89500
    },
    {
      "epoch": 769.2307692307693,
      "grad_norm": 6.059779644012451,
      "learning_rate": 4.230769230769231e-05,
      "loss": 3.9358,
      "step": 90000
    },
    {
      "epoch": 773.5042735042736,
      "grad_norm": 9.136564254760742,
      "learning_rate": 4.2264957264957264e-05,
      "loss": 3.9389,
      "step": 90500
    },
    {
      "epoch": 777.7777777777778,
      "grad_norm": 9.33652400970459,
      "learning_rate": 4.222222222222222e-05,
      "loss": 3.9354,
      "step": 91000
    },
    {
      "epoch": 782.0512820512821,
      "grad_norm": 8.501041412353516,
      "learning_rate": 4.217948717948718e-05,
      "loss": 3.9363,
      "step": 91500
    },
    {
      "epoch": 786.3247863247864,
      "grad_norm": 4.7778191566467285,
      "learning_rate": 4.2136752136752136e-05,
      "loss": 3.9381,
      "step": 92000
    },
    {
      "epoch": 790.5982905982906,
      "grad_norm": 6.496784210205078,
      "learning_rate": 4.209401709401709e-05,
      "loss": 3.9323,
      "step": 92500
    },
    {
      "epoch": 794.8717948717949,
      "grad_norm": 7.8700408935546875,
      "learning_rate": 4.205128205128206e-05,
      "loss": 3.9355,
      "step": 93000
    },
    {
      "epoch": 799.1452991452992,
      "grad_norm": 6.194124698638916,
      "learning_rate": 4.2008547008547014e-05,
      "loss": 3.9306,
      "step": 93500
    },
    {
      "epoch": 803.4188034188035,
      "grad_norm": 6.3413615226745605,
      "learning_rate": 4.196581196581197e-05,
      "loss": 3.9412,
      "step": 94000
    },
    {
      "epoch": 807.6923076923077,
      "grad_norm": 6.7048563957214355,
      "learning_rate": 4.192307692307693e-05,
      "loss": 3.9396,
      "step": 94500
    },
    {
      "epoch": 811.965811965812,
      "grad_norm": 6.426498889923096,
      "learning_rate": 4.1880341880341886e-05,
      "loss": 3.9315,
      "step": 95000
    },
    {
      "epoch": 816.2393162393163,
      "grad_norm": 5.19972562789917,
      "learning_rate": 4.183760683760684e-05,
      "loss": 3.9389,
      "step": 95500
    },
    {
      "epoch": 820.5128205128206,
      "grad_norm": 7.129374980926514,
      "learning_rate": 4.17948717948718e-05,
      "loss": 3.9383,
      "step": 96000
    },
    {
      "epoch": 824.7863247863248,
      "grad_norm": 6.120849132537842,
      "learning_rate": 4.175213675213676e-05,
      "loss": 3.9424,
      "step": 96500
    },
    {
      "epoch": 829.0598290598291,
      "grad_norm": 6.235012054443359,
      "learning_rate": 4.1709401709401715e-05,
      "loss": 3.9305,
      "step": 97000
    },
    {
      "epoch": 833.3333333333334,
      "grad_norm": 9.285923957824707,
      "learning_rate": 4.166666666666667e-05,
      "loss": 3.9338,
      "step": 97500
    },
    {
      "epoch": 837.6068376068376,
      "grad_norm": 6.395662307739258,
      "learning_rate": 4.162393162393163e-05,
      "loss": 3.9276,
      "step": 98000
    },
    {
      "epoch": 841.8803418803419,
      "grad_norm": 5.993895053863525,
      "learning_rate": 4.1581196581196586e-05,
      "loss": 3.9429,
      "step": 98500
    },
    {
      "epoch": 846.1538461538462,
      "grad_norm": 7.645120143890381,
      "learning_rate": 4.1538461538461544e-05,
      "loss": 3.9358,
      "step": 99000
    },
    {
      "epoch": 850.4273504273505,
      "grad_norm": 6.279493808746338,
      "learning_rate": 4.14957264957265e-05,
      "loss": 3.9238,
      "step": 99500
    },
    {
      "epoch": 854.7008547008547,
      "grad_norm": 6.811514854431152,
      "learning_rate": 4.145299145299146e-05,
      "loss": 3.9283,
      "step": 100000
    },
    {
      "epoch": 858.974358974359,
      "grad_norm": 7.746912002563477,
      "learning_rate": 4.1410256410256415e-05,
      "loss": 3.9287,
      "step": 100500
    },
    {
      "epoch": 863.2478632478633,
      "grad_norm": 6.898199081420898,
      "learning_rate": 4.1367521367521366e-05,
      "loss": 3.9296,
      "step": 101000
    },
    {
      "epoch": 867.5213675213676,
      "grad_norm": 6.560009479522705,
      "learning_rate": 4.132478632478632e-05,
      "loss": 3.919,
      "step": 101500
    },
    {
      "epoch": 871.7948717948718,
      "grad_norm": 6.428352355957031,
      "learning_rate": 4.128205128205128e-05,
      "loss": 3.9189,
      "step": 102000
    },
    {
      "epoch": 876.0683760683761,
      "grad_norm": 8.228758811950684,
      "learning_rate": 4.123931623931624e-05,
      "loss": 3.9226,
      "step": 102500
    },
    {
      "epoch": 880.3418803418804,
      "grad_norm": 7.808410167694092,
      "learning_rate": 4.1196581196581195e-05,
      "loss": 3.9372,
      "step": 103000
    },
    {
      "epoch": 884.6153846153846,
      "grad_norm": 8.02485466003418,
      "learning_rate": 4.115384615384615e-05,
      "loss": 3.9231,
      "step": 103500
    },
    {
      "epoch": 888.8888888888889,
      "grad_norm": 7.0687761306762695,
      "learning_rate": 4.111111111111111e-05,
      "loss": 3.9294,
      "step": 104000
    },
    {
      "epoch": 893.1623931623932,
      "grad_norm": 6.921252727508545,
      "learning_rate": 4.1068376068376066e-05,
      "loss": 3.9366,
      "step": 104500
    },
    {
      "epoch": 897.4358974358975,
      "grad_norm": 6.633902549743652,
      "learning_rate": 4.1025641025641023e-05,
      "loss": 3.9292,
      "step": 105000
    },
    {
      "epoch": 901.7094017094017,
      "grad_norm": 5.694268226623535,
      "learning_rate": 4.098290598290598e-05,
      "loss": 3.9244,
      "step": 105500
    },
    {
      "epoch": 905.982905982906,
      "grad_norm": 10.882898330688477,
      "learning_rate": 4.094017094017094e-05,
      "loss": 3.9203,
      "step": 106000
    },
    {
      "epoch": 910.2564102564103,
      "grad_norm": 7.610199928283691,
      "learning_rate": 4.0897435897435895e-05,
      "loss": 3.9291,
      "step": 106500
    },
    {
      "epoch": 914.5299145299145,
      "grad_norm": 5.436959266662598,
      "learning_rate": 4.085470085470086e-05,
      "loss": 3.9221,
      "step": 107000
    },
    {
      "epoch": 918.8034188034188,
      "grad_norm": 7.352177143096924,
      "learning_rate": 4.0811965811965816e-05,
      "loss": 3.9291,
      "step": 107500
    },
    {
      "epoch": 923.0769230769231,
      "grad_norm": 8.29963207244873,
      "learning_rate": 4.0769230769230773e-05,
      "loss": 3.9245,
      "step": 108000
    },
    {
      "epoch": 927.3504273504274,
      "grad_norm": 6.81034517288208,
      "learning_rate": 4.072649572649573e-05,
      "loss": 3.9245,
      "step": 108500
    },
    {
      "epoch": 931.6239316239316,
      "grad_norm": 7.053950309753418,
      "learning_rate": 4.068376068376069e-05,
      "loss": 3.9278,
      "step": 109000
    },
    {
      "epoch": 935.8974358974359,
      "grad_norm": 9.440195083618164,
      "learning_rate": 4.0641025641025645e-05,
      "loss": 3.9248,
      "step": 109500
    },
    {
      "epoch": 940.1709401709402,
      "grad_norm": 5.570390224456787,
      "learning_rate": 4.05982905982906e-05,
      "loss": 3.9158,
      "step": 110000
    },
    {
      "epoch": 944.4444444444445,
      "grad_norm": 7.22826623916626,
      "learning_rate": 4.055555555555556e-05,
      "loss": 3.9263,
      "step": 110500
    },
    {
      "epoch": 948.7179487179487,
      "grad_norm": 4.857020854949951,
      "learning_rate": 4.051282051282052e-05,
      "loss": 3.9174,
      "step": 111000
    },
    {
      "epoch": 952.991452991453,
      "grad_norm": 5.927299976348877,
      "learning_rate": 4.0470085470085474e-05,
      "loss": 3.9181,
      "step": 111500
    },
    {
      "epoch": 957.2649572649573,
      "grad_norm": 7.000720977783203,
      "learning_rate": 4.042735042735043e-05,
      "loss": 3.9163,
      "step": 112000
    },
    {
      "epoch": 961.5384615384615,
      "grad_norm": 8.2637357711792,
      "learning_rate": 4.038461538461539e-05,
      "loss": 3.9199,
      "step": 112500
    },
    {
      "epoch": 965.8119658119658,
      "grad_norm": 8.145536422729492,
      "learning_rate": 4.0341880341880346e-05,
      "loss": 3.9121,
      "step": 113000
    },
    {
      "epoch": 970.0854700854701,
      "grad_norm": 6.233683109283447,
      "learning_rate": 4.02991452991453e-05,
      "loss": 3.9259,
      "step": 113500
    },
    {
      "epoch": 974.3589743589744,
      "grad_norm": 8.074170112609863,
      "learning_rate": 4.025641025641026e-05,
      "loss": 3.9245,
      "step": 114000
    },
    {
      "epoch": 978.6324786324786,
      "grad_norm": 5.164620399475098,
      "learning_rate": 4.021367521367522e-05,
      "loss": 3.913,
      "step": 114500
    },
    {
      "epoch": 982.9059829059829,
      "grad_norm": 5.799078941345215,
      "learning_rate": 4.0170940170940174e-05,
      "loss": 3.9199,
      "step": 115000
    },
    {
      "epoch": 987.1794871794872,
      "grad_norm": 7.7591094970703125,
      "learning_rate": 4.012820512820513e-05,
      "loss": 3.92,
      "step": 115500
    },
    {
      "epoch": 991.4529914529915,
      "grad_norm": 10.754416465759277,
      "learning_rate": 4.008547008547009e-05,
      "loss": 3.9146,
      "step": 116000
    },
    {
      "epoch": 995.7264957264957,
      "grad_norm": 7.450811862945557,
      "learning_rate": 4.0042735042735046e-05,
      "loss": 3.9072,
      "step": 116500
    },
    {
      "epoch": 1000.0,
      "grad_norm": 7.534578323364258,
      "learning_rate": 4e-05,
      "loss": 3.9179,
      "step": 117000
    },
    {
      "epoch": 1004.2735042735043,
      "grad_norm": 16.66469955444336,
      "learning_rate": 3.995726495726496e-05,
      "loss": 3.9141,
      "step": 117500
    },
    {
      "epoch": 1008.5470085470085,
      "grad_norm": 7.125945568084717,
      "learning_rate": 3.991452991452992e-05,
      "loss": 3.9186,
      "step": 118000
    },
    {
      "epoch": 1012.8205128205128,
      "grad_norm": 7.861736297607422,
      "learning_rate": 3.9871794871794875e-05,
      "loss": 3.9229,
      "step": 118500
    },
    {
      "epoch": 1017.0940170940171,
      "grad_norm": 5.922479629516602,
      "learning_rate": 3.9829059829059825e-05,
      "loss": 3.9227,
      "step": 119000
    },
    {
      "epoch": 1021.3675213675214,
      "grad_norm": 6.924526214599609,
      "learning_rate": 3.978632478632478e-05,
      "loss": 3.9177,
      "step": 119500
    },
    {
      "epoch": 1025.6410256410256,
      "grad_norm": 5.243896484375,
      "learning_rate": 3.974358974358974e-05,
      "loss": 3.9148,
      "step": 120000
    },
    {
      "epoch": 1029.91452991453,
      "grad_norm": 10.86159610748291,
      "learning_rate": 3.9700854700854704e-05,
      "loss": 3.9138,
      "step": 120500
    },
    {
      "epoch": 1034.1880341880342,
      "grad_norm": 7.111548900604248,
      "learning_rate": 3.965811965811966e-05,
      "loss": 3.9144,
      "step": 121000
    },
    {
      "epoch": 1038.4615384615386,
      "grad_norm": 6.929369926452637,
      "learning_rate": 3.961538461538462e-05,
      "loss": 3.9139,
      "step": 121500
    },
    {
      "epoch": 1042.7350427350427,
      "grad_norm": 6.078151226043701,
      "learning_rate": 3.9572649572649575e-05,
      "loss": 3.9295,
      "step": 122000
    },
    {
      "epoch": 1047.008547008547,
      "grad_norm": 7.018303394317627,
      "learning_rate": 3.952991452991453e-05,
      "loss": 3.9102,
      "step": 122500
    },
    {
      "epoch": 1051.2820512820513,
      "grad_norm": 4.660222053527832,
      "learning_rate": 3.948717948717949e-05,
      "loss": 3.9117,
      "step": 123000
    },
    {
      "epoch": 1055.5555555555557,
      "grad_norm": 5.0254340171813965,
      "learning_rate": 3.944444444444445e-05,
      "loss": 3.9162,
      "step": 123500
    },
    {
      "epoch": 1059.8290598290598,
      "grad_norm": 6.977751731872559,
      "learning_rate": 3.9401709401709404e-05,
      "loss": 3.9245,
      "step": 124000
    },
    {
      "epoch": 1064.1025641025642,
      "grad_norm": 7.983242034912109,
      "learning_rate": 3.935897435897436e-05,
      "loss": 3.9167,
      "step": 124500
    },
    {
      "epoch": 1068.3760683760684,
      "grad_norm": 5.584043502807617,
      "learning_rate": 3.931623931623932e-05,
      "loss": 3.9082,
      "step": 125000
    },
    {
      "epoch": 1072.6495726495727,
      "grad_norm": 6.646012783050537,
      "learning_rate": 3.9273504273504276e-05,
      "loss": 3.9206,
      "step": 125500
    },
    {
      "epoch": 1076.923076923077,
      "grad_norm": 6.884326457977295,
      "learning_rate": 3.923076923076923e-05,
      "loss": 3.9088,
      "step": 126000
    },
    {
      "epoch": 1081.1965811965813,
      "grad_norm": 6.15372896194458,
      "learning_rate": 3.918803418803419e-05,
      "loss": 3.9167,
      "step": 126500
    },
    {
      "epoch": 1085.4700854700855,
      "grad_norm": 20.107873916625977,
      "learning_rate": 3.914529914529915e-05,
      "loss": 3.9011,
      "step": 127000
    },
    {
      "epoch": 1089.7435897435898,
      "grad_norm": 5.489475727081299,
      "learning_rate": 3.9102564102564105e-05,
      "loss": 3.9093,
      "step": 127500
    },
    {
      "epoch": 1094.017094017094,
      "grad_norm": 6.812025547027588,
      "learning_rate": 3.905982905982906e-05,
      "loss": 3.9111,
      "step": 128000
    },
    {
      "epoch": 1098.2905982905984,
      "grad_norm": 6.0143818855285645,
      "learning_rate": 3.901709401709402e-05,
      "loss": 3.9121,
      "step": 128500
    },
    {
      "epoch": 1102.5641025641025,
      "grad_norm": 5.944926738739014,
      "learning_rate": 3.8974358974358976e-05,
      "loss": 3.9079,
      "step": 129000
    },
    {
      "epoch": 1106.837606837607,
      "grad_norm": 10.64007568359375,
      "learning_rate": 3.8931623931623934e-05,
      "loss": 3.9069,
      "step": 129500
    },
    {
      "epoch": 1111.111111111111,
      "grad_norm": 5.982423305511475,
      "learning_rate": 3.888888888888889e-05,
      "loss": 3.9038,
      "step": 130000
    },
    {
      "epoch": 1115.3846153846155,
      "grad_norm": 6.333535194396973,
      "learning_rate": 3.884615384615385e-05,
      "loss": 3.9193,
      "step": 130500
    },
    {
      "epoch": 1119.6581196581196,
      "grad_norm": 6.67844295501709,
      "learning_rate": 3.8803418803418805e-05,
      "loss": 3.9087,
      "step": 131000
    },
    {
      "epoch": 1123.931623931624,
      "grad_norm": 7.960541725158691,
      "learning_rate": 3.876068376068376e-05,
      "loss": 3.9105,
      "step": 131500
    },
    {
      "epoch": 1128.2051282051282,
      "grad_norm": 5.816122055053711,
      "learning_rate": 3.871794871794872e-05,
      "loss": 3.9058,
      "step": 132000
    },
    {
      "epoch": 1132.4786324786326,
      "grad_norm": 9.33767318725586,
      "learning_rate": 3.867521367521368e-05,
      "loss": 3.9209,
      "step": 132500
    },
    {
      "epoch": 1136.7521367521367,
      "grad_norm": 6.790988922119141,
      "learning_rate": 3.8632478632478634e-05,
      "loss": 3.8998,
      "step": 133000
    },
    {
      "epoch": 1141.025641025641,
      "grad_norm": 5.650710105895996,
      "learning_rate": 3.858974358974359e-05,
      "loss": 3.9121,
      "step": 133500
    },
    {
      "epoch": 1145.2991452991453,
      "grad_norm": 6.9959306716918945,
      "learning_rate": 3.854700854700855e-05,
      "loss": 3.9022,
      "step": 134000
    },
    {
      "epoch": 1149.5726495726497,
      "grad_norm": 5.570718288421631,
      "learning_rate": 3.8504273504273506e-05,
      "loss": 3.9066,
      "step": 134500
    },
    {
      "epoch": 1153.8461538461538,
      "grad_norm": 6.652150630950928,
      "learning_rate": 3.846153846153846e-05,
      "loss": 3.9089,
      "step": 135000
    },
    {
      "epoch": 1158.1196581196582,
      "grad_norm": 5.155848979949951,
      "learning_rate": 3.841880341880342e-05,
      "loss": 3.8993,
      "step": 135500
    },
    {
      "epoch": 1162.3931623931624,
      "grad_norm": 9.795127868652344,
      "learning_rate": 3.837606837606838e-05,
      "loss": 3.9111,
      "step": 136000
    },
    {
      "epoch": 1166.6666666666667,
      "grad_norm": 9.439269065856934,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 3.8982,
      "step": 136500
    },
    {
      "epoch": 1170.940170940171,
      "grad_norm": 9.015193939208984,
      "learning_rate": 3.82905982905983e-05,
      "loss": 3.9115,
      "step": 137000
    },
    {
      "epoch": 1175.2136752136753,
      "grad_norm": 6.734692573547363,
      "learning_rate": 3.824786324786325e-05,
      "loss": 3.9039,
      "step": 137500
    },
    {
      "epoch": 1179.4871794871794,
      "grad_norm": 7.4832892417907715,
      "learning_rate": 3.8205128205128206e-05,
      "loss": 3.9109,
      "step": 138000
    },
    {
      "epoch": 1183.7606837606838,
      "grad_norm": 8.61874008178711,
      "learning_rate": 3.816239316239316e-05,
      "loss": 3.9076,
      "step": 138500
    },
    {
      "epoch": 1188.034188034188,
      "grad_norm": 7.006282329559326,
      "learning_rate": 3.811965811965812e-05,
      "loss": 3.9124,
      "step": 139000
    },
    {
      "epoch": 1192.3076923076924,
      "grad_norm": 6.468670845031738,
      "learning_rate": 3.807692307692308e-05,
      "loss": 3.8984,
      "step": 139500
    },
    {
      "epoch": 1196.5811965811965,
      "grad_norm": 7.2872796058654785,
      "learning_rate": 3.8034188034188035e-05,
      "loss": 3.902,
      "step": 140000
    },
    {
      "epoch": 1200.854700854701,
      "grad_norm": 8.148943901062012,
      "learning_rate": 3.799145299145299e-05,
      "loss": 3.9057,
      "step": 140500
    },
    {
      "epoch": 1205.128205128205,
      "grad_norm": 5.326008319854736,
      "learning_rate": 3.794871794871795e-05,
      "loss": 3.9044,
      "step": 141000
    },
    {
      "epoch": 1209.4017094017095,
      "grad_norm": 8.578001976013184,
      "learning_rate": 3.7905982905982907e-05,
      "loss": 3.9028,
      "step": 141500
    },
    {
      "epoch": 1213.6752136752136,
      "grad_norm": 9.740901947021484,
      "learning_rate": 3.7863247863247864e-05,
      "loss": 3.898,
      "step": 142000
    },
    {
      "epoch": 1217.948717948718,
      "grad_norm": 7.2452850341796875,
      "learning_rate": 3.782051282051282e-05,
      "loss": 3.9036,
      "step": 142500
    },
    {
      "epoch": 1222.2222222222222,
      "grad_norm": 6.877384185791016,
      "learning_rate": 3.777777777777778e-05,
      "loss": 3.8933,
      "step": 143000
    },
    {
      "epoch": 1226.4957264957266,
      "grad_norm": 12.606060028076172,
      "learning_rate": 3.7735042735042735e-05,
      "loss": 3.9141,
      "step": 143500
    },
    {
      "epoch": 1230.7692307692307,
      "grad_norm": 7.142357349395752,
      "learning_rate": 3.769230769230769e-05,
      "loss": 3.8981,
      "step": 144000
    },
    {
      "epoch": 1235.042735042735,
      "grad_norm": 5.758839130401611,
      "learning_rate": 3.764957264957265e-05,
      "loss": 3.8986,
      "step": 144500
    },
    {
      "epoch": 1239.3162393162393,
      "grad_norm": 6.188665866851807,
      "learning_rate": 3.760683760683761e-05,
      "loss": 3.9017,
      "step": 145000
    },
    {
      "epoch": 1243.5897435897436,
      "grad_norm": 6.873031139373779,
      "learning_rate": 3.7564102564102564e-05,
      "loss": 3.9071,
      "step": 145500
    },
    {
      "epoch": 1247.8632478632478,
      "grad_norm": 6.409467697143555,
      "learning_rate": 3.752136752136752e-05,
      "loss": 3.9073,
      "step": 146000
    },
    {
      "epoch": 1252.1367521367522,
      "grad_norm": 9.185282707214355,
      "learning_rate": 3.747863247863248e-05,
      "loss": 3.9019,
      "step": 146500
    },
    {
      "epoch": 1256.4102564102564,
      "grad_norm": 5.982016563415527,
      "learning_rate": 3.7435897435897436e-05,
      "loss": 3.8894,
      "step": 147000
    },
    {
      "epoch": 1260.6837606837607,
      "grad_norm": 7.775376796722412,
      "learning_rate": 3.739316239316239e-05,
      "loss": 3.9023,
      "step": 147500
    },
    {
      "epoch": 1264.957264957265,
      "grad_norm": 8.477492332458496,
      "learning_rate": 3.735042735042735e-05,
      "loss": 3.9033,
      "step": 148000
    },
    {
      "epoch": 1269.2307692307693,
      "grad_norm": 5.378364562988281,
      "learning_rate": 3.730769230769231e-05,
      "loss": 3.9032,
      "step": 148500
    },
    {
      "epoch": 1273.5042735042734,
      "grad_norm": 6.654353618621826,
      "learning_rate": 3.7264957264957265e-05,
      "loss": 3.9083,
      "step": 149000
    },
    {
      "epoch": 1277.7777777777778,
      "grad_norm": 5.791096210479736,
      "learning_rate": 3.722222222222222e-05,
      "loss": 3.8961,
      "step": 149500
    },
    {
      "epoch": 1282.051282051282,
      "grad_norm": 6.453015327453613,
      "learning_rate": 3.717948717948718e-05,
      "loss": 3.9031,
      "step": 150000
    },
    {
      "epoch": 1286.3247863247864,
      "grad_norm": 6.5045013427734375,
      "learning_rate": 3.713675213675214e-05,
      "loss": 3.9043,
      "step": 150500
    },
    {
      "epoch": 1290.5982905982905,
      "grad_norm": 8.266364097595215,
      "learning_rate": 3.70940170940171e-05,
      "loss": 3.9019,
      "step": 151000
    },
    {
      "epoch": 1294.871794871795,
      "grad_norm": 5.924482345581055,
      "learning_rate": 3.705128205128206e-05,
      "loss": 3.8839,
      "step": 151500
    },
    {
      "epoch": 1299.145299145299,
      "grad_norm": 6.050508975982666,
      "learning_rate": 3.7008547008547015e-05,
      "loss": 3.8984,
      "step": 152000
    },
    {
      "epoch": 1303.4188034188035,
      "grad_norm": 9.699873924255371,
      "learning_rate": 3.696581196581197e-05,
      "loss": 3.895,
      "step": 152500
    },
    {
      "epoch": 1307.6923076923076,
      "grad_norm": 6.622016906738281,
      "learning_rate": 3.692307692307693e-05,
      "loss": 3.9053,
      "step": 153000
    },
    {
      "epoch": 1311.965811965812,
      "grad_norm": 8.406899452209473,
      "learning_rate": 3.6880341880341886e-05,
      "loss": 3.8967,
      "step": 153500
    },
    {
      "epoch": 1316.2393162393162,
      "grad_norm": 8.868720054626465,
      "learning_rate": 3.6837606837606844e-05,
      "loss": 3.9027,
      "step": 154000
    },
    {
      "epoch": 1320.5128205128206,
      "grad_norm": 7.1497955322265625,
      "learning_rate": 3.67948717948718e-05,
      "loss": 3.885,
      "step": 154500
    },
    {
      "epoch": 1324.7863247863247,
      "grad_norm": 7.68508768081665,
      "learning_rate": 3.675213675213676e-05,
      "loss": 3.9107,
      "step": 155000
    },
    {
      "epoch": 1329.059829059829,
      "grad_norm": 7.473965167999268,
      "learning_rate": 3.670940170940171e-05,
      "loss": 3.8875,
      "step": 155500
    },
    {
      "epoch": 1333.3333333333333,
      "grad_norm": 5.836130142211914,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 3.8967,
      "step": 156000
    },
    {
      "epoch": 1337.6068376068376,
      "grad_norm": 7.7762274742126465,
      "learning_rate": 3.662393162393162e-05,
      "loss": 3.8976,
      "step": 156500
    },
    {
      "epoch": 1341.8803418803418,
      "grad_norm": 7.14823579788208,
      "learning_rate": 3.658119658119658e-05,
      "loss": 3.8922,
      "step": 157000
    },
    {
      "epoch": 1346.1538461538462,
      "grad_norm": 8.294669151306152,
      "learning_rate": 3.653846153846154e-05,
      "loss": 3.8921,
      "step": 157500
    },
    {
      "epoch": 1350.4273504273503,
      "grad_norm": 7.286296367645264,
      "learning_rate": 3.6495726495726495e-05,
      "loss": 3.8865,
      "step": 158000
    },
    {
      "epoch": 1354.7008547008547,
      "grad_norm": 7.252664089202881,
      "learning_rate": 3.645299145299145e-05,
      "loss": 3.8992,
      "step": 158500
    },
    {
      "epoch": 1358.974358974359,
      "grad_norm": 6.8899688720703125,
      "learning_rate": 3.641025641025641e-05,
      "loss": 3.9107,
      "step": 159000
    },
    {
      "epoch": 1363.2478632478633,
      "grad_norm": 6.62986421585083,
      "learning_rate": 3.6367521367521366e-05,
      "loss": 3.8898,
      "step": 159500
    },
    {
      "epoch": 1367.5213675213674,
      "grad_norm": 11.313315391540527,
      "learning_rate": 3.6324786324786323e-05,
      "loss": 3.8953,
      "step": 160000
    },
    {
      "epoch": 1371.7948717948718,
      "grad_norm": 6.891268253326416,
      "learning_rate": 3.628205128205128e-05,
      "loss": 3.9002,
      "step": 160500
    },
    {
      "epoch": 1376.068376068376,
      "grad_norm": 8.90712833404541,
      "learning_rate": 3.623931623931624e-05,
      "loss": 3.8959,
      "step": 161000
    },
    {
      "epoch": 1380.3418803418804,
      "grad_norm": 8.561731338500977,
      "learning_rate": 3.6196581196581195e-05,
      "loss": 3.8855,
      "step": 161500
    },
    {
      "epoch": 1384.6153846153845,
      "grad_norm": 5.722553730010986,
      "learning_rate": 3.615384615384615e-05,
      "loss": 3.8923,
      "step": 162000
    },
    {
      "epoch": 1388.888888888889,
      "grad_norm": 9.234941482543945,
      "learning_rate": 3.611111111111111e-05,
      "loss": 3.8919,
      "step": 162500
    },
    {
      "epoch": 1393.162393162393,
      "grad_norm": 6.336771488189697,
      "learning_rate": 3.606837606837607e-05,
      "loss": 3.8855,
      "step": 163000
    },
    {
      "epoch": 1397.4358974358975,
      "grad_norm": 5.59092378616333,
      "learning_rate": 3.6025641025641024e-05,
      "loss": 3.8899,
      "step": 163500
    },
    {
      "epoch": 1401.7094017094016,
      "grad_norm": 5.962862014770508,
      "learning_rate": 3.598290598290598e-05,
      "loss": 3.8888,
      "step": 164000
    },
    {
      "epoch": 1405.982905982906,
      "grad_norm": 8.078895568847656,
      "learning_rate": 3.5940170940170945e-05,
      "loss": 3.8941,
      "step": 164500
    },
    {
      "epoch": 1410.2564102564102,
      "grad_norm": 9.226444244384766,
      "learning_rate": 3.58974358974359e-05,
      "loss": 3.8951,
      "step": 165000
    },
    {
      "epoch": 1414.5299145299145,
      "grad_norm": 8.431792259216309,
      "learning_rate": 3.585470085470086e-05,
      "loss": 3.8959,
      "step": 165500
    },
    {
      "epoch": 1418.8034188034187,
      "grad_norm": 6.839963436126709,
      "learning_rate": 3.581196581196582e-05,
      "loss": 3.8908,
      "step": 166000
    },
    {
      "epoch": 1423.076923076923,
      "grad_norm": 5.44870138168335,
      "learning_rate": 3.5769230769230774e-05,
      "loss": 3.8854,
      "step": 166500
    },
    {
      "epoch": 1427.3504273504273,
      "grad_norm": 9.201324462890625,
      "learning_rate": 3.572649572649573e-05,
      "loss": 3.8941,
      "step": 167000
    },
    {
      "epoch": 1431.6239316239316,
      "grad_norm": 6.444371223449707,
      "learning_rate": 3.568376068376069e-05,
      "loss": 3.8781,
      "step": 167500
    },
    {
      "epoch": 1435.8974358974358,
      "grad_norm": 9.065454483032227,
      "learning_rate": 3.5641025641025646e-05,
      "loss": 3.9017,
      "step": 168000
    },
    {
      "epoch": 1440.1709401709402,
      "grad_norm": 7.653104305267334,
      "learning_rate": 3.55982905982906e-05,
      "loss": 3.884,
      "step": 168500
    },
    {
      "epoch": 1444.4444444444443,
      "grad_norm": 8.872462272644043,
      "learning_rate": 3.555555555555556e-05,
      "loss": 3.9052,
      "step": 169000
    },
    {
      "epoch": 1448.7179487179487,
      "grad_norm": 7.8770036697387695,
      "learning_rate": 3.551282051282052e-05,
      "loss": 3.8911,
      "step": 169500
    },
    {
      "epoch": 1452.991452991453,
      "grad_norm": 7.5474348068237305,
      "learning_rate": 3.5470085470085474e-05,
      "loss": 3.8874,
      "step": 170000
    },
    {
      "epoch": 1457.2649572649573,
      "grad_norm": 8.946928024291992,
      "learning_rate": 3.542735042735043e-05,
      "loss": 3.8872,
      "step": 170500
    },
    {
      "epoch": 1461.5384615384614,
      "grad_norm": 6.898177146911621,
      "learning_rate": 3.538461538461539e-05,
      "loss": 3.8968,
      "step": 171000
    },
    {
      "epoch": 1465.8119658119658,
      "grad_norm": 26.157161712646484,
      "learning_rate": 3.5341880341880346e-05,
      "loss": 3.883,
      "step": 171500
    },
    {
      "epoch": 1470.08547008547,
      "grad_norm": 5.657871246337891,
      "learning_rate": 3.52991452991453e-05,
      "loss": 3.8925,
      "step": 172000
    },
    {
      "epoch": 1474.3589743589744,
      "grad_norm": 7.0443644523620605,
      "learning_rate": 3.525641025641026e-05,
      "loss": 3.8937,
      "step": 172500
    },
    {
      "epoch": 1478.6324786324785,
      "grad_norm": 5.250459671020508,
      "learning_rate": 3.521367521367522e-05,
      "loss": 3.8998,
      "step": 173000
    },
    {
      "epoch": 1482.905982905983,
      "grad_norm": 6.8388261795043945,
      "learning_rate": 3.5170940170940175e-05,
      "loss": 3.8843,
      "step": 173500
    },
    {
      "epoch": 1487.179487179487,
      "grad_norm": 6.325049877166748,
      "learning_rate": 3.5128205128205125e-05,
      "loss": 3.8915,
      "step": 174000
    },
    {
      "epoch": 1491.4529914529915,
      "grad_norm": 5.923136234283447,
      "learning_rate": 3.508547008547008e-05,
      "loss": 3.8835,
      "step": 174500
    },
    {
      "epoch": 1495.7264957264956,
      "grad_norm": 7.353410243988037,
      "learning_rate": 3.504273504273504e-05,
      "loss": 3.8862,
      "step": 175000
    },
    {
      "epoch": 1500.0,
      "grad_norm": 8.88832950592041,
      "learning_rate": 3.5e-05,
      "loss": 3.8887,
      "step": 175500
    },
    {
      "epoch": 1504.2735042735044,
      "grad_norm": 7.17301607131958,
      "learning_rate": 3.4957264957264954e-05,
      "loss": 3.8947,
      "step": 176000
    },
    {
      "epoch": 1508.5470085470085,
      "grad_norm": 6.155588626861572,
      "learning_rate": 3.491452991452991e-05,
      "loss": 3.8739,
      "step": 176500
    },
    {
      "epoch": 1512.820512820513,
      "grad_norm": 6.687053203582764,
      "learning_rate": 3.487179487179487e-05,
      "loss": 3.8901,
      "step": 177000
    },
    {
      "epoch": 1517.094017094017,
      "grad_norm": 5.1387739181518555,
      "learning_rate": 3.4829059829059826e-05,
      "loss": 3.8882,
      "step": 177500
    },
    {
      "epoch": 1521.3675213675215,
      "grad_norm": 5.3556108474731445,
      "learning_rate": 3.478632478632479e-05,
      "loss": 3.8989,
      "step": 178000
    },
    {
      "epoch": 1525.6410256410256,
      "grad_norm": 7.139941215515137,
      "learning_rate": 3.474358974358975e-05,
      "loss": 3.8833,
      "step": 178500
    },
    {
      "epoch": 1529.91452991453,
      "grad_norm": 5.826701641082764,
      "learning_rate": 3.4700854700854704e-05,
      "loss": 3.8811,
      "step": 179000
    },
    {
      "epoch": 1534.1880341880342,
      "grad_norm": 8.797523498535156,
      "learning_rate": 3.465811965811966e-05,
      "loss": 3.8855,
      "step": 179500
    },
    {
      "epoch": 1538.4615384615386,
      "grad_norm": 7.352578639984131,
      "learning_rate": 3.461538461538462e-05,
      "loss": 3.8889,
      "step": 180000
    },
    {
      "epoch": 1542.7350427350427,
      "grad_norm": 5.126039028167725,
      "learning_rate": 3.4572649572649576e-05,
      "loss": 3.8762,
      "step": 180500
    },
    {
      "epoch": 1547.008547008547,
      "grad_norm": 6.078268051147461,
      "learning_rate": 3.452991452991453e-05,
      "loss": 3.8891,
      "step": 181000
    },
    {
      "epoch": 1551.2820512820513,
      "grad_norm": 5.789132595062256,
      "learning_rate": 3.448717948717949e-05,
      "loss": 3.8898,
      "step": 181500
    },
    {
      "epoch": 1555.5555555555557,
      "grad_norm": 7.569526195526123,
      "learning_rate": 3.444444444444445e-05,
      "loss": 3.8912,
      "step": 182000
    },
    {
      "epoch": 1559.8290598290598,
      "grad_norm": 5.916326999664307,
      "learning_rate": 3.4401709401709405e-05,
      "loss": 3.8867,
      "step": 182500
    },
    {
      "epoch": 1564.1025641025642,
      "grad_norm": 4.832009792327881,
      "learning_rate": 3.435897435897436e-05,
      "loss": 3.873,
      "step": 183000
    },
    {
      "epoch": 1568.3760683760684,
      "grad_norm": 5.660408020019531,
      "learning_rate": 3.431623931623932e-05,
      "loss": 3.8937,
      "step": 183500
    },
    {
      "epoch": 1572.6495726495727,
      "grad_norm": 6.381187915802002,
      "learning_rate": 3.4273504273504276e-05,
      "loss": 3.8905,
      "step": 184000
    },
    {
      "epoch": 1576.923076923077,
      "grad_norm": 6.137252330780029,
      "learning_rate": 3.4230769230769234e-05,
      "loss": 3.8834,
      "step": 184500
    },
    {
      "epoch": 1581.1965811965813,
      "grad_norm": 5.522212982177734,
      "learning_rate": 3.418803418803419e-05,
      "loss": 3.8862,
      "step": 185000
    },
    {
      "epoch": 1585.4700854700855,
      "grad_norm": 8.1517972946167,
      "learning_rate": 3.414529914529915e-05,
      "loss": 3.8793,
      "step": 185500
    },
    {
      "epoch": 1589.7435897435898,
      "grad_norm": 6.236490726470947,
      "learning_rate": 3.4102564102564105e-05,
      "loss": 3.8773,
      "step": 186000
    },
    {
      "epoch": 1594.017094017094,
      "grad_norm": 8.88163948059082,
      "learning_rate": 3.405982905982906e-05,
      "loss": 3.8862,
      "step": 186500
    },
    {
      "epoch": 1598.2905982905984,
      "grad_norm": 11.103718757629395,
      "learning_rate": 3.401709401709402e-05,
      "loss": 3.89,
      "step": 187000
    },
    {
      "epoch": 1602.5641025641025,
      "grad_norm": 5.66525936126709,
      "learning_rate": 3.397435897435898e-05,
      "loss": 3.8811,
      "step": 187500
    },
    {
      "epoch": 1606.837606837607,
      "grad_norm": 7.1092658042907715,
      "learning_rate": 3.3931623931623934e-05,
      "loss": 3.8826,
      "step": 188000
    },
    {
      "epoch": 1611.111111111111,
      "grad_norm": 8.084446907043457,
      "learning_rate": 3.388888888888889e-05,
      "loss": 3.8982,
      "step": 188500
    },
    {
      "epoch": 1615.3846153846155,
      "grad_norm": 7.755466938018799,
      "learning_rate": 3.384615384615385e-05,
      "loss": 3.8778,
      "step": 189000
    },
    {
      "epoch": 1619.6581196581196,
      "grad_norm": 5.133908271789551,
      "learning_rate": 3.3803418803418806e-05,
      "loss": 3.8882,
      "step": 189500
    },
    {
      "epoch": 1623.931623931624,
      "grad_norm": 6.284000873565674,
      "learning_rate": 3.376068376068376e-05,
      "loss": 3.8728,
      "step": 190000
    },
    {
      "epoch": 1628.2051282051282,
      "grad_norm": 6.059098720550537,
      "learning_rate": 3.371794871794872e-05,
      "loss": 3.8851,
      "step": 190500
    },
    {
      "epoch": 1632.4786324786326,
      "grad_norm": 7.031802654266357,
      "learning_rate": 3.367521367521368e-05,
      "loss": 3.8777,
      "step": 191000
    },
    {
      "epoch": 1636.7521367521367,
      "grad_norm": 7.412784099578857,
      "learning_rate": 3.3632478632478634e-05,
      "loss": 3.8831,
      "step": 191500
    },
    {
      "epoch": 1641.025641025641,
      "grad_norm": 6.855136871337891,
      "learning_rate": 3.358974358974359e-05,
      "loss": 3.8729,
      "step": 192000
    },
    {
      "epoch": 1645.2991452991453,
      "grad_norm": 5.768770694732666,
      "learning_rate": 3.354700854700855e-05,
      "loss": 3.8888,
      "step": 192500
    },
    {
      "epoch": 1649.5726495726497,
      "grad_norm": 5.942259788513184,
      "learning_rate": 3.3504273504273506e-05,
      "loss": 3.8842,
      "step": 193000
    },
    {
      "epoch": 1653.8461538461538,
      "grad_norm": 7.120217800140381,
      "learning_rate": 3.346153846153846e-05,
      "loss": 3.8773,
      "step": 193500
    },
    {
      "epoch": 1658.1196581196582,
      "grad_norm": 6.186959266662598,
      "learning_rate": 3.341880341880342e-05,
      "loss": 3.8821,
      "step": 194000
    },
    {
      "epoch": 1662.3931623931624,
      "grad_norm": 6.908320903778076,
      "learning_rate": 3.337606837606838e-05,
      "loss": 3.8788,
      "step": 194500
    },
    {
      "epoch": 1666.6666666666667,
      "grad_norm": 8.969461441040039,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 3.8883,
      "step": 195000
    },
    {
      "epoch": 1670.940170940171,
      "grad_norm": 6.900783538818359,
      "learning_rate": 3.329059829059829e-05,
      "loss": 3.8796,
      "step": 195500
    },
    {
      "epoch": 1675.2136752136753,
      "grad_norm": 7.78842306137085,
      "learning_rate": 3.324786324786325e-05,
      "loss": 3.8834,
      "step": 196000
    },
    {
      "epoch": 1679.4871794871794,
      "grad_norm": 6.43311071395874,
      "learning_rate": 3.3205128205128207e-05,
      "loss": 3.8702,
      "step": 196500
    },
    {
      "epoch": 1683.7606837606838,
      "grad_norm": 7.339198589324951,
      "learning_rate": 3.3162393162393164e-05,
      "loss": 3.892,
      "step": 197000
    },
    {
      "epoch": 1688.034188034188,
      "grad_norm": 8.448479652404785,
      "learning_rate": 3.311965811965812e-05,
      "loss": 3.8789,
      "step": 197500
    },
    {
      "epoch": 1692.3076923076924,
      "grad_norm": 6.473570346832275,
      "learning_rate": 3.307692307692308e-05,
      "loss": 3.8833,
      "step": 198000
    },
    {
      "epoch": 1696.5811965811965,
      "grad_norm": 6.910654544830322,
      "learning_rate": 3.3034188034188035e-05,
      "loss": 3.8742,
      "step": 198500
    },
    {
      "epoch": 1700.854700854701,
      "grad_norm": 6.645727157592773,
      "learning_rate": 3.299145299145299e-05,
      "loss": 3.8787,
      "step": 199000
    },
    {
      "epoch": 1705.128205128205,
      "grad_norm": 6.086300849914551,
      "learning_rate": 3.294871794871795e-05,
      "loss": 3.8761,
      "step": 199500
    },
    {
      "epoch": 1709.4017094017095,
      "grad_norm": 6.716450214385986,
      "learning_rate": 3.290598290598291e-05,
      "loss": 3.8754,
      "step": 200000
    },
    {
      "epoch": 1713.6752136752136,
      "grad_norm": 8.327093124389648,
      "learning_rate": 3.2863247863247864e-05,
      "loss": 3.8861,
      "step": 200500
    },
    {
      "epoch": 1717.948717948718,
      "grad_norm": 6.359273910522461,
      "learning_rate": 3.282051282051282e-05,
      "loss": 3.8737,
      "step": 201000
    },
    {
      "epoch": 1722.2222222222222,
      "grad_norm": 5.38441801071167,
      "learning_rate": 3.277777777777778e-05,
      "loss": 3.8785,
      "step": 201500
    },
    {
      "epoch": 1726.4957264957266,
      "grad_norm": 7.520225524902344,
      "learning_rate": 3.2735042735042736e-05,
      "loss": 3.8693,
      "step": 202000
    },
    {
      "epoch": 1730.7692307692307,
      "grad_norm": 12.958513259887695,
      "learning_rate": 3.269230769230769e-05,
      "loss": 3.8839,
      "step": 202500
    },
    {
      "epoch": 1735.042735042735,
      "grad_norm": 6.710107803344727,
      "learning_rate": 3.264957264957265e-05,
      "loss": 3.8876,
      "step": 203000
    },
    {
      "epoch": 1739.3162393162393,
      "grad_norm": 7.76682186126709,
      "learning_rate": 3.260683760683761e-05,
      "loss": 3.8861,
      "step": 203500
    },
    {
      "epoch": 1743.5897435897436,
      "grad_norm": 5.995610237121582,
      "learning_rate": 3.2564102564102565e-05,
      "loss": 3.88,
      "step": 204000
    },
    {
      "epoch": 1747.8632478632478,
      "grad_norm": 8.188546180725098,
      "learning_rate": 3.252136752136752e-05,
      "loss": 3.8789,
      "step": 204500
    },
    {
      "epoch": 1752.1367521367522,
      "grad_norm": 8.501296997070312,
      "learning_rate": 3.247863247863248e-05,
      "loss": 3.8848,
      "step": 205000
    },
    {
      "epoch": 1756.4102564102564,
      "grad_norm": 7.217730522155762,
      "learning_rate": 3.2435897435897436e-05,
      "loss": 3.8825,
      "step": 205500
    },
    {
      "epoch": 1760.6837606837607,
      "grad_norm": 5.646606922149658,
      "learning_rate": 3.2393162393162394e-05,
      "loss": 3.8772,
      "step": 206000
    },
    {
      "epoch": 1764.957264957265,
      "grad_norm": 8.068549156188965,
      "learning_rate": 3.235042735042735e-05,
      "loss": 3.8774,
      "step": 206500
    },
    {
      "epoch": 1769.2307692307693,
      "grad_norm": 7.6161065101623535,
      "learning_rate": 3.230769230769231e-05,
      "loss": 3.8741,
      "step": 207000
    },
    {
      "epoch": 1773.5042735042734,
      "grad_norm": 9.595163345336914,
      "learning_rate": 3.2264957264957265e-05,
      "loss": 3.8782,
      "step": 207500
    },
    {
      "epoch": 1777.7777777777778,
      "grad_norm": 9.765974044799805,
      "learning_rate": 3.222222222222223e-05,
      "loss": 3.8729,
      "step": 208000
    },
    {
      "epoch": 1782.051282051282,
      "grad_norm": 9.229324340820312,
      "learning_rate": 3.2179487179487186e-05,
      "loss": 3.886,
      "step": 208500
    },
    {
      "epoch": 1786.3247863247864,
      "grad_norm": 7.125509262084961,
      "learning_rate": 3.2136752136752144e-05,
      "loss": 3.8771,
      "step": 209000
    },
    {
      "epoch": 1790.5982905982905,
      "grad_norm": 7.029506206512451,
      "learning_rate": 3.20940170940171e-05,
      "loss": 3.8672,
      "step": 209500
    },
    {
      "epoch": 1794.871794871795,
      "grad_norm": 8.332058906555176,
      "learning_rate": 3.205128205128206e-05,
      "loss": 3.8817,
      "step": 210000
    },
    {
      "epoch": 1799.145299145299,
      "grad_norm": 6.5019307136535645,
      "learning_rate": 3.200854700854701e-05,
      "loss": 3.8846,
      "step": 210500
    },
    {
      "epoch": 1803.4188034188035,
      "grad_norm": 7.087681293487549,
      "learning_rate": 3.1965811965811966e-05,
      "loss": 3.8814,
      "step": 211000
    },
    {
      "epoch": 1807.6923076923076,
      "grad_norm": 6.2073516845703125,
      "learning_rate": 3.192307692307692e-05,
      "loss": 3.875,
      "step": 211500
    },
    {
      "epoch": 1811.965811965812,
      "grad_norm": 6.740688800811768,
      "learning_rate": 3.188034188034188e-05,
      "loss": 3.8751,
      "step": 212000
    },
    {
      "epoch": 1816.2393162393162,
      "grad_norm": 11.333822250366211,
      "learning_rate": 3.183760683760684e-05,
      "loss": 3.8782,
      "step": 212500
    },
    {
      "epoch": 1820.5128205128206,
      "grad_norm": 6.1607136726379395,
      "learning_rate": 3.1794871794871795e-05,
      "loss": 3.8729,
      "step": 213000
    },
    {
      "epoch": 1824.7863247863247,
      "grad_norm": 6.190780162811279,
      "learning_rate": 3.175213675213675e-05,
      "loss": 3.8725,
      "step": 213500
    },
    {
      "epoch": 1829.059829059829,
      "grad_norm": 7.300971984863281,
      "learning_rate": 3.170940170940171e-05,
      "loss": 3.8789,
      "step": 214000
    },
    {
      "epoch": 1833.3333333333333,
      "grad_norm": 7.274680137634277,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 3.8846,
      "step": 214500
    },
    {
      "epoch": 1837.6068376068376,
      "grad_norm": 7.255353927612305,
      "learning_rate": 3.162393162393162e-05,
      "loss": 3.8741,
      "step": 215000
    },
    {
      "epoch": 1841.8803418803418,
      "grad_norm": 6.6894450187683105,
      "learning_rate": 3.158119658119658e-05,
      "loss": 3.8733,
      "step": 215500
    },
    {
      "epoch": 1846.1538461538462,
      "grad_norm": 5.961180210113525,
      "learning_rate": 3.153846153846154e-05,
      "loss": 3.8779,
      "step": 216000
    },
    {
      "epoch": 1850.4273504273503,
      "grad_norm": 5.566915988922119,
      "learning_rate": 3.1495726495726495e-05,
      "loss": 3.8797,
      "step": 216500
    },
    {
      "epoch": 1854.7008547008547,
      "grad_norm": 6.978036880493164,
      "learning_rate": 3.145299145299145e-05,
      "loss": 3.8812,
      "step": 217000
    },
    {
      "epoch": 1858.974358974359,
      "grad_norm": 5.646541118621826,
      "learning_rate": 3.141025641025641e-05,
      "loss": 3.8731,
      "step": 217500
    },
    {
      "epoch": 1863.2478632478633,
      "grad_norm": 5.884077072143555,
      "learning_rate": 3.136752136752137e-05,
      "loss": 3.8797,
      "step": 218000
    },
    {
      "epoch": 1867.5213675213674,
      "grad_norm": 6.465122699737549,
      "learning_rate": 3.1324786324786324e-05,
      "loss": 3.8554,
      "step": 218500
    },
    {
      "epoch": 1871.7948717948718,
      "grad_norm": 7.561049938201904,
      "learning_rate": 3.128205128205128e-05,
      "loss": 3.8824,
      "step": 219000
    },
    {
      "epoch": 1876.068376068376,
      "grad_norm": 5.614378929138184,
      "learning_rate": 3.123931623931624e-05,
      "loss": 3.8658,
      "step": 219500
    },
    {
      "epoch": 1880.3418803418804,
      "grad_norm": 9.743213653564453,
      "learning_rate": 3.1196581196581195e-05,
      "loss": 3.8757,
      "step": 220000
    },
    {
      "epoch": 1884.6153846153845,
      "grad_norm": 6.706124305725098,
      "learning_rate": 3.115384615384615e-05,
      "loss": 3.8781,
      "step": 220500
    },
    {
      "epoch": 1888.888888888889,
      "grad_norm": 8.171833038330078,
      "learning_rate": 3.111111111111111e-05,
      "loss": 3.8796,
      "step": 221000
    },
    {
      "epoch": 1893.162393162393,
      "grad_norm": 6.09094762802124,
      "learning_rate": 3.1068376068376074e-05,
      "loss": 3.8697,
      "step": 221500
    },
    {
      "epoch": 1897.4358974358975,
      "grad_norm": 9.573050498962402,
      "learning_rate": 3.102564102564103e-05,
      "loss": 3.8758,
      "step": 222000
    },
    {
      "epoch": 1901.7094017094016,
      "grad_norm": 5.917961597442627,
      "learning_rate": 3.098290598290599e-05,
      "loss": 3.8777,
      "step": 222500
    },
    {
      "epoch": 1905.982905982906,
      "grad_norm": 5.593689441680908,
      "learning_rate": 3.0940170940170946e-05,
      "loss": 3.8653,
      "step": 223000
    },
    {
      "epoch": 1910.2564102564102,
      "grad_norm": 6.65146017074585,
      "learning_rate": 3.08974358974359e-05,
      "loss": 3.8767,
      "step": 223500
    },
    {
      "epoch": 1914.5299145299145,
      "grad_norm": 5.921352386474609,
      "learning_rate": 3.085470085470086e-05,
      "loss": 3.8657,
      "step": 224000
    },
    {
      "epoch": 1918.8034188034187,
      "grad_norm": 5.9994025230407715,
      "learning_rate": 3.081196581196582e-05,
      "loss": 3.873,
      "step": 224500
    },
    {
      "epoch": 1923.076923076923,
      "grad_norm": 5.317912578582764,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 3.867,
      "step": 225000
    },
    {
      "epoch": 1927.3504273504273,
      "grad_norm": 8.656668663024902,
      "learning_rate": 3.072649572649573e-05,
      "loss": 3.873,
      "step": 225500
    },
    {
      "epoch": 1931.6239316239316,
      "grad_norm": 7.053510665893555,
      "learning_rate": 3.068376068376069e-05,
      "loss": 3.8715,
      "step": 226000
    },
    {
      "epoch": 1935.8974358974358,
      "grad_norm": 7.458500862121582,
      "learning_rate": 3.0641025641025646e-05,
      "loss": 3.8696,
      "step": 226500
    },
    {
      "epoch": 1940.1709401709402,
      "grad_norm": 9.778066635131836,
      "learning_rate": 3.05982905982906e-05,
      "loss": 3.8756,
      "step": 227000
    },
    {
      "epoch": 1944.4444444444443,
      "grad_norm": 6.306413173675537,
      "learning_rate": 3.055555555555556e-05,
      "loss": 3.8736,
      "step": 227500
    },
    {
      "epoch": 1948.7179487179487,
      "grad_norm": 6.9407267570495605,
      "learning_rate": 3.0512820512820518e-05,
      "loss": 3.8757,
      "step": 228000
    },
    {
      "epoch": 1952.991452991453,
      "grad_norm": 6.672031879425049,
      "learning_rate": 3.0470085470085475e-05,
      "loss": 3.8737,
      "step": 228500
    },
    {
      "epoch": 1957.2649572649573,
      "grad_norm": 6.252605438232422,
      "learning_rate": 3.0427350427350425e-05,
      "loss": 3.8661,
      "step": 229000
    },
    {
      "epoch": 1961.5384615384614,
      "grad_norm": 7.23162841796875,
      "learning_rate": 3.0384615384615382e-05,
      "loss": 3.8791,
      "step": 229500
    },
    {
      "epoch": 1965.8119658119658,
      "grad_norm": 6.171245574951172,
      "learning_rate": 3.034188034188034e-05,
      "loss": 3.8811,
      "step": 230000
    },
    {
      "epoch": 1970.08547008547,
      "grad_norm": 5.427062034606934,
      "learning_rate": 3.0299145299145297e-05,
      "loss": 3.8786,
      "step": 230500
    },
    {
      "epoch": 1974.3589743589744,
      "grad_norm": 8.708120346069336,
      "learning_rate": 3.0256410256410257e-05,
      "loss": 3.8715,
      "step": 231000
    },
    {
      "epoch": 1978.6324786324785,
      "grad_norm": 6.557018756866455,
      "learning_rate": 3.0213675213675215e-05,
      "loss": 3.8695,
      "step": 231500
    },
    {
      "epoch": 1982.905982905983,
      "grad_norm": 5.99291467666626,
      "learning_rate": 3.0170940170940172e-05,
      "loss": 3.8763,
      "step": 232000
    },
    {
      "epoch": 1987.179487179487,
      "grad_norm": 6.44040060043335,
      "learning_rate": 3.012820512820513e-05,
      "loss": 3.8669,
      "step": 232500
    },
    {
      "epoch": 1991.4529914529915,
      "grad_norm": 6.718740463256836,
      "learning_rate": 3.0085470085470086e-05,
      "loss": 3.8741,
      "step": 233000
    },
    {
      "epoch": 1995.7264957264956,
      "grad_norm": 7.087294578552246,
      "learning_rate": 3.0042735042735044e-05,
      "loss": 3.866,
      "step": 233500
    },
    {
      "epoch": 2000.0,
      "grad_norm": 11.214970588684082,
      "learning_rate": 3e-05,
      "loss": 3.8704,
      "step": 234000
    },
    {
      "epoch": 2004.2735042735044,
      "grad_norm": 7.135955810546875,
      "learning_rate": 2.9957264957264958e-05,
      "loss": 3.8636,
      "step": 234500
    },
    {
      "epoch": 2008.5470085470085,
      "grad_norm": 6.994925022125244,
      "learning_rate": 2.9914529914529915e-05,
      "loss": 3.8735,
      "step": 235000
    },
    {
      "epoch": 2012.820512820513,
      "grad_norm": 6.596508502960205,
      "learning_rate": 2.9871794871794872e-05,
      "loss": 3.8614,
      "step": 235500
    },
    {
      "epoch": 2017.094017094017,
      "grad_norm": 6.021636962890625,
      "learning_rate": 2.982905982905983e-05,
      "loss": 3.8752,
      "step": 236000
    },
    {
      "epoch": 2021.3675213675215,
      "grad_norm": 6.063498020172119,
      "learning_rate": 2.9786324786324787e-05,
      "loss": 3.8698,
      "step": 236500
    },
    {
      "epoch": 2025.6410256410256,
      "grad_norm": 6.839400768280029,
      "learning_rate": 2.9743589743589744e-05,
      "loss": 3.8675,
      "step": 237000
    },
    {
      "epoch": 2029.91452991453,
      "grad_norm": 6.324821472167969,
      "learning_rate": 2.97008547008547e-05,
      "loss": 3.8762,
      "step": 237500
    },
    {
      "epoch": 2034.1880341880342,
      "grad_norm": 9.658435821533203,
      "learning_rate": 2.965811965811966e-05,
      "loss": 3.8647,
      "step": 238000
    },
    {
      "epoch": 2038.4615384615386,
      "grad_norm": 4.987521171569824,
      "learning_rate": 2.9615384615384616e-05,
      "loss": 3.8725,
      "step": 238500
    },
    {
      "epoch": 2042.7350427350427,
      "grad_norm": 8.262955665588379,
      "learning_rate": 2.9572649572649573e-05,
      "loss": 3.8765,
      "step": 239000
    },
    {
      "epoch": 2047.008547008547,
      "grad_norm": 5.402341842651367,
      "learning_rate": 2.9529914529914533e-05,
      "loss": 3.8757,
      "step": 239500
    },
    {
      "epoch": 2051.2820512820513,
      "grad_norm": 5.730078220367432,
      "learning_rate": 2.948717948717949e-05,
      "loss": 3.8739,
      "step": 240000
    },
    {
      "epoch": 2055.5555555555557,
      "grad_norm": 7.305825233459473,
      "learning_rate": 2.9444444444444448e-05,
      "loss": 3.8638,
      "step": 240500
    },
    {
      "epoch": 2059.82905982906,
      "grad_norm": 6.6525678634643555,
      "learning_rate": 2.9401709401709405e-05,
      "loss": 3.8773,
      "step": 241000
    },
    {
      "epoch": 2064.102564102564,
      "grad_norm": 7.378720760345459,
      "learning_rate": 2.9358974358974362e-05,
      "loss": 3.8647,
      "step": 241500
    },
    {
      "epoch": 2068.3760683760684,
      "grad_norm": 5.782235622406006,
      "learning_rate": 2.931623931623932e-05,
      "loss": 3.866,
      "step": 242000
    },
    {
      "epoch": 2072.6495726495727,
      "grad_norm": 6.66444206237793,
      "learning_rate": 2.9273504273504277e-05,
      "loss": 3.8702,
      "step": 242500
    },
    {
      "epoch": 2076.923076923077,
      "grad_norm": 6.007595539093018,
      "learning_rate": 2.9230769230769234e-05,
      "loss": 3.8665,
      "step": 243000
    },
    {
      "epoch": 2081.196581196581,
      "grad_norm": 8.48533821105957,
      "learning_rate": 2.918803418803419e-05,
      "loss": 3.8641,
      "step": 243500
    },
    {
      "epoch": 2085.4700854700855,
      "grad_norm": 5.331730365753174,
      "learning_rate": 2.914529914529915e-05,
      "loss": 3.8617,
      "step": 244000
    },
    {
      "epoch": 2089.74358974359,
      "grad_norm": 6.848256587982178,
      "learning_rate": 2.9102564102564106e-05,
      "loss": 3.8749,
      "step": 244500
    },
    {
      "epoch": 2094.017094017094,
      "grad_norm": 6.398602485656738,
      "learning_rate": 2.9059829059829063e-05,
      "loss": 3.8668,
      "step": 245000
    },
    {
      "epoch": 2098.290598290598,
      "grad_norm": 5.058210849761963,
      "learning_rate": 2.901709401709402e-05,
      "loss": 3.8732,
      "step": 245500
    },
    {
      "epoch": 2102.5641025641025,
      "grad_norm": 5.580383777618408,
      "learning_rate": 2.8974358974358977e-05,
      "loss": 3.8814,
      "step": 246000
    },
    {
      "epoch": 2106.837606837607,
      "grad_norm": 7.429088115692139,
      "learning_rate": 2.8931623931623934e-05,
      "loss": 3.8701,
      "step": 246500
    },
    {
      "epoch": 2111.1111111111113,
      "grad_norm": 5.976251125335693,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 3.8667,
      "step": 247000
    },
    {
      "epoch": 2115.3846153846152,
      "grad_norm": 6.612605571746826,
      "learning_rate": 2.8846153846153845e-05,
      "loss": 3.8664,
      "step": 247500
    },
    {
      "epoch": 2119.6581196581196,
      "grad_norm": 9.209565162658691,
      "learning_rate": 2.8803418803418803e-05,
      "loss": 3.8643,
      "step": 248000
    },
    {
      "epoch": 2123.931623931624,
      "grad_norm": 6.357812881469727,
      "learning_rate": 2.876068376068376e-05,
      "loss": 3.8759,
      "step": 248500
    },
    {
      "epoch": 2128.2051282051284,
      "grad_norm": 5.879672527313232,
      "learning_rate": 2.8717948717948717e-05,
      "loss": 3.8716,
      "step": 249000
    },
    {
      "epoch": 2132.4786324786323,
      "grad_norm": 7.216097831726074,
      "learning_rate": 2.8675213675213674e-05,
      "loss": 3.8712,
      "step": 249500
    },
    {
      "epoch": 2136.7521367521367,
      "grad_norm": 5.483220100402832,
      "learning_rate": 2.863247863247863e-05,
      "loss": 3.8661,
      "step": 250000
    },
    {
      "epoch": 2141.025641025641,
      "grad_norm": 6.336112022399902,
      "learning_rate": 2.858974358974359e-05,
      "loss": 3.8761,
      "step": 250500
    },
    {
      "epoch": 2145.2991452991455,
      "grad_norm": 6.196169853210449,
      "learning_rate": 2.8547008547008546e-05,
      "loss": 3.866,
      "step": 251000
    },
    {
      "epoch": 2149.5726495726494,
      "grad_norm": 8.791023254394531,
      "learning_rate": 2.8504273504273503e-05,
      "loss": 3.8709,
      "step": 251500
    },
    {
      "epoch": 2153.846153846154,
      "grad_norm": 4.934384346008301,
      "learning_rate": 2.846153846153846e-05,
      "loss": 3.8712,
      "step": 252000
    },
    {
      "epoch": 2158.119658119658,
      "grad_norm": 6.6350531578063965,
      "learning_rate": 2.8418803418803418e-05,
      "loss": 3.8676,
      "step": 252500
    },
    {
      "epoch": 2162.3931623931626,
      "grad_norm": 6.321859359741211,
      "learning_rate": 2.8376068376068378e-05,
      "loss": 3.8669,
      "step": 253000
    },
    {
      "epoch": 2166.6666666666665,
      "grad_norm": 6.579145908355713,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 3.8703,
      "step": 253500
    },
    {
      "epoch": 2170.940170940171,
      "grad_norm": 9.954607009887695,
      "learning_rate": 2.8290598290598293e-05,
      "loss": 3.863,
      "step": 254000
    },
    {
      "epoch": 2175.2136752136753,
      "grad_norm": 5.303163051605225,
      "learning_rate": 2.824786324786325e-05,
      "loss": 3.8646,
      "step": 254500
    },
    {
      "epoch": 2179.4871794871797,
      "grad_norm": 4.604946613311768,
      "learning_rate": 2.8205128205128207e-05,
      "loss": 3.8651,
      "step": 255000
    },
    {
      "epoch": 2183.7606837606836,
      "grad_norm": 6.886598587036133,
      "learning_rate": 2.8162393162393164e-05,
      "loss": 3.8714,
      "step": 255500
    },
    {
      "epoch": 2188.034188034188,
      "grad_norm": 7.55396032333374,
      "learning_rate": 2.811965811965812e-05,
      "loss": 3.8742,
      "step": 256000
    },
    {
      "epoch": 2192.3076923076924,
      "grad_norm": 7.90762186050415,
      "learning_rate": 2.807692307692308e-05,
      "loss": 3.8725,
      "step": 256500
    },
    {
      "epoch": 2196.5811965811968,
      "grad_norm": 6.729964733123779,
      "learning_rate": 2.8034188034188036e-05,
      "loss": 3.8671,
      "step": 257000
    },
    {
      "epoch": 2200.8547008547007,
      "grad_norm": 6.17934513092041,
      "learning_rate": 2.7991452991452993e-05,
      "loss": 3.8688,
      "step": 257500
    },
    {
      "epoch": 2205.128205128205,
      "grad_norm": 7.386281967163086,
      "learning_rate": 2.794871794871795e-05,
      "loss": 3.8624,
      "step": 258000
    },
    {
      "epoch": 2209.4017094017095,
      "grad_norm": 6.762639045715332,
      "learning_rate": 2.7905982905982907e-05,
      "loss": 3.8548,
      "step": 258500
    },
    {
      "epoch": 2213.675213675214,
      "grad_norm": 6.185686111450195,
      "learning_rate": 2.7863247863247865e-05,
      "loss": 3.8671,
      "step": 259000
    },
    {
      "epoch": 2217.948717948718,
      "grad_norm": 7.579869270324707,
      "learning_rate": 2.7820512820512822e-05,
      "loss": 3.8666,
      "step": 259500
    },
    {
      "epoch": 2222.222222222222,
      "grad_norm": 6.825736045837402,
      "learning_rate": 2.777777777777778e-05,
      "loss": 3.8698,
      "step": 260000
    },
    {
      "epoch": 2226.4957264957266,
      "grad_norm": 5.1796956062316895,
      "learning_rate": 2.7735042735042736e-05,
      "loss": 3.87,
      "step": 260500
    },
    {
      "epoch": 2230.769230769231,
      "grad_norm": 6.446352481842041,
      "learning_rate": 2.7692307692307694e-05,
      "loss": 3.8703,
      "step": 261000
    },
    {
      "epoch": 2235.042735042735,
      "grad_norm": 5.971834659576416,
      "learning_rate": 2.7649572649572654e-05,
      "loss": 3.8674,
      "step": 261500
    },
    {
      "epoch": 2239.3162393162393,
      "grad_norm": 7.060756683349609,
      "learning_rate": 2.760683760683761e-05,
      "loss": 3.8719,
      "step": 262000
    },
    {
      "epoch": 2243.5897435897436,
      "grad_norm": 6.387053966522217,
      "learning_rate": 2.756410256410257e-05,
      "loss": 3.8611,
      "step": 262500
    },
    {
      "epoch": 2247.863247863248,
      "grad_norm": 6.359095573425293,
      "learning_rate": 2.7521367521367526e-05,
      "loss": 3.8606,
      "step": 263000
    },
    {
      "epoch": 2252.136752136752,
      "grad_norm": 6.435451030731201,
      "learning_rate": 2.7478632478632483e-05,
      "loss": 3.8723,
      "step": 263500
    },
    {
      "epoch": 2256.4102564102564,
      "grad_norm": 5.12763786315918,
      "learning_rate": 2.743589743589744e-05,
      "loss": 3.8654,
      "step": 264000
    },
    {
      "epoch": 2260.6837606837607,
      "grad_norm": 8.139139175415039,
      "learning_rate": 2.7393162393162397e-05,
      "loss": 3.8678,
      "step": 264500
    },
    {
      "epoch": 2264.957264957265,
      "grad_norm": 6.118929386138916,
      "learning_rate": 2.7350427350427355e-05,
      "loss": 3.8638,
      "step": 265000
    },
    {
      "epoch": 2269.230769230769,
      "grad_norm": 6.97443962097168,
      "learning_rate": 2.7307692307692305e-05,
      "loss": 3.8741,
      "step": 265500
    },
    {
      "epoch": 2273.5042735042734,
      "grad_norm": 6.208436965942383,
      "learning_rate": 2.7264957264957262e-05,
      "loss": 3.8582,
      "step": 266000
    },
    {
      "epoch": 2277.777777777778,
      "grad_norm": 7.378619194030762,
      "learning_rate": 2.7222222222222223e-05,
      "loss": 3.861,
      "step": 266500
    },
    {
      "epoch": 2282.051282051282,
      "grad_norm": 8.57574462890625,
      "learning_rate": 2.717948717948718e-05,
      "loss": 3.8687,
      "step": 267000
    },
    {
      "epoch": 2286.324786324786,
      "grad_norm": 7.401900768280029,
      "learning_rate": 2.7136752136752137e-05,
      "loss": 3.8648,
      "step": 267500
    },
    {
      "epoch": 2290.5982905982905,
      "grad_norm": 8.652878761291504,
      "learning_rate": 2.7094017094017094e-05,
      "loss": 3.8687,
      "step": 268000
    },
    {
      "epoch": 2294.871794871795,
      "grad_norm": 5.2012481689453125,
      "learning_rate": 2.705128205128205e-05,
      "loss": 3.8612,
      "step": 268500
    },
    {
      "epoch": 2299.1452991452993,
      "grad_norm": 10.328161239624023,
      "learning_rate": 2.700854700854701e-05,
      "loss": 3.8714,
      "step": 269000
    },
    {
      "epoch": 2303.4188034188032,
      "grad_norm": 8.59821891784668,
      "learning_rate": 2.6965811965811966e-05,
      "loss": 3.8706,
      "step": 269500
    },
    {
      "epoch": 2307.6923076923076,
      "grad_norm": 5.995849609375,
      "learning_rate": 2.6923076923076923e-05,
      "loss": 3.8673,
      "step": 270000
    },
    {
      "epoch": 2311.965811965812,
      "grad_norm": 6.707541465759277,
      "learning_rate": 2.688034188034188e-05,
      "loss": 3.8659,
      "step": 270500
    },
    {
      "epoch": 2316.2393162393164,
      "grad_norm": 5.857332229614258,
      "learning_rate": 2.6837606837606838e-05,
      "loss": 3.8586,
      "step": 271000
    },
    {
      "epoch": 2320.5128205128203,
      "grad_norm": 7.37634801864624,
      "learning_rate": 2.6794871794871795e-05,
      "loss": 3.8727,
      "step": 271500
    },
    {
      "epoch": 2324.7863247863247,
      "grad_norm": 7.949588298797607,
      "learning_rate": 2.6752136752136752e-05,
      "loss": 3.8751,
      "step": 272000
    },
    {
      "epoch": 2329.059829059829,
      "grad_norm": 7.231234073638916,
      "learning_rate": 2.670940170940171e-05,
      "loss": 3.8672,
      "step": 272500
    },
    {
      "epoch": 2333.3333333333335,
      "grad_norm": 6.0908708572387695,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 3.8592,
      "step": 273000
    },
    {
      "epoch": 2337.6068376068374,
      "grad_norm": 9.237683296203613,
      "learning_rate": 2.6623931623931624e-05,
      "loss": 3.8611,
      "step": 273500
    },
    {
      "epoch": 2341.880341880342,
      "grad_norm": 7.281806468963623,
      "learning_rate": 2.658119658119658e-05,
      "loss": 3.855,
      "step": 274000
    },
    {
      "epoch": 2346.153846153846,
      "grad_norm": 9.162506103515625,
      "learning_rate": 2.6538461538461538e-05,
      "loss": 3.866,
      "step": 274500
    },
    {
      "epoch": 2350.4273504273506,
      "grad_norm": 8.60576343536377,
      "learning_rate": 2.64957264957265e-05,
      "loss": 3.8591,
      "step": 275000
    },
    {
      "epoch": 2354.7008547008545,
      "grad_norm": 6.212117671966553,
      "learning_rate": 2.6452991452991456e-05,
      "loss": 3.8673,
      "step": 275500
    },
    {
      "epoch": 2358.974358974359,
      "grad_norm": 7.315773010253906,
      "learning_rate": 2.6410256410256413e-05,
      "loss": 3.8614,
      "step": 276000
    },
    {
      "epoch": 2363.2478632478633,
      "grad_norm": 6.560557842254639,
      "learning_rate": 2.636752136752137e-05,
      "loss": 3.8744,
      "step": 276500
    },
    {
      "epoch": 2367.5213675213677,
      "grad_norm": 6.4314680099487305,
      "learning_rate": 2.6324786324786328e-05,
      "loss": 3.8604,
      "step": 277000
    },
    {
      "epoch": 2371.7948717948716,
      "grad_norm": 6.43577766418457,
      "learning_rate": 2.6282051282051285e-05,
      "loss": 3.8633,
      "step": 277500
    },
    {
      "epoch": 2376.068376068376,
      "grad_norm": 7.562428951263428,
      "learning_rate": 2.6239316239316242e-05,
      "loss": 3.8639,
      "step": 278000
    },
    {
      "epoch": 2380.3418803418804,
      "grad_norm": 7.317022800445557,
      "learning_rate": 2.61965811965812e-05,
      "loss": 3.8621,
      "step": 278500
    },
    {
      "epoch": 2384.6153846153848,
      "grad_norm": 6.664881229400635,
      "learning_rate": 2.6153846153846157e-05,
      "loss": 3.8579,
      "step": 279000
    },
    {
      "epoch": 2388.8888888888887,
      "grad_norm": 6.91467809677124,
      "learning_rate": 2.6111111111111114e-05,
      "loss": 3.865,
      "step": 279500
    },
    {
      "epoch": 2393.162393162393,
      "grad_norm": 6.073693752288818,
      "learning_rate": 2.606837606837607e-05,
      "loss": 3.8701,
      "step": 280000
    },
    {
      "epoch": 2397.4358974358975,
      "grad_norm": 5.824352741241455,
      "learning_rate": 2.6025641025641028e-05,
      "loss": 3.8554,
      "step": 280500
    },
    {
      "epoch": 2401.709401709402,
      "grad_norm": 8.229446411132812,
      "learning_rate": 2.5982905982905985e-05,
      "loss": 3.8623,
      "step": 281000
    },
    {
      "epoch": 2405.982905982906,
      "grad_norm": 9.294300079345703,
      "learning_rate": 2.5940170940170943e-05,
      "loss": 3.8695,
      "step": 281500
    },
    {
      "epoch": 2410.25641025641,
      "grad_norm": 6.727072715759277,
      "learning_rate": 2.58974358974359e-05,
      "loss": 3.8608,
      "step": 282000
    },
    {
      "epoch": 2414.5299145299145,
      "grad_norm": 8.375951766967773,
      "learning_rate": 2.5854700854700857e-05,
      "loss": 3.8717,
      "step": 282500
    },
    {
      "epoch": 2418.803418803419,
      "grad_norm": 8.253819465637207,
      "learning_rate": 2.5811965811965814e-05,
      "loss": 3.8604,
      "step": 283000
    },
    {
      "epoch": 2423.076923076923,
      "grad_norm": 6.689769268035889,
      "learning_rate": 2.5769230769230768e-05,
      "loss": 3.8621,
      "step": 283500
    },
    {
      "epoch": 2427.3504273504273,
      "grad_norm": 5.315905570983887,
      "learning_rate": 2.5726495726495725e-05,
      "loss": 3.8722,
      "step": 284000
    },
    {
      "epoch": 2431.6239316239316,
      "grad_norm": 7.004143238067627,
      "learning_rate": 2.5683760683760682e-05,
      "loss": 3.8661,
      "step": 284500
    },
    {
      "epoch": 2435.897435897436,
      "grad_norm": 5.58003044128418,
      "learning_rate": 2.564102564102564e-05,
      "loss": 3.8542,
      "step": 285000
    },
    {
      "epoch": 2440.17094017094,
      "grad_norm": 9.202375411987305,
      "learning_rate": 2.5598290598290597e-05,
      "loss": 3.8535,
      "step": 285500
    },
    {
      "epoch": 2444.4444444444443,
      "grad_norm": 7.963063716888428,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 3.8589,
      "step": 286000
    },
    {
      "epoch": 2448.7179487179487,
      "grad_norm": 5.71821403503418,
      "learning_rate": 2.551282051282051e-05,
      "loss": 3.8674,
      "step": 286500
    },
    {
      "epoch": 2452.991452991453,
      "grad_norm": 6.239468574523926,
      "learning_rate": 2.547008547008547e-05,
      "loss": 3.8566,
      "step": 287000
    },
    {
      "epoch": 2457.264957264957,
      "grad_norm": 6.313135147094727,
      "learning_rate": 2.5427350427350426e-05,
      "loss": 3.8589,
      "step": 287500
    },
    {
      "epoch": 2461.5384615384614,
      "grad_norm": 13.461230278015137,
      "learning_rate": 2.5384615384615383e-05,
      "loss": 3.8688,
      "step": 288000
    },
    {
      "epoch": 2465.811965811966,
      "grad_norm": 6.837730884552002,
      "learning_rate": 2.5341880341880344e-05,
      "loss": 3.8623,
      "step": 288500
    },
    {
      "epoch": 2470.08547008547,
      "grad_norm": 8.42053508758545,
      "learning_rate": 2.52991452991453e-05,
      "loss": 3.8649,
      "step": 289000
    },
    {
      "epoch": 2474.358974358974,
      "grad_norm": 6.05740213394165,
      "learning_rate": 2.5256410256410258e-05,
      "loss": 3.8653,
      "step": 289500
    },
    {
      "epoch": 2478.6324786324785,
      "grad_norm": 11.736297607421875,
      "learning_rate": 2.5213675213675215e-05,
      "loss": 3.8604,
      "step": 290000
    },
    {
      "epoch": 2482.905982905983,
      "grad_norm": 8.421445846557617,
      "learning_rate": 2.5170940170940172e-05,
      "loss": 3.8586,
      "step": 290500
    },
    {
      "epoch": 2487.1794871794873,
      "grad_norm": 6.8439507484436035,
      "learning_rate": 2.512820512820513e-05,
      "loss": 3.8547,
      "step": 291000
    },
    {
      "epoch": 2491.4529914529912,
      "grad_norm": 6.929172992706299,
      "learning_rate": 2.5085470085470087e-05,
      "loss": 3.8595,
      "step": 291500
    },
    {
      "epoch": 2495.7264957264956,
      "grad_norm": 8.34686279296875,
      "learning_rate": 2.5042735042735044e-05,
      "loss": 3.8589,
      "step": 292000
    },
    {
      "epoch": 2500.0,
      "grad_norm": 8.32435417175293,
      "learning_rate": 2.5e-05,
      "loss": 3.8612,
      "step": 292500
    },
    {
      "epoch": 2504.2735042735044,
      "grad_norm": 7.806347370147705,
      "learning_rate": 2.495726495726496e-05,
      "loss": 3.8559,
      "step": 293000
    },
    {
      "epoch": 2508.5470085470088,
      "grad_norm": 8.284542083740234,
      "learning_rate": 2.4914529914529916e-05,
      "loss": 3.8724,
      "step": 293500
    },
    {
      "epoch": 2512.8205128205127,
      "grad_norm": 4.711062431335449,
      "learning_rate": 2.4871794871794873e-05,
      "loss": 3.8604,
      "step": 294000
    },
    {
      "epoch": 2517.094017094017,
      "grad_norm": 5.5187225341796875,
      "learning_rate": 2.482905982905983e-05,
      "loss": 3.8643,
      "step": 294500
    },
    {
      "epoch": 2521.3675213675215,
      "grad_norm": 10.870867729187012,
      "learning_rate": 2.4786324786324787e-05,
      "loss": 3.8712,
      "step": 295000
    },
    {
      "epoch": 2525.641025641026,
      "grad_norm": 8.128747940063477,
      "learning_rate": 2.4743589743589744e-05,
      "loss": 3.8572,
      "step": 295500
    },
    {
      "epoch": 2529.91452991453,
      "grad_norm": 4.801506996154785,
      "learning_rate": 2.47008547008547e-05,
      "loss": 3.8533,
      "step": 296000
    },
    {
      "epoch": 2534.188034188034,
      "grad_norm": 10.674232482910156,
      "learning_rate": 2.465811965811966e-05,
      "loss": 3.8564,
      "step": 296500
    },
    {
      "epoch": 2538.4615384615386,
      "grad_norm": 7.190107822418213,
      "learning_rate": 2.461538461538462e-05,
      "loss": 3.868,
      "step": 297000
    },
    {
      "epoch": 2542.735042735043,
      "grad_norm": 6.911982536315918,
      "learning_rate": 2.4572649572649573e-05,
      "loss": 3.8441,
      "step": 297500
    },
    {
      "epoch": 2547.008547008547,
      "grad_norm": 5.5095038414001465,
      "learning_rate": 2.452991452991453e-05,
      "loss": 3.8637,
      "step": 298000
    },
    {
      "epoch": 2551.2820512820513,
      "grad_norm": 7.9434356689453125,
      "learning_rate": 2.4487179487179488e-05,
      "loss": 3.8547,
      "step": 298500
    },
    {
      "epoch": 2555.5555555555557,
      "grad_norm": 5.157257080078125,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 3.8583,
      "step": 299000
    },
    {
      "epoch": 2559.82905982906,
      "grad_norm": 9.203621864318848,
      "learning_rate": 2.4401709401709402e-05,
      "loss": 3.8622,
      "step": 299500
    },
    {
      "epoch": 2564.102564102564,
      "grad_norm": 10.79738712310791,
      "learning_rate": 2.435897435897436e-05,
      "loss": 3.8541,
      "step": 300000
    },
    {
      "epoch": 2568.3760683760684,
      "grad_norm": 8.502331733703613,
      "learning_rate": 2.4316239316239317e-05,
      "loss": 3.8625,
      "step": 300500
    },
    {
      "epoch": 2572.6495726495727,
      "grad_norm": 7.642268180847168,
      "learning_rate": 2.4273504273504274e-05,
      "loss": 3.8543,
      "step": 301000
    },
    {
      "epoch": 2576.923076923077,
      "grad_norm": 6.5482401847839355,
      "learning_rate": 2.423076923076923e-05,
      "loss": 3.8615,
      "step": 301500
    },
    {
      "epoch": 2581.196581196581,
      "grad_norm": 9.769577980041504,
      "learning_rate": 2.4188034188034188e-05,
      "loss": 3.864,
      "step": 302000
    },
    {
      "epoch": 2585.4700854700855,
      "grad_norm": 5.978809833526611,
      "learning_rate": 2.4145299145299145e-05,
      "loss": 3.8568,
      "step": 302500
    },
    {
      "epoch": 2589.74358974359,
      "grad_norm": 7.2167816162109375,
      "learning_rate": 2.4102564102564103e-05,
      "loss": 3.8616,
      "step": 303000
    },
    {
      "epoch": 2594.017094017094,
      "grad_norm": 6.761388301849365,
      "learning_rate": 2.4059829059829063e-05,
      "loss": 3.8582,
      "step": 303500
    },
    {
      "epoch": 2598.290598290598,
      "grad_norm": 7.131763458251953,
      "learning_rate": 2.401709401709402e-05,
      "loss": 3.8631,
      "step": 304000
    },
    {
      "epoch": 2602.5641025641025,
      "grad_norm": 15.352784156799316,
      "learning_rate": 2.3974358974358978e-05,
      "loss": 3.8526,
      "step": 304500
    },
    {
      "epoch": 2606.837606837607,
      "grad_norm": 6.448829174041748,
      "learning_rate": 2.3931623931623935e-05,
      "loss": 3.8557,
      "step": 305000
    },
    {
      "epoch": 2611.1111111111113,
      "grad_norm": 5.656983852386475,
      "learning_rate": 2.3888888888888892e-05,
      "loss": 3.8694,
      "step": 305500
    },
    {
      "epoch": 2615.3846153846152,
      "grad_norm": 6.085046768188477,
      "learning_rate": 2.384615384615385e-05,
      "loss": 3.8618,
      "step": 306000
    },
    {
      "epoch": 2619.6581196581196,
      "grad_norm": 6.973981857299805,
      "learning_rate": 2.3803418803418803e-05,
      "loss": 3.8668,
      "step": 306500
    },
    {
      "epoch": 2623.931623931624,
      "grad_norm": 5.274648666381836,
      "learning_rate": 2.376068376068376e-05,
      "loss": 3.8604,
      "step": 307000
    },
    {
      "epoch": 2628.2051282051284,
      "grad_norm": 5.859671115875244,
      "learning_rate": 2.3717948717948718e-05,
      "loss": 3.8618,
      "step": 307500
    },
    {
      "epoch": 2632.4786324786323,
      "grad_norm": 4.485450267791748,
      "learning_rate": 2.3675213675213675e-05,
      "loss": 3.8626,
      "step": 308000
    },
    {
      "epoch": 2636.7521367521367,
      "grad_norm": 5.470067024230957,
      "learning_rate": 2.3632478632478632e-05,
      "loss": 3.8635,
      "step": 308500
    },
    {
      "epoch": 2641.025641025641,
      "grad_norm": 6.5475592613220215,
      "learning_rate": 2.358974358974359e-05,
      "loss": 3.8648,
      "step": 309000
    },
    {
      "epoch": 2645.2991452991455,
      "grad_norm": 7.974522590637207,
      "learning_rate": 2.3547008547008546e-05,
      "loss": 3.8654,
      "step": 309500
    },
    {
      "epoch": 2649.5726495726494,
      "grad_norm": 6.925262451171875,
      "learning_rate": 2.3504273504273504e-05,
      "loss": 3.8583,
      "step": 310000
    },
    {
      "epoch": 2653.846153846154,
      "grad_norm": 9.865472793579102,
      "learning_rate": 2.3461538461538464e-05,
      "loss": 3.8591,
      "step": 310500
    },
    {
      "epoch": 2658.119658119658,
      "grad_norm": 5.245920181274414,
      "learning_rate": 2.341880341880342e-05,
      "loss": 3.8537,
      "step": 311000
    },
    {
      "epoch": 2662.3931623931626,
      "grad_norm": 7.3860249519348145,
      "learning_rate": 2.337606837606838e-05,
      "loss": 3.8633,
      "step": 311500
    },
    {
      "epoch": 2666.6666666666665,
      "grad_norm": 7.148440837860107,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 3.8617,
      "step": 312000
    },
    {
      "epoch": 2670.940170940171,
      "grad_norm": 5.6234917640686035,
      "learning_rate": 2.3290598290598293e-05,
      "loss": 3.8631,
      "step": 312500
    },
    {
      "epoch": 2675.2136752136753,
      "grad_norm": 6.267797470092773,
      "learning_rate": 2.324786324786325e-05,
      "loss": 3.8642,
      "step": 313000
    },
    {
      "epoch": 2679.4871794871797,
      "grad_norm": 5.403331756591797,
      "learning_rate": 2.3205128205128207e-05,
      "loss": 3.8641,
      "step": 313500
    },
    {
      "epoch": 2683.7606837606836,
      "grad_norm": 7.670177459716797,
      "learning_rate": 2.3162393162393165e-05,
      "loss": 3.867,
      "step": 314000
    },
    {
      "epoch": 2688.034188034188,
      "grad_norm": 6.533986568450928,
      "learning_rate": 2.3119658119658122e-05,
      "loss": 3.86,
      "step": 314500
    },
    {
      "epoch": 2692.3076923076924,
      "grad_norm": 8.16472053527832,
      "learning_rate": 2.307692307692308e-05,
      "loss": 3.8507,
      "step": 315000
    },
    {
      "epoch": 2696.5811965811968,
      "grad_norm": 7.703214645385742,
      "learning_rate": 2.3034188034188033e-05,
      "loss": 3.8611,
      "step": 315500
    },
    {
      "epoch": 2700.8547008547007,
      "grad_norm": 4.493631839752197,
      "learning_rate": 2.299145299145299e-05,
      "loss": 3.8643,
      "step": 316000
    },
    {
      "epoch": 2705.128205128205,
      "grad_norm": 7.96334171295166,
      "learning_rate": 2.2948717948717947e-05,
      "loss": 3.8642,
      "step": 316500
    },
    {
      "epoch": 2709.4017094017095,
      "grad_norm": 7.058523178100586,
      "learning_rate": 2.2905982905982905e-05,
      "loss": 3.8622,
      "step": 317000
    },
    {
      "epoch": 2713.675213675214,
      "grad_norm": 8.571613311767578,
      "learning_rate": 2.2863247863247865e-05,
      "loss": 3.853,
      "step": 317500
    },
    {
      "epoch": 2717.948717948718,
      "grad_norm": 5.830678939819336,
      "learning_rate": 2.2820512820512822e-05,
      "loss": 3.861,
      "step": 318000
    },
    {
      "epoch": 2722.222222222222,
      "grad_norm": 10.849736213684082,
      "learning_rate": 2.277777777777778e-05,
      "loss": 3.8706,
      "step": 318500
    },
    {
      "epoch": 2726.4957264957266,
      "grad_norm": 5.8316755294799805,
      "learning_rate": 2.2735042735042737e-05,
      "loss": 3.8543,
      "step": 319000
    },
    {
      "epoch": 2730.769230769231,
      "grad_norm": 12.347990036010742,
      "learning_rate": 2.2692307692307694e-05,
      "loss": 3.8535,
      "step": 319500
    },
    {
      "epoch": 2735.042735042735,
      "grad_norm": 7.6253862380981445,
      "learning_rate": 2.264957264957265e-05,
      "loss": 3.8545,
      "step": 320000
    },
    {
      "epoch": 2739.3162393162393,
      "grad_norm": 6.5087690353393555,
      "learning_rate": 2.260683760683761e-05,
      "loss": 3.8554,
      "step": 320500
    },
    {
      "epoch": 2743.5897435897436,
      "grad_norm": 5.88273811340332,
      "learning_rate": 2.2564102564102566e-05,
      "loss": 3.8625,
      "step": 321000
    },
    {
      "epoch": 2747.863247863248,
      "grad_norm": 6.2084455490112305,
      "learning_rate": 2.2521367521367523e-05,
      "loss": 3.8689,
      "step": 321500
    },
    {
      "epoch": 2752.136752136752,
      "grad_norm": 6.206622123718262,
      "learning_rate": 2.247863247863248e-05,
      "loss": 3.8669,
      "step": 322000
    },
    {
      "epoch": 2756.4102564102564,
      "grad_norm": 7.245204925537109,
      "learning_rate": 2.2435897435897437e-05,
      "loss": 3.8679,
      "step": 322500
    },
    {
      "epoch": 2760.6837606837607,
      "grad_norm": 9.1981201171875,
      "learning_rate": 2.2393162393162394e-05,
      "loss": 3.8627,
      "step": 323000
    },
    {
      "epoch": 2764.957264957265,
      "grad_norm": 5.100063800811768,
      "learning_rate": 2.235042735042735e-05,
      "loss": 3.8489,
      "step": 323500
    },
    {
      "epoch": 2769.230769230769,
      "grad_norm": 6.808504581451416,
      "learning_rate": 2.230769230769231e-05,
      "loss": 3.8599,
      "step": 324000
    },
    {
      "epoch": 2773.5042735042734,
      "grad_norm": 7.894433498382568,
      "learning_rate": 2.2264957264957266e-05,
      "loss": 3.8573,
      "step": 324500
    },
    {
      "epoch": 2777.777777777778,
      "grad_norm": 5.641517639160156,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 3.8619,
      "step": 325000
    },
    {
      "epoch": 2782.051282051282,
      "grad_norm": 6.035812854766846,
      "learning_rate": 2.217948717948718e-05,
      "loss": 3.8574,
      "step": 325500
    }
  ],
  "logging_steps": 500,
  "max_steps": 585000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5000,
  "save_steps": 14800,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.164659699510707e+16,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
