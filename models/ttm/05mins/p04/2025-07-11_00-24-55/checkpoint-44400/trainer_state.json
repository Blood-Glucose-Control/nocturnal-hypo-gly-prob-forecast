{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 379.4871794871795,
  "eval_steps": 500,
  "global_step": 44400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.273504273504273,
      "grad_norm": 2.1290321350097656,
      "learning_rate": 4.995726495726496e-05,
      "loss": 5.1623,
      "step": 500
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 3.515603542327881,
      "learning_rate": 4.991452991452992e-05,
      "loss": 4.7907,
      "step": 1000
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 5.302112102508545,
      "learning_rate": 4.9871794871794874e-05,
      "loss": 4.6422,
      "step": 1500
    },
    {
      "epoch": 17.094017094017094,
      "grad_norm": 4.725725173950195,
      "learning_rate": 4.982905982905983e-05,
      "loss": 4.5607,
      "step": 2000
    },
    {
      "epoch": 21.367521367521366,
      "grad_norm": 5.004260063171387,
      "learning_rate": 4.978632478632479e-05,
      "loss": 4.5122,
      "step": 2500
    },
    {
      "epoch": 25.641025641025642,
      "grad_norm": 4.929769039154053,
      "learning_rate": 4.9743589743589746e-05,
      "loss": 4.4729,
      "step": 3000
    },
    {
      "epoch": 29.914529914529915,
      "grad_norm": 4.510822296142578,
      "learning_rate": 4.97008547008547e-05,
      "loss": 4.456,
      "step": 3500
    },
    {
      "epoch": 34.18803418803419,
      "grad_norm": 7.050263404846191,
      "learning_rate": 4.965811965811966e-05,
      "loss": 4.4189,
      "step": 4000
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 7.3173418045043945,
      "learning_rate": 4.961538461538462e-05,
      "loss": 4.3964,
      "step": 4500
    },
    {
      "epoch": 42.73504273504273,
      "grad_norm": 8.408453941345215,
      "learning_rate": 4.9572649572649575e-05,
      "loss": 4.3934,
      "step": 5000
    },
    {
      "epoch": 47.00854700854701,
      "grad_norm": 6.575353145599365,
      "learning_rate": 4.952991452991453e-05,
      "loss": 4.3655,
      "step": 5500
    },
    {
      "epoch": 51.282051282051285,
      "grad_norm": 5.145970344543457,
      "learning_rate": 4.948717948717949e-05,
      "loss": 4.3427,
      "step": 6000
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 5.29254150390625,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 4.3254,
      "step": 6500
    },
    {
      "epoch": 59.82905982905983,
      "grad_norm": 13.340841293334961,
      "learning_rate": 4.94017094017094e-05,
      "loss": 4.3108,
      "step": 7000
    },
    {
      "epoch": 64.1025641025641,
      "grad_norm": 7.039699554443359,
      "learning_rate": 4.935897435897436e-05,
      "loss": 4.3092,
      "step": 7500
    },
    {
      "epoch": 68.37606837606837,
      "grad_norm": 8.254450798034668,
      "learning_rate": 4.931623931623932e-05,
      "loss": 4.2853,
      "step": 8000
    },
    {
      "epoch": 72.64957264957265,
      "grad_norm": 10.41873836517334,
      "learning_rate": 4.927350427350428e-05,
      "loss": 4.2847,
      "step": 8500
    },
    {
      "epoch": 76.92307692307692,
      "grad_norm": 6.928525447845459,
      "learning_rate": 4.923076923076924e-05,
      "loss": 4.2722,
      "step": 9000
    },
    {
      "epoch": 81.19658119658119,
      "grad_norm": 7.024401664733887,
      "learning_rate": 4.918803418803419e-05,
      "loss": 4.2597,
      "step": 9500
    },
    {
      "epoch": 85.47008547008546,
      "grad_norm": 7.552577018737793,
      "learning_rate": 4.9145299145299147e-05,
      "loss": 4.2508,
      "step": 10000
    },
    {
      "epoch": 89.74358974358974,
      "grad_norm": 8.843289375305176,
      "learning_rate": 4.9102564102564104e-05,
      "loss": 4.2525,
      "step": 10500
    },
    {
      "epoch": 94.01709401709402,
      "grad_norm": 7.617059230804443,
      "learning_rate": 4.905982905982906e-05,
      "loss": 4.235,
      "step": 11000
    },
    {
      "epoch": 98.2905982905983,
      "grad_norm": 5.512274265289307,
      "learning_rate": 4.901709401709402e-05,
      "loss": 4.2337,
      "step": 11500
    },
    {
      "epoch": 102.56410256410257,
      "grad_norm": 7.132622241973877,
      "learning_rate": 4.8974358974358975e-05,
      "loss": 4.2151,
      "step": 12000
    },
    {
      "epoch": 106.83760683760684,
      "grad_norm": 6.0566534996032715,
      "learning_rate": 4.893162393162393e-05,
      "loss": 4.2214,
      "step": 12500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 12.124021530151367,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.2068,
      "step": 13000
    },
    {
      "epoch": 115.38461538461539,
      "grad_norm": 11.0662202835083,
      "learning_rate": 4.884615384615385e-05,
      "loss": 4.2076,
      "step": 13500
    },
    {
      "epoch": 119.65811965811966,
      "grad_norm": 7.243983745574951,
      "learning_rate": 4.8803418803418804e-05,
      "loss": 4.1974,
      "step": 14000
    },
    {
      "epoch": 123.93162393162393,
      "grad_norm": 7.888822555541992,
      "learning_rate": 4.876068376068376e-05,
      "loss": 4.1813,
      "step": 14500
    },
    {
      "epoch": 128.2051282051282,
      "grad_norm": 7.011981010437012,
      "learning_rate": 4.871794871794872e-05,
      "loss": 4.1851,
      "step": 15000
    },
    {
      "epoch": 132.47863247863248,
      "grad_norm": 6.351913928985596,
      "learning_rate": 4.8675213675213676e-05,
      "loss": 4.1697,
      "step": 15500
    },
    {
      "epoch": 136.75213675213675,
      "grad_norm": 7.328882694244385,
      "learning_rate": 4.863247863247863e-05,
      "loss": 4.1742,
      "step": 16000
    },
    {
      "epoch": 141.02564102564102,
      "grad_norm": 6.786298751831055,
      "learning_rate": 4.858974358974359e-05,
      "loss": 4.1609,
      "step": 16500
    },
    {
      "epoch": 145.2991452991453,
      "grad_norm": 6.474168300628662,
      "learning_rate": 4.854700854700855e-05,
      "loss": 4.1605,
      "step": 17000
    },
    {
      "epoch": 149.57264957264957,
      "grad_norm": 8.576292037963867,
      "learning_rate": 4.8504273504273505e-05,
      "loss": 4.1724,
      "step": 17500
    },
    {
      "epoch": 153.84615384615384,
      "grad_norm": 7.031434059143066,
      "learning_rate": 4.846153846153846e-05,
      "loss": 4.1451,
      "step": 18000
    },
    {
      "epoch": 158.1196581196581,
      "grad_norm": 6.402857780456543,
      "learning_rate": 4.841880341880342e-05,
      "loss": 4.1451,
      "step": 18500
    },
    {
      "epoch": 162.39316239316238,
      "grad_norm": 9.402010917663574,
      "learning_rate": 4.8376068376068376e-05,
      "loss": 4.1437,
      "step": 19000
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 6.044010639190674,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 4.1404,
      "step": 19500
    },
    {
      "epoch": 170.94017094017093,
      "grad_norm": 6.289105415344238,
      "learning_rate": 4.829059829059829e-05,
      "loss": 4.1316,
      "step": 20000
    },
    {
      "epoch": 175.2136752136752,
      "grad_norm": 8.11441421508789,
      "learning_rate": 4.824786324786325e-05,
      "loss": 4.1322,
      "step": 20500
    },
    {
      "epoch": 179.48717948717947,
      "grad_norm": 6.081551551818848,
      "learning_rate": 4.8205128205128205e-05,
      "loss": 4.1295,
      "step": 21000
    },
    {
      "epoch": 183.76068376068375,
      "grad_norm": 11.564292907714844,
      "learning_rate": 4.816239316239316e-05,
      "loss": 4.1294,
      "step": 21500
    },
    {
      "epoch": 188.03418803418805,
      "grad_norm": 6.247604846954346,
      "learning_rate": 4.8119658119658126e-05,
      "loss": 4.109,
      "step": 22000
    },
    {
      "epoch": 192.30769230769232,
      "grad_norm": 6.9817376136779785,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 4.127,
      "step": 22500
    },
    {
      "epoch": 196.5811965811966,
      "grad_norm": 8.155413627624512,
      "learning_rate": 4.803418803418804e-05,
      "loss": 4.1126,
      "step": 23000
    },
    {
      "epoch": 200.85470085470087,
      "grad_norm": 6.863190650939941,
      "learning_rate": 4.7991452991453e-05,
      "loss": 4.1093,
      "step": 23500
    },
    {
      "epoch": 205.12820512820514,
      "grad_norm": 11.453500747680664,
      "learning_rate": 4.7948717948717955e-05,
      "loss": 4.1132,
      "step": 24000
    },
    {
      "epoch": 209.4017094017094,
      "grad_norm": 6.0719475746154785,
      "learning_rate": 4.790598290598291e-05,
      "loss": 4.0981,
      "step": 24500
    },
    {
      "epoch": 213.67521367521368,
      "grad_norm": 7.080769062042236,
      "learning_rate": 4.786324786324787e-05,
      "loss": 4.1045,
      "step": 25000
    },
    {
      "epoch": 217.94871794871796,
      "grad_norm": 7.436689376831055,
      "learning_rate": 4.782051282051283e-05,
      "loss": 4.0937,
      "step": 25500
    },
    {
      "epoch": 222.22222222222223,
      "grad_norm": 5.551352024078369,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 4.0836,
      "step": 26000
    },
    {
      "epoch": 226.4957264957265,
      "grad_norm": 9.403289794921875,
      "learning_rate": 4.773504273504274e-05,
      "loss": 4.0886,
      "step": 26500
    },
    {
      "epoch": 230.76923076923077,
      "grad_norm": 6.150911808013916,
      "learning_rate": 4.76923076923077e-05,
      "loss": 4.0906,
      "step": 27000
    },
    {
      "epoch": 235.04273504273505,
      "grad_norm": 4.876595497131348,
      "learning_rate": 4.764957264957265e-05,
      "loss": 4.0751,
      "step": 27500
    },
    {
      "epoch": 239.31623931623932,
      "grad_norm": 8.254741668701172,
      "learning_rate": 4.7606837606837606e-05,
      "loss": 4.0774,
      "step": 28000
    },
    {
      "epoch": 243.5897435897436,
      "grad_norm": 7.1539177894592285,
      "learning_rate": 4.7564102564102563e-05,
      "loss": 4.0839,
      "step": 28500
    },
    {
      "epoch": 247.86324786324786,
      "grad_norm": 5.6175537109375,
      "learning_rate": 4.752136752136752e-05,
      "loss": 4.0715,
      "step": 29000
    },
    {
      "epoch": 252.13675213675214,
      "grad_norm": 5.4050822257995605,
      "learning_rate": 4.747863247863248e-05,
      "loss": 4.0777,
      "step": 29500
    },
    {
      "epoch": 256.4102564102564,
      "grad_norm": 9.442435264587402,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 4.0747,
      "step": 30000
    },
    {
      "epoch": 260.6837606837607,
      "grad_norm": 8.601881980895996,
      "learning_rate": 4.739316239316239e-05,
      "loss": 4.0625,
      "step": 30500
    },
    {
      "epoch": 264.95726495726495,
      "grad_norm": 7.6944756507873535,
      "learning_rate": 4.735042735042735e-05,
      "loss": 4.0687,
      "step": 31000
    },
    {
      "epoch": 269.2307692307692,
      "grad_norm": 6.2381978034973145,
      "learning_rate": 4.730769230769231e-05,
      "loss": 4.0706,
      "step": 31500
    },
    {
      "epoch": 273.5042735042735,
      "grad_norm": 8.458966255187988,
      "learning_rate": 4.7264957264957264e-05,
      "loss": 4.0671,
      "step": 32000
    },
    {
      "epoch": 277.77777777777777,
      "grad_norm": 6.759262561798096,
      "learning_rate": 4.722222222222222e-05,
      "loss": 4.0666,
      "step": 32500
    },
    {
      "epoch": 282.05128205128204,
      "grad_norm": 6.305822849273682,
      "learning_rate": 4.717948717948718e-05,
      "loss": 4.057,
      "step": 33000
    },
    {
      "epoch": 286.3247863247863,
      "grad_norm": 7.897851943969727,
      "learning_rate": 4.7136752136752136e-05,
      "loss": 4.0568,
      "step": 33500
    },
    {
      "epoch": 290.5982905982906,
      "grad_norm": 6.220949649810791,
      "learning_rate": 4.709401709401709e-05,
      "loss": 4.0512,
      "step": 34000
    },
    {
      "epoch": 294.87179487179486,
      "grad_norm": 6.10500431060791,
      "learning_rate": 4.705128205128205e-05,
      "loss": 4.0621,
      "step": 34500
    },
    {
      "epoch": 299.14529914529913,
      "grad_norm": 5.766659259796143,
      "learning_rate": 4.700854700854701e-05,
      "loss": 4.0478,
      "step": 35000
    },
    {
      "epoch": 303.4188034188034,
      "grad_norm": 6.696651935577393,
      "learning_rate": 4.6965811965811964e-05,
      "loss": 4.0495,
      "step": 35500
    },
    {
      "epoch": 307.6923076923077,
      "grad_norm": 5.952033519744873,
      "learning_rate": 4.692307692307693e-05,
      "loss": 4.0417,
      "step": 36000
    },
    {
      "epoch": 311.96581196581195,
      "grad_norm": 6.345422267913818,
      "learning_rate": 4.6880341880341886e-05,
      "loss": 4.0546,
      "step": 36500
    },
    {
      "epoch": 316.2393162393162,
      "grad_norm": 7.947479248046875,
      "learning_rate": 4.683760683760684e-05,
      "loss": 4.0455,
      "step": 37000
    },
    {
      "epoch": 320.5128205128205,
      "grad_norm": 8.901809692382812,
      "learning_rate": 4.67948717948718e-05,
      "loss": 4.0476,
      "step": 37500
    },
    {
      "epoch": 324.78632478632477,
      "grad_norm": 7.532938480377197,
      "learning_rate": 4.675213675213676e-05,
      "loss": 4.0307,
      "step": 38000
    },
    {
      "epoch": 329.05982905982904,
      "grad_norm": 16.986122131347656,
      "learning_rate": 4.6709401709401714e-05,
      "loss": 4.0339,
      "step": 38500
    },
    {
      "epoch": 333.3333333333333,
      "grad_norm": 5.276901721954346,
      "learning_rate": 4.666666666666667e-05,
      "loss": 4.0361,
      "step": 39000
    },
    {
      "epoch": 337.6068376068376,
      "grad_norm": 7.788668155670166,
      "learning_rate": 4.662393162393163e-05,
      "loss": 4.0336,
      "step": 39500
    },
    {
      "epoch": 341.88034188034186,
      "grad_norm": 6.863504886627197,
      "learning_rate": 4.6581196581196586e-05,
      "loss": 4.0262,
      "step": 40000
    },
    {
      "epoch": 346.15384615384613,
      "grad_norm": 7.218404769897461,
      "learning_rate": 4.653846153846154e-05,
      "loss": 4.0359,
      "step": 40500
    },
    {
      "epoch": 350.4273504273504,
      "grad_norm": 6.467072010040283,
      "learning_rate": 4.64957264957265e-05,
      "loss": 4.0221,
      "step": 41000
    },
    {
      "epoch": 354.7008547008547,
      "grad_norm": 8.331160545349121,
      "learning_rate": 4.645299145299146e-05,
      "loss": 4.0312,
      "step": 41500
    },
    {
      "epoch": 358.97435897435895,
      "grad_norm": 6.1337971687316895,
      "learning_rate": 4.6410256410256415e-05,
      "loss": 4.0274,
      "step": 42000
    },
    {
      "epoch": 363.2478632478632,
      "grad_norm": 5.521655082702637,
      "learning_rate": 4.636752136752137e-05,
      "loss": 4.0245,
      "step": 42500
    },
    {
      "epoch": 367.5213675213675,
      "grad_norm": 6.372340679168701,
      "learning_rate": 4.632478632478633e-05,
      "loss": 4.0253,
      "step": 43000
    },
    {
      "epoch": 371.79487179487177,
      "grad_norm": 7.783965587615967,
      "learning_rate": 4.6282051282051287e-05,
      "loss": 4.026,
      "step": 43500
    },
    {
      "epoch": 376.0683760683761,
      "grad_norm": 6.777711868286133,
      "learning_rate": 4.6239316239316244e-05,
      "loss": 4.0222,
      "step": 44000
    }
  ],
  "logging_steps": 500,
  "max_steps": 585000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5000,
  "save_steps": 14800,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5679099272121984.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
