{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 126.4957264957265,
  "eval_steps": 500,
  "global_step": 14800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.273504273504273,
      "grad_norm": 2.1290321350097656,
      "learning_rate": 4.995726495726496e-05,
      "loss": 5.1623,
      "step": 500
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 3.515603542327881,
      "learning_rate": 4.991452991452992e-05,
      "loss": 4.7907,
      "step": 1000
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 5.302112102508545,
      "learning_rate": 4.9871794871794874e-05,
      "loss": 4.6422,
      "step": 1500
    },
    {
      "epoch": 17.094017094017094,
      "grad_norm": 4.725725173950195,
      "learning_rate": 4.982905982905983e-05,
      "loss": 4.5607,
      "step": 2000
    },
    {
      "epoch": 21.367521367521366,
      "grad_norm": 5.004260063171387,
      "learning_rate": 4.978632478632479e-05,
      "loss": 4.5122,
      "step": 2500
    },
    {
      "epoch": 25.641025641025642,
      "grad_norm": 4.929769039154053,
      "learning_rate": 4.9743589743589746e-05,
      "loss": 4.4729,
      "step": 3000
    },
    {
      "epoch": 29.914529914529915,
      "grad_norm": 4.510822296142578,
      "learning_rate": 4.97008547008547e-05,
      "loss": 4.456,
      "step": 3500
    },
    {
      "epoch": 34.18803418803419,
      "grad_norm": 7.050263404846191,
      "learning_rate": 4.965811965811966e-05,
      "loss": 4.4189,
      "step": 4000
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 7.3173418045043945,
      "learning_rate": 4.961538461538462e-05,
      "loss": 4.3964,
      "step": 4500
    },
    {
      "epoch": 42.73504273504273,
      "grad_norm": 8.408453941345215,
      "learning_rate": 4.9572649572649575e-05,
      "loss": 4.3934,
      "step": 5000
    },
    {
      "epoch": 47.00854700854701,
      "grad_norm": 6.575353145599365,
      "learning_rate": 4.952991452991453e-05,
      "loss": 4.3655,
      "step": 5500
    },
    {
      "epoch": 51.282051282051285,
      "grad_norm": 5.145970344543457,
      "learning_rate": 4.948717948717949e-05,
      "loss": 4.3427,
      "step": 6000
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 5.29254150390625,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 4.3254,
      "step": 6500
    },
    {
      "epoch": 59.82905982905983,
      "grad_norm": 13.340841293334961,
      "learning_rate": 4.94017094017094e-05,
      "loss": 4.3108,
      "step": 7000
    },
    {
      "epoch": 64.1025641025641,
      "grad_norm": 7.039699554443359,
      "learning_rate": 4.935897435897436e-05,
      "loss": 4.3092,
      "step": 7500
    },
    {
      "epoch": 68.37606837606837,
      "grad_norm": 8.254450798034668,
      "learning_rate": 4.931623931623932e-05,
      "loss": 4.2853,
      "step": 8000
    },
    {
      "epoch": 72.64957264957265,
      "grad_norm": 10.41873836517334,
      "learning_rate": 4.927350427350428e-05,
      "loss": 4.2847,
      "step": 8500
    },
    {
      "epoch": 76.92307692307692,
      "grad_norm": 6.928525447845459,
      "learning_rate": 4.923076923076924e-05,
      "loss": 4.2722,
      "step": 9000
    },
    {
      "epoch": 81.19658119658119,
      "grad_norm": 7.024401664733887,
      "learning_rate": 4.918803418803419e-05,
      "loss": 4.2597,
      "step": 9500
    },
    {
      "epoch": 85.47008547008546,
      "grad_norm": 7.552577018737793,
      "learning_rate": 4.9145299145299147e-05,
      "loss": 4.2508,
      "step": 10000
    },
    {
      "epoch": 89.74358974358974,
      "grad_norm": 8.843289375305176,
      "learning_rate": 4.9102564102564104e-05,
      "loss": 4.2525,
      "step": 10500
    },
    {
      "epoch": 94.01709401709402,
      "grad_norm": 7.617059230804443,
      "learning_rate": 4.905982905982906e-05,
      "loss": 4.235,
      "step": 11000
    },
    {
      "epoch": 98.2905982905983,
      "grad_norm": 5.512274265289307,
      "learning_rate": 4.901709401709402e-05,
      "loss": 4.2337,
      "step": 11500
    },
    {
      "epoch": 102.56410256410257,
      "grad_norm": 7.132622241973877,
      "learning_rate": 4.8974358974358975e-05,
      "loss": 4.2151,
      "step": 12000
    },
    {
      "epoch": 106.83760683760684,
      "grad_norm": 6.0566534996032715,
      "learning_rate": 4.893162393162393e-05,
      "loss": 4.2214,
      "step": 12500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 12.124021530151367,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.2068,
      "step": 13000
    },
    {
      "epoch": 115.38461538461539,
      "grad_norm": 11.0662202835083,
      "learning_rate": 4.884615384615385e-05,
      "loss": 4.2076,
      "step": 13500
    },
    {
      "epoch": 119.65811965811966,
      "grad_norm": 7.243983745574951,
      "learning_rate": 4.8803418803418804e-05,
      "loss": 4.1974,
      "step": 14000
    },
    {
      "epoch": 123.93162393162393,
      "grad_norm": 7.888822555541992,
      "learning_rate": 4.876068376068376e-05,
      "loss": 4.1813,
      "step": 14500
    }
  ],
  "logging_steps": 500,
  "max_steps": 585000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5000,
  "save_steps": 14800,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1893049465764096.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
