{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4680.34188034188,
  "eval_steps": 500,
  "global_step": 547600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.273504273504273,
      "grad_norm": 2.1290321350097656,
      "learning_rate": 4.995726495726496e-05,
      "loss": 5.1623,
      "step": 500
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 3.515603542327881,
      "learning_rate": 4.991452991452992e-05,
      "loss": 4.7907,
      "step": 1000
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 5.302112102508545,
      "learning_rate": 4.9871794871794874e-05,
      "loss": 4.6422,
      "step": 1500
    },
    {
      "epoch": 17.094017094017094,
      "grad_norm": 4.725725173950195,
      "learning_rate": 4.982905982905983e-05,
      "loss": 4.5607,
      "step": 2000
    },
    {
      "epoch": 21.367521367521366,
      "grad_norm": 5.004260063171387,
      "learning_rate": 4.978632478632479e-05,
      "loss": 4.5122,
      "step": 2500
    },
    {
      "epoch": 25.641025641025642,
      "grad_norm": 4.929769039154053,
      "learning_rate": 4.9743589743589746e-05,
      "loss": 4.4729,
      "step": 3000
    },
    {
      "epoch": 29.914529914529915,
      "grad_norm": 4.510822296142578,
      "learning_rate": 4.97008547008547e-05,
      "loss": 4.456,
      "step": 3500
    },
    {
      "epoch": 34.18803418803419,
      "grad_norm": 7.050263404846191,
      "learning_rate": 4.965811965811966e-05,
      "loss": 4.4189,
      "step": 4000
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 7.3173418045043945,
      "learning_rate": 4.961538461538462e-05,
      "loss": 4.3964,
      "step": 4500
    },
    {
      "epoch": 42.73504273504273,
      "grad_norm": 8.408453941345215,
      "learning_rate": 4.9572649572649575e-05,
      "loss": 4.3934,
      "step": 5000
    },
    {
      "epoch": 47.00854700854701,
      "grad_norm": 6.575353145599365,
      "learning_rate": 4.952991452991453e-05,
      "loss": 4.3655,
      "step": 5500
    },
    {
      "epoch": 51.282051282051285,
      "grad_norm": 5.145970344543457,
      "learning_rate": 4.948717948717949e-05,
      "loss": 4.3427,
      "step": 6000
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 5.29254150390625,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 4.3254,
      "step": 6500
    },
    {
      "epoch": 59.82905982905983,
      "grad_norm": 13.340841293334961,
      "learning_rate": 4.94017094017094e-05,
      "loss": 4.3108,
      "step": 7000
    },
    {
      "epoch": 64.1025641025641,
      "grad_norm": 7.039699554443359,
      "learning_rate": 4.935897435897436e-05,
      "loss": 4.3092,
      "step": 7500
    },
    {
      "epoch": 68.37606837606837,
      "grad_norm": 8.254450798034668,
      "learning_rate": 4.931623931623932e-05,
      "loss": 4.2853,
      "step": 8000
    },
    {
      "epoch": 72.64957264957265,
      "grad_norm": 10.41873836517334,
      "learning_rate": 4.927350427350428e-05,
      "loss": 4.2847,
      "step": 8500
    },
    {
      "epoch": 76.92307692307692,
      "grad_norm": 6.928525447845459,
      "learning_rate": 4.923076923076924e-05,
      "loss": 4.2722,
      "step": 9000
    },
    {
      "epoch": 81.19658119658119,
      "grad_norm": 7.024401664733887,
      "learning_rate": 4.918803418803419e-05,
      "loss": 4.2597,
      "step": 9500
    },
    {
      "epoch": 85.47008547008546,
      "grad_norm": 7.552577018737793,
      "learning_rate": 4.9145299145299147e-05,
      "loss": 4.2508,
      "step": 10000
    },
    {
      "epoch": 89.74358974358974,
      "grad_norm": 8.843289375305176,
      "learning_rate": 4.9102564102564104e-05,
      "loss": 4.2525,
      "step": 10500
    },
    {
      "epoch": 94.01709401709402,
      "grad_norm": 7.617059230804443,
      "learning_rate": 4.905982905982906e-05,
      "loss": 4.235,
      "step": 11000
    },
    {
      "epoch": 98.2905982905983,
      "grad_norm": 5.512274265289307,
      "learning_rate": 4.901709401709402e-05,
      "loss": 4.2337,
      "step": 11500
    },
    {
      "epoch": 102.56410256410257,
      "grad_norm": 7.132622241973877,
      "learning_rate": 4.8974358974358975e-05,
      "loss": 4.2151,
      "step": 12000
    },
    {
      "epoch": 106.83760683760684,
      "grad_norm": 6.0566534996032715,
      "learning_rate": 4.893162393162393e-05,
      "loss": 4.2214,
      "step": 12500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 12.124021530151367,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.2068,
      "step": 13000
    },
    {
      "epoch": 115.38461538461539,
      "grad_norm": 11.0662202835083,
      "learning_rate": 4.884615384615385e-05,
      "loss": 4.2076,
      "step": 13500
    },
    {
      "epoch": 119.65811965811966,
      "grad_norm": 7.243983745574951,
      "learning_rate": 4.8803418803418804e-05,
      "loss": 4.1974,
      "step": 14000
    },
    {
      "epoch": 123.93162393162393,
      "grad_norm": 7.888822555541992,
      "learning_rate": 4.876068376068376e-05,
      "loss": 4.1813,
      "step": 14500
    },
    {
      "epoch": 128.2051282051282,
      "grad_norm": 7.011981010437012,
      "learning_rate": 4.871794871794872e-05,
      "loss": 4.1851,
      "step": 15000
    },
    {
      "epoch": 132.47863247863248,
      "grad_norm": 6.351913928985596,
      "learning_rate": 4.8675213675213676e-05,
      "loss": 4.1697,
      "step": 15500
    },
    {
      "epoch": 136.75213675213675,
      "grad_norm": 7.328882694244385,
      "learning_rate": 4.863247863247863e-05,
      "loss": 4.1742,
      "step": 16000
    },
    {
      "epoch": 141.02564102564102,
      "grad_norm": 6.786298751831055,
      "learning_rate": 4.858974358974359e-05,
      "loss": 4.1609,
      "step": 16500
    },
    {
      "epoch": 145.2991452991453,
      "grad_norm": 6.474168300628662,
      "learning_rate": 4.854700854700855e-05,
      "loss": 4.1605,
      "step": 17000
    },
    {
      "epoch": 149.57264957264957,
      "grad_norm": 8.576292037963867,
      "learning_rate": 4.8504273504273505e-05,
      "loss": 4.1724,
      "step": 17500
    },
    {
      "epoch": 153.84615384615384,
      "grad_norm": 7.031434059143066,
      "learning_rate": 4.846153846153846e-05,
      "loss": 4.1451,
      "step": 18000
    },
    {
      "epoch": 158.1196581196581,
      "grad_norm": 6.402857780456543,
      "learning_rate": 4.841880341880342e-05,
      "loss": 4.1451,
      "step": 18500
    },
    {
      "epoch": 162.39316239316238,
      "grad_norm": 9.402010917663574,
      "learning_rate": 4.8376068376068376e-05,
      "loss": 4.1437,
      "step": 19000
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 6.044010639190674,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 4.1404,
      "step": 19500
    },
    {
      "epoch": 170.94017094017093,
      "grad_norm": 6.289105415344238,
      "learning_rate": 4.829059829059829e-05,
      "loss": 4.1316,
      "step": 20000
    },
    {
      "epoch": 175.2136752136752,
      "grad_norm": 8.11441421508789,
      "learning_rate": 4.824786324786325e-05,
      "loss": 4.1322,
      "step": 20500
    },
    {
      "epoch": 179.48717948717947,
      "grad_norm": 6.081551551818848,
      "learning_rate": 4.8205128205128205e-05,
      "loss": 4.1295,
      "step": 21000
    },
    {
      "epoch": 183.76068376068375,
      "grad_norm": 11.564292907714844,
      "learning_rate": 4.816239316239316e-05,
      "loss": 4.1294,
      "step": 21500
    },
    {
      "epoch": 188.03418803418805,
      "grad_norm": 6.247604846954346,
      "learning_rate": 4.8119658119658126e-05,
      "loss": 4.109,
      "step": 22000
    },
    {
      "epoch": 192.30769230769232,
      "grad_norm": 6.9817376136779785,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 4.127,
      "step": 22500
    },
    {
      "epoch": 196.5811965811966,
      "grad_norm": 8.155413627624512,
      "learning_rate": 4.803418803418804e-05,
      "loss": 4.1126,
      "step": 23000
    },
    {
      "epoch": 200.85470085470087,
      "grad_norm": 6.863190650939941,
      "learning_rate": 4.7991452991453e-05,
      "loss": 4.1093,
      "step": 23500
    },
    {
      "epoch": 205.12820512820514,
      "grad_norm": 11.453500747680664,
      "learning_rate": 4.7948717948717955e-05,
      "loss": 4.1132,
      "step": 24000
    },
    {
      "epoch": 209.4017094017094,
      "grad_norm": 6.0719475746154785,
      "learning_rate": 4.790598290598291e-05,
      "loss": 4.0981,
      "step": 24500
    },
    {
      "epoch": 213.67521367521368,
      "grad_norm": 7.080769062042236,
      "learning_rate": 4.786324786324787e-05,
      "loss": 4.1045,
      "step": 25000
    },
    {
      "epoch": 217.94871794871796,
      "grad_norm": 7.436689376831055,
      "learning_rate": 4.782051282051283e-05,
      "loss": 4.0937,
      "step": 25500
    },
    {
      "epoch": 222.22222222222223,
      "grad_norm": 5.551352024078369,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 4.0836,
      "step": 26000
    },
    {
      "epoch": 226.4957264957265,
      "grad_norm": 9.403289794921875,
      "learning_rate": 4.773504273504274e-05,
      "loss": 4.0886,
      "step": 26500
    },
    {
      "epoch": 230.76923076923077,
      "grad_norm": 6.150911808013916,
      "learning_rate": 4.76923076923077e-05,
      "loss": 4.0906,
      "step": 27000
    },
    {
      "epoch": 235.04273504273505,
      "grad_norm": 4.876595497131348,
      "learning_rate": 4.764957264957265e-05,
      "loss": 4.0751,
      "step": 27500
    },
    {
      "epoch": 239.31623931623932,
      "grad_norm": 8.254741668701172,
      "learning_rate": 4.7606837606837606e-05,
      "loss": 4.0774,
      "step": 28000
    },
    {
      "epoch": 243.5897435897436,
      "grad_norm": 7.1539177894592285,
      "learning_rate": 4.7564102564102563e-05,
      "loss": 4.0839,
      "step": 28500
    },
    {
      "epoch": 247.86324786324786,
      "grad_norm": 5.6175537109375,
      "learning_rate": 4.752136752136752e-05,
      "loss": 4.0715,
      "step": 29000
    },
    {
      "epoch": 252.13675213675214,
      "grad_norm": 5.4050822257995605,
      "learning_rate": 4.747863247863248e-05,
      "loss": 4.0777,
      "step": 29500
    },
    {
      "epoch": 256.4102564102564,
      "grad_norm": 9.442435264587402,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 4.0747,
      "step": 30000
    },
    {
      "epoch": 260.6837606837607,
      "grad_norm": 8.601881980895996,
      "learning_rate": 4.739316239316239e-05,
      "loss": 4.0625,
      "step": 30500
    },
    {
      "epoch": 264.95726495726495,
      "grad_norm": 7.6944756507873535,
      "learning_rate": 4.735042735042735e-05,
      "loss": 4.0687,
      "step": 31000
    },
    {
      "epoch": 269.2307692307692,
      "grad_norm": 6.2381978034973145,
      "learning_rate": 4.730769230769231e-05,
      "loss": 4.0706,
      "step": 31500
    },
    {
      "epoch": 273.5042735042735,
      "grad_norm": 8.458966255187988,
      "learning_rate": 4.7264957264957264e-05,
      "loss": 4.0671,
      "step": 32000
    },
    {
      "epoch": 277.77777777777777,
      "grad_norm": 6.759262561798096,
      "learning_rate": 4.722222222222222e-05,
      "loss": 4.0666,
      "step": 32500
    },
    {
      "epoch": 282.05128205128204,
      "grad_norm": 6.305822849273682,
      "learning_rate": 4.717948717948718e-05,
      "loss": 4.057,
      "step": 33000
    },
    {
      "epoch": 286.3247863247863,
      "grad_norm": 7.897851943969727,
      "learning_rate": 4.7136752136752136e-05,
      "loss": 4.0568,
      "step": 33500
    },
    {
      "epoch": 290.5982905982906,
      "grad_norm": 6.220949649810791,
      "learning_rate": 4.709401709401709e-05,
      "loss": 4.0512,
      "step": 34000
    },
    {
      "epoch": 294.87179487179486,
      "grad_norm": 6.10500431060791,
      "learning_rate": 4.705128205128205e-05,
      "loss": 4.0621,
      "step": 34500
    },
    {
      "epoch": 299.14529914529913,
      "grad_norm": 5.766659259796143,
      "learning_rate": 4.700854700854701e-05,
      "loss": 4.0478,
      "step": 35000
    },
    {
      "epoch": 303.4188034188034,
      "grad_norm": 6.696651935577393,
      "learning_rate": 4.6965811965811964e-05,
      "loss": 4.0495,
      "step": 35500
    },
    {
      "epoch": 307.6923076923077,
      "grad_norm": 5.952033519744873,
      "learning_rate": 4.692307692307693e-05,
      "loss": 4.0417,
      "step": 36000
    },
    {
      "epoch": 311.96581196581195,
      "grad_norm": 6.345422267913818,
      "learning_rate": 4.6880341880341886e-05,
      "loss": 4.0546,
      "step": 36500
    },
    {
      "epoch": 316.2393162393162,
      "grad_norm": 7.947479248046875,
      "learning_rate": 4.683760683760684e-05,
      "loss": 4.0455,
      "step": 37000
    },
    {
      "epoch": 320.5128205128205,
      "grad_norm": 8.901809692382812,
      "learning_rate": 4.67948717948718e-05,
      "loss": 4.0476,
      "step": 37500
    },
    {
      "epoch": 324.78632478632477,
      "grad_norm": 7.532938480377197,
      "learning_rate": 4.675213675213676e-05,
      "loss": 4.0307,
      "step": 38000
    },
    {
      "epoch": 329.05982905982904,
      "grad_norm": 16.986122131347656,
      "learning_rate": 4.6709401709401714e-05,
      "loss": 4.0339,
      "step": 38500
    },
    {
      "epoch": 333.3333333333333,
      "grad_norm": 5.276901721954346,
      "learning_rate": 4.666666666666667e-05,
      "loss": 4.0361,
      "step": 39000
    },
    {
      "epoch": 337.6068376068376,
      "grad_norm": 7.788668155670166,
      "learning_rate": 4.662393162393163e-05,
      "loss": 4.0336,
      "step": 39500
    },
    {
      "epoch": 341.88034188034186,
      "grad_norm": 6.863504886627197,
      "learning_rate": 4.6581196581196586e-05,
      "loss": 4.0262,
      "step": 40000
    },
    {
      "epoch": 346.15384615384613,
      "grad_norm": 7.218404769897461,
      "learning_rate": 4.653846153846154e-05,
      "loss": 4.0359,
      "step": 40500
    },
    {
      "epoch": 350.4273504273504,
      "grad_norm": 6.467072010040283,
      "learning_rate": 4.64957264957265e-05,
      "loss": 4.0221,
      "step": 41000
    },
    {
      "epoch": 354.7008547008547,
      "grad_norm": 8.331160545349121,
      "learning_rate": 4.645299145299146e-05,
      "loss": 4.0312,
      "step": 41500
    },
    {
      "epoch": 358.97435897435895,
      "grad_norm": 6.1337971687316895,
      "learning_rate": 4.6410256410256415e-05,
      "loss": 4.0274,
      "step": 42000
    },
    {
      "epoch": 363.2478632478632,
      "grad_norm": 5.521655082702637,
      "learning_rate": 4.636752136752137e-05,
      "loss": 4.0245,
      "step": 42500
    },
    {
      "epoch": 367.5213675213675,
      "grad_norm": 6.372340679168701,
      "learning_rate": 4.632478632478633e-05,
      "loss": 4.0253,
      "step": 43000
    },
    {
      "epoch": 371.79487179487177,
      "grad_norm": 7.783965587615967,
      "learning_rate": 4.6282051282051287e-05,
      "loss": 4.026,
      "step": 43500
    },
    {
      "epoch": 376.0683760683761,
      "grad_norm": 6.777711868286133,
      "learning_rate": 4.6239316239316244e-05,
      "loss": 4.0222,
      "step": 44000
    },
    {
      "epoch": 380.34188034188037,
      "grad_norm": 7.369783401489258,
      "learning_rate": 4.61965811965812e-05,
      "loss": 4.0202,
      "step": 44500
    },
    {
      "epoch": 384.61538461538464,
      "grad_norm": 7.54788875579834,
      "learning_rate": 4.615384615384616e-05,
      "loss": 4.0173,
      "step": 45000
    },
    {
      "epoch": 388.8888888888889,
      "grad_norm": 9.773552894592285,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 4.025,
      "step": 45500
    },
    {
      "epoch": 393.1623931623932,
      "grad_norm": 12.026010513305664,
      "learning_rate": 4.6068376068376066e-05,
      "loss": 4.0097,
      "step": 46000
    },
    {
      "epoch": 397.43589743589746,
      "grad_norm": 6.430022716522217,
      "learning_rate": 4.602564102564102e-05,
      "loss": 4.0132,
      "step": 46500
    },
    {
      "epoch": 401.70940170940173,
      "grad_norm": 7.782326698303223,
      "learning_rate": 4.598290598290598e-05,
      "loss": 4.0038,
      "step": 47000
    },
    {
      "epoch": 405.982905982906,
      "grad_norm": 6.288475036621094,
      "learning_rate": 4.594017094017094e-05,
      "loss": 4.0144,
      "step": 47500
    },
    {
      "epoch": 410.2564102564103,
      "grad_norm": 8.162703514099121,
      "learning_rate": 4.5897435897435895e-05,
      "loss": 4.0118,
      "step": 48000
    },
    {
      "epoch": 414.52991452991455,
      "grad_norm": 7.063260555267334,
      "learning_rate": 4.585470085470085e-05,
      "loss": 4.0035,
      "step": 48500
    },
    {
      "epoch": 418.8034188034188,
      "grad_norm": 7.308281421661377,
      "learning_rate": 4.581196581196581e-05,
      "loss": 4.0019,
      "step": 49000
    },
    {
      "epoch": 423.0769230769231,
      "grad_norm": 7.16190767288208,
      "learning_rate": 4.576923076923077e-05,
      "loss": 4.0122,
      "step": 49500
    },
    {
      "epoch": 427.35042735042737,
      "grad_norm": 6.485702991485596,
      "learning_rate": 4.572649572649573e-05,
      "loss": 4.0015,
      "step": 50000
    },
    {
      "epoch": 431.62393162393164,
      "grad_norm": 7.548284530639648,
      "learning_rate": 4.568376068376069e-05,
      "loss": 3.9942,
      "step": 50500
    },
    {
      "epoch": 435.8974358974359,
      "grad_norm": 7.711718559265137,
      "learning_rate": 4.5641025641025645e-05,
      "loss": 3.9966,
      "step": 51000
    },
    {
      "epoch": 440.1709401709402,
      "grad_norm": 8.694419860839844,
      "learning_rate": 4.55982905982906e-05,
      "loss": 4.0033,
      "step": 51500
    },
    {
      "epoch": 444.44444444444446,
      "grad_norm": 9.156994819641113,
      "learning_rate": 4.555555555555556e-05,
      "loss": 3.9963,
      "step": 52000
    },
    {
      "epoch": 448.71794871794873,
      "grad_norm": 6.439909934997559,
      "learning_rate": 4.5512820512820516e-05,
      "loss": 4.0051,
      "step": 52500
    },
    {
      "epoch": 452.991452991453,
      "grad_norm": 6.06203031539917,
      "learning_rate": 4.5470085470085474e-05,
      "loss": 3.9946,
      "step": 53000
    },
    {
      "epoch": 457.2649572649573,
      "grad_norm": 6.1918253898620605,
      "learning_rate": 4.542735042735043e-05,
      "loss": 3.9983,
      "step": 53500
    },
    {
      "epoch": 461.53846153846155,
      "grad_norm": 7.51957368850708,
      "learning_rate": 4.538461538461539e-05,
      "loss": 3.9958,
      "step": 54000
    },
    {
      "epoch": 465.8119658119658,
      "grad_norm": 6.095141887664795,
      "learning_rate": 4.5341880341880345e-05,
      "loss": 3.9925,
      "step": 54500
    },
    {
      "epoch": 470.0854700854701,
      "grad_norm": 8.821395874023438,
      "learning_rate": 4.52991452991453e-05,
      "loss": 4.0011,
      "step": 55000
    },
    {
      "epoch": 474.35897435897436,
      "grad_norm": 8.34216594696045,
      "learning_rate": 4.525641025641026e-05,
      "loss": 3.9966,
      "step": 55500
    },
    {
      "epoch": 478.63247863247864,
      "grad_norm": 6.039234638214111,
      "learning_rate": 4.521367521367522e-05,
      "loss": 3.9946,
      "step": 56000
    },
    {
      "epoch": 482.9059829059829,
      "grad_norm": 6.080371379852295,
      "learning_rate": 4.5170940170940174e-05,
      "loss": 3.995,
      "step": 56500
    },
    {
      "epoch": 487.1794871794872,
      "grad_norm": 8.233863830566406,
      "learning_rate": 4.512820512820513e-05,
      "loss": 3.9836,
      "step": 57000
    },
    {
      "epoch": 491.45299145299145,
      "grad_norm": 7.048008441925049,
      "learning_rate": 4.508547008547009e-05,
      "loss": 3.9871,
      "step": 57500
    },
    {
      "epoch": 495.7264957264957,
      "grad_norm": 5.639457702636719,
      "learning_rate": 4.5042735042735046e-05,
      "loss": 3.9761,
      "step": 58000
    },
    {
      "epoch": 500.0,
      "grad_norm": 9.386116981506348,
      "learning_rate": 4.5e-05,
      "loss": 3.9891,
      "step": 58500
    },
    {
      "epoch": 504.2735042735043,
      "grad_norm": 8.15343189239502,
      "learning_rate": 4.495726495726496e-05,
      "loss": 3.9897,
      "step": 59000
    },
    {
      "epoch": 508.54700854700855,
      "grad_norm": 9.558808326721191,
      "learning_rate": 4.491452991452992e-05,
      "loss": 3.9873,
      "step": 59500
    },
    {
      "epoch": 512.8205128205128,
      "grad_norm": 10.98904037475586,
      "learning_rate": 4.4871794871794874e-05,
      "loss": 3.9831,
      "step": 60000
    },
    {
      "epoch": 517.0940170940171,
      "grad_norm": 7.508718490600586,
      "learning_rate": 4.482905982905983e-05,
      "loss": 3.9728,
      "step": 60500
    },
    {
      "epoch": 521.3675213675214,
      "grad_norm": 8.752313613891602,
      "learning_rate": 4.478632478632479e-05,
      "loss": 3.9839,
      "step": 61000
    },
    {
      "epoch": 525.6410256410256,
      "grad_norm": 6.115170478820801,
      "learning_rate": 4.4743589743589746e-05,
      "loss": 3.9807,
      "step": 61500
    },
    {
      "epoch": 529.9145299145299,
      "grad_norm": 6.219410419464111,
      "learning_rate": 4.47008547008547e-05,
      "loss": 3.9856,
      "step": 62000
    },
    {
      "epoch": 534.1880341880342,
      "grad_norm": 6.511816501617432,
      "learning_rate": 4.465811965811966e-05,
      "loss": 3.9728,
      "step": 62500
    },
    {
      "epoch": 538.4615384615385,
      "grad_norm": 10.061970710754395,
      "learning_rate": 4.461538461538462e-05,
      "loss": 3.9772,
      "step": 63000
    },
    {
      "epoch": 542.7350427350427,
      "grad_norm": 10.08215045928955,
      "learning_rate": 4.4572649572649575e-05,
      "loss": 3.9691,
      "step": 63500
    },
    {
      "epoch": 547.008547008547,
      "grad_norm": 6.054327964782715,
      "learning_rate": 4.452991452991453e-05,
      "loss": 3.9782,
      "step": 64000
    },
    {
      "epoch": 551.2820512820513,
      "grad_norm": 9.000885963439941,
      "learning_rate": 4.448717948717949e-05,
      "loss": 3.9757,
      "step": 64500
    },
    {
      "epoch": 555.5555555555555,
      "grad_norm": 6.924674034118652,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 3.9634,
      "step": 65000
    },
    {
      "epoch": 559.8290598290598,
      "grad_norm": 7.483743190765381,
      "learning_rate": 4.4401709401709404e-05,
      "loss": 3.9728,
      "step": 65500
    },
    {
      "epoch": 564.1025641025641,
      "grad_norm": 5.2609052658081055,
      "learning_rate": 4.435897435897436e-05,
      "loss": 3.9697,
      "step": 66000
    },
    {
      "epoch": 568.3760683760684,
      "grad_norm": 6.079471588134766,
      "learning_rate": 4.431623931623932e-05,
      "loss": 3.9639,
      "step": 66500
    },
    {
      "epoch": 572.6495726495726,
      "grad_norm": 6.238564491271973,
      "learning_rate": 4.4273504273504275e-05,
      "loss": 3.9693,
      "step": 67000
    },
    {
      "epoch": 576.9230769230769,
      "grad_norm": 6.779789447784424,
      "learning_rate": 4.423076923076923e-05,
      "loss": 3.967,
      "step": 67500
    },
    {
      "epoch": 581.1965811965812,
      "grad_norm": 8.94941520690918,
      "learning_rate": 4.418803418803419e-05,
      "loss": 3.9738,
      "step": 68000
    },
    {
      "epoch": 585.4700854700855,
      "grad_norm": 6.445152282714844,
      "learning_rate": 4.414529914529915e-05,
      "loss": 3.9677,
      "step": 68500
    },
    {
      "epoch": 589.7435897435897,
      "grad_norm": 9.103602409362793,
      "learning_rate": 4.4102564102564104e-05,
      "loss": 3.9722,
      "step": 69000
    },
    {
      "epoch": 594.017094017094,
      "grad_norm": 6.753680229187012,
      "learning_rate": 4.405982905982906e-05,
      "loss": 3.9565,
      "step": 69500
    },
    {
      "epoch": 598.2905982905983,
      "grad_norm": 6.498974800109863,
      "learning_rate": 4.401709401709402e-05,
      "loss": 3.9653,
      "step": 70000
    },
    {
      "epoch": 602.5641025641025,
      "grad_norm": 6.302903652191162,
      "learning_rate": 4.3974358974358976e-05,
      "loss": 3.9772,
      "step": 70500
    },
    {
      "epoch": 606.8376068376068,
      "grad_norm": 8.50960922241211,
      "learning_rate": 4.393162393162393e-05,
      "loss": 3.969,
      "step": 71000
    },
    {
      "epoch": 611.1111111111111,
      "grad_norm": 11.961792945861816,
      "learning_rate": 4.388888888888889e-05,
      "loss": 3.9646,
      "step": 71500
    },
    {
      "epoch": 615.3846153846154,
      "grad_norm": 7.538358211517334,
      "learning_rate": 4.384615384615385e-05,
      "loss": 3.9644,
      "step": 72000
    },
    {
      "epoch": 619.6581196581196,
      "grad_norm": 6.098117351531982,
      "learning_rate": 4.3803418803418805e-05,
      "loss": 3.9676,
      "step": 72500
    },
    {
      "epoch": 623.9316239316239,
      "grad_norm": 6.95728063583374,
      "learning_rate": 4.376068376068376e-05,
      "loss": 3.9575,
      "step": 73000
    },
    {
      "epoch": 628.2051282051282,
      "grad_norm": 7.169442653656006,
      "learning_rate": 4.371794871794872e-05,
      "loss": 3.965,
      "step": 73500
    },
    {
      "epoch": 632.4786324786324,
      "grad_norm": 7.487086772918701,
      "learning_rate": 4.3675213675213676e-05,
      "loss": 3.9607,
      "step": 74000
    },
    {
      "epoch": 636.7521367521367,
      "grad_norm": 8.206954002380371,
      "learning_rate": 4.3632478632478634e-05,
      "loss": 3.9575,
      "step": 74500
    },
    {
      "epoch": 641.025641025641,
      "grad_norm": 8.36374568939209,
      "learning_rate": 4.358974358974359e-05,
      "loss": 3.9674,
      "step": 75000
    },
    {
      "epoch": 645.2991452991453,
      "grad_norm": 8.200075149536133,
      "learning_rate": 4.354700854700855e-05,
      "loss": 3.9613,
      "step": 75500
    },
    {
      "epoch": 649.5726495726495,
      "grad_norm": 7.155755519866943,
      "learning_rate": 4.3504273504273505e-05,
      "loss": 3.9597,
      "step": 76000
    },
    {
      "epoch": 653.8461538461538,
      "grad_norm": 8.682790756225586,
      "learning_rate": 4.346153846153846e-05,
      "loss": 3.9581,
      "step": 76500
    },
    {
      "epoch": 658.1196581196581,
      "grad_norm": 10.735983848571777,
      "learning_rate": 4.341880341880342e-05,
      "loss": 3.9679,
      "step": 77000
    },
    {
      "epoch": 662.3931623931624,
      "grad_norm": 7.385895252227783,
      "learning_rate": 4.337606837606838e-05,
      "loss": 3.9498,
      "step": 77500
    },
    {
      "epoch": 666.6666666666666,
      "grad_norm": 5.647291660308838,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 3.9591,
      "step": 78000
    },
    {
      "epoch": 670.9401709401709,
      "grad_norm": 5.892566680908203,
      "learning_rate": 4.329059829059829e-05,
      "loss": 3.9534,
      "step": 78500
    },
    {
      "epoch": 675.2136752136752,
      "grad_norm": 6.375795841217041,
      "learning_rate": 4.324786324786325e-05,
      "loss": 3.9556,
      "step": 79000
    },
    {
      "epoch": 679.4871794871794,
      "grad_norm": 7.242469787597656,
      "learning_rate": 4.320512820512821e-05,
      "loss": 3.9609,
      "step": 79500
    },
    {
      "epoch": 683.7606837606837,
      "grad_norm": 10.83326244354248,
      "learning_rate": 4.316239316239317e-05,
      "loss": 3.9667,
      "step": 80000
    },
    {
      "epoch": 688.034188034188,
      "grad_norm": 8.191494941711426,
      "learning_rate": 4.311965811965813e-05,
      "loss": 3.9398,
      "step": 80500
    },
    {
      "epoch": 692.3076923076923,
      "grad_norm": 8.680912971496582,
      "learning_rate": 4.3076923076923084e-05,
      "loss": 3.9568,
      "step": 81000
    },
    {
      "epoch": 696.5811965811965,
      "grad_norm": 8.823606491088867,
      "learning_rate": 4.303418803418804e-05,
      "loss": 3.9554,
      "step": 81500
    },
    {
      "epoch": 700.8547008547008,
      "grad_norm": 7.222215175628662,
      "learning_rate": 4.2991452991453e-05,
      "loss": 3.9546,
      "step": 82000
    },
    {
      "epoch": 705.1282051282051,
      "grad_norm": 5.584237098693848,
      "learning_rate": 4.294871794871795e-05,
      "loss": 3.9513,
      "step": 82500
    },
    {
      "epoch": 709.4017094017094,
      "grad_norm": 7.259988784790039,
      "learning_rate": 4.2905982905982906e-05,
      "loss": 3.938,
      "step": 83000
    },
    {
      "epoch": 713.6752136752136,
      "grad_norm": 5.676225662231445,
      "learning_rate": 4.286324786324786e-05,
      "loss": 3.9578,
      "step": 83500
    },
    {
      "epoch": 717.9487179487179,
      "grad_norm": 7.094985485076904,
      "learning_rate": 4.282051282051282e-05,
      "loss": 3.9449,
      "step": 84000
    },
    {
      "epoch": 722.2222222222222,
      "grad_norm": 10.262001991271973,
      "learning_rate": 4.277777777777778e-05,
      "loss": 3.9453,
      "step": 84500
    },
    {
      "epoch": 726.4957264957264,
      "grad_norm": 5.974347114562988,
      "learning_rate": 4.2735042735042735e-05,
      "loss": 3.9486,
      "step": 85000
    },
    {
      "epoch": 730.7692307692307,
      "grad_norm": 9.638751983642578,
      "learning_rate": 4.269230769230769e-05,
      "loss": 3.9465,
      "step": 85500
    },
    {
      "epoch": 735.042735042735,
      "grad_norm": 6.363375663757324,
      "learning_rate": 4.264957264957265e-05,
      "loss": 3.9397,
      "step": 86000
    },
    {
      "epoch": 739.3162393162393,
      "grad_norm": 7.853148937225342,
      "learning_rate": 4.260683760683761e-05,
      "loss": 3.9521,
      "step": 86500
    },
    {
      "epoch": 743.5897435897435,
      "grad_norm": 6.903509140014648,
      "learning_rate": 4.2564102564102564e-05,
      "loss": 3.9447,
      "step": 87000
    },
    {
      "epoch": 747.8632478632478,
      "grad_norm": 7.012662887573242,
      "learning_rate": 4.252136752136752e-05,
      "loss": 3.9438,
      "step": 87500
    },
    {
      "epoch": 752.1367521367522,
      "grad_norm": 6.141195297241211,
      "learning_rate": 4.247863247863248e-05,
      "loss": 3.9458,
      "step": 88000
    },
    {
      "epoch": 756.4102564102565,
      "grad_norm": 6.218745231628418,
      "learning_rate": 4.2435897435897435e-05,
      "loss": 3.9381,
      "step": 88500
    },
    {
      "epoch": 760.6837606837607,
      "grad_norm": 5.906060218811035,
      "learning_rate": 4.239316239316239e-05,
      "loss": 3.949,
      "step": 89000
    },
    {
      "epoch": 764.957264957265,
      "grad_norm": 5.518407821655273,
      "learning_rate": 4.235042735042735e-05,
      "loss": 3.9394,
      "step": 89500
    },
    {
      "epoch": 769.2307692307693,
      "grad_norm": 6.059779644012451,
      "learning_rate": 4.230769230769231e-05,
      "loss": 3.9358,
      "step": 90000
    },
    {
      "epoch": 773.5042735042736,
      "grad_norm": 9.136564254760742,
      "learning_rate": 4.2264957264957264e-05,
      "loss": 3.9389,
      "step": 90500
    },
    {
      "epoch": 777.7777777777778,
      "grad_norm": 9.33652400970459,
      "learning_rate": 4.222222222222222e-05,
      "loss": 3.9354,
      "step": 91000
    },
    {
      "epoch": 782.0512820512821,
      "grad_norm": 8.501041412353516,
      "learning_rate": 4.217948717948718e-05,
      "loss": 3.9363,
      "step": 91500
    },
    {
      "epoch": 786.3247863247864,
      "grad_norm": 4.7778191566467285,
      "learning_rate": 4.2136752136752136e-05,
      "loss": 3.9381,
      "step": 92000
    },
    {
      "epoch": 790.5982905982906,
      "grad_norm": 6.496784210205078,
      "learning_rate": 4.209401709401709e-05,
      "loss": 3.9323,
      "step": 92500
    },
    {
      "epoch": 794.8717948717949,
      "grad_norm": 7.8700408935546875,
      "learning_rate": 4.205128205128206e-05,
      "loss": 3.9355,
      "step": 93000
    },
    {
      "epoch": 799.1452991452992,
      "grad_norm": 6.194124698638916,
      "learning_rate": 4.2008547008547014e-05,
      "loss": 3.9306,
      "step": 93500
    },
    {
      "epoch": 803.4188034188035,
      "grad_norm": 6.3413615226745605,
      "learning_rate": 4.196581196581197e-05,
      "loss": 3.9412,
      "step": 94000
    },
    {
      "epoch": 807.6923076923077,
      "grad_norm": 6.7048563957214355,
      "learning_rate": 4.192307692307693e-05,
      "loss": 3.9396,
      "step": 94500
    },
    {
      "epoch": 811.965811965812,
      "grad_norm": 6.426498889923096,
      "learning_rate": 4.1880341880341886e-05,
      "loss": 3.9315,
      "step": 95000
    },
    {
      "epoch": 816.2393162393163,
      "grad_norm": 5.19972562789917,
      "learning_rate": 4.183760683760684e-05,
      "loss": 3.9389,
      "step": 95500
    },
    {
      "epoch": 820.5128205128206,
      "grad_norm": 7.129374980926514,
      "learning_rate": 4.17948717948718e-05,
      "loss": 3.9383,
      "step": 96000
    },
    {
      "epoch": 824.7863247863248,
      "grad_norm": 6.120849132537842,
      "learning_rate": 4.175213675213676e-05,
      "loss": 3.9424,
      "step": 96500
    },
    {
      "epoch": 829.0598290598291,
      "grad_norm": 6.235012054443359,
      "learning_rate": 4.1709401709401715e-05,
      "loss": 3.9305,
      "step": 97000
    },
    {
      "epoch": 833.3333333333334,
      "grad_norm": 9.285923957824707,
      "learning_rate": 4.166666666666667e-05,
      "loss": 3.9338,
      "step": 97500
    },
    {
      "epoch": 837.6068376068376,
      "grad_norm": 6.395662307739258,
      "learning_rate": 4.162393162393163e-05,
      "loss": 3.9276,
      "step": 98000
    },
    {
      "epoch": 841.8803418803419,
      "grad_norm": 5.993895053863525,
      "learning_rate": 4.1581196581196586e-05,
      "loss": 3.9429,
      "step": 98500
    },
    {
      "epoch": 846.1538461538462,
      "grad_norm": 7.645120143890381,
      "learning_rate": 4.1538461538461544e-05,
      "loss": 3.9358,
      "step": 99000
    },
    {
      "epoch": 850.4273504273505,
      "grad_norm": 6.279493808746338,
      "learning_rate": 4.14957264957265e-05,
      "loss": 3.9238,
      "step": 99500
    },
    {
      "epoch": 854.7008547008547,
      "grad_norm": 6.811514854431152,
      "learning_rate": 4.145299145299146e-05,
      "loss": 3.9283,
      "step": 100000
    },
    {
      "epoch": 858.974358974359,
      "grad_norm": 7.746912002563477,
      "learning_rate": 4.1410256410256415e-05,
      "loss": 3.9287,
      "step": 100500
    },
    {
      "epoch": 863.2478632478633,
      "grad_norm": 6.898199081420898,
      "learning_rate": 4.1367521367521366e-05,
      "loss": 3.9296,
      "step": 101000
    },
    {
      "epoch": 867.5213675213676,
      "grad_norm": 6.560009479522705,
      "learning_rate": 4.132478632478632e-05,
      "loss": 3.919,
      "step": 101500
    },
    {
      "epoch": 871.7948717948718,
      "grad_norm": 6.428352355957031,
      "learning_rate": 4.128205128205128e-05,
      "loss": 3.9189,
      "step": 102000
    },
    {
      "epoch": 876.0683760683761,
      "grad_norm": 8.228758811950684,
      "learning_rate": 4.123931623931624e-05,
      "loss": 3.9226,
      "step": 102500
    },
    {
      "epoch": 880.3418803418804,
      "grad_norm": 7.808410167694092,
      "learning_rate": 4.1196581196581195e-05,
      "loss": 3.9372,
      "step": 103000
    },
    {
      "epoch": 884.6153846153846,
      "grad_norm": 8.02485466003418,
      "learning_rate": 4.115384615384615e-05,
      "loss": 3.9231,
      "step": 103500
    },
    {
      "epoch": 888.8888888888889,
      "grad_norm": 7.0687761306762695,
      "learning_rate": 4.111111111111111e-05,
      "loss": 3.9294,
      "step": 104000
    },
    {
      "epoch": 893.1623931623932,
      "grad_norm": 6.921252727508545,
      "learning_rate": 4.1068376068376066e-05,
      "loss": 3.9366,
      "step": 104500
    },
    {
      "epoch": 897.4358974358975,
      "grad_norm": 6.633902549743652,
      "learning_rate": 4.1025641025641023e-05,
      "loss": 3.9292,
      "step": 105000
    },
    {
      "epoch": 901.7094017094017,
      "grad_norm": 5.694268226623535,
      "learning_rate": 4.098290598290598e-05,
      "loss": 3.9244,
      "step": 105500
    },
    {
      "epoch": 905.982905982906,
      "grad_norm": 10.882898330688477,
      "learning_rate": 4.094017094017094e-05,
      "loss": 3.9203,
      "step": 106000
    },
    {
      "epoch": 910.2564102564103,
      "grad_norm": 7.610199928283691,
      "learning_rate": 4.0897435897435895e-05,
      "loss": 3.9291,
      "step": 106500
    },
    {
      "epoch": 914.5299145299145,
      "grad_norm": 5.436959266662598,
      "learning_rate": 4.085470085470086e-05,
      "loss": 3.9221,
      "step": 107000
    },
    {
      "epoch": 918.8034188034188,
      "grad_norm": 7.352177143096924,
      "learning_rate": 4.0811965811965816e-05,
      "loss": 3.9291,
      "step": 107500
    },
    {
      "epoch": 923.0769230769231,
      "grad_norm": 8.29963207244873,
      "learning_rate": 4.0769230769230773e-05,
      "loss": 3.9245,
      "step": 108000
    },
    {
      "epoch": 927.3504273504274,
      "grad_norm": 6.81034517288208,
      "learning_rate": 4.072649572649573e-05,
      "loss": 3.9245,
      "step": 108500
    },
    {
      "epoch": 931.6239316239316,
      "grad_norm": 7.053950309753418,
      "learning_rate": 4.068376068376069e-05,
      "loss": 3.9278,
      "step": 109000
    },
    {
      "epoch": 935.8974358974359,
      "grad_norm": 9.440195083618164,
      "learning_rate": 4.0641025641025645e-05,
      "loss": 3.9248,
      "step": 109500
    },
    {
      "epoch": 940.1709401709402,
      "grad_norm": 5.570390224456787,
      "learning_rate": 4.05982905982906e-05,
      "loss": 3.9158,
      "step": 110000
    },
    {
      "epoch": 944.4444444444445,
      "grad_norm": 7.22826623916626,
      "learning_rate": 4.055555555555556e-05,
      "loss": 3.9263,
      "step": 110500
    },
    {
      "epoch": 948.7179487179487,
      "grad_norm": 4.857020854949951,
      "learning_rate": 4.051282051282052e-05,
      "loss": 3.9174,
      "step": 111000
    },
    {
      "epoch": 952.991452991453,
      "grad_norm": 5.927299976348877,
      "learning_rate": 4.0470085470085474e-05,
      "loss": 3.9181,
      "step": 111500
    },
    {
      "epoch": 957.2649572649573,
      "grad_norm": 7.000720977783203,
      "learning_rate": 4.042735042735043e-05,
      "loss": 3.9163,
      "step": 112000
    },
    {
      "epoch": 961.5384615384615,
      "grad_norm": 8.2637357711792,
      "learning_rate": 4.038461538461539e-05,
      "loss": 3.9199,
      "step": 112500
    },
    {
      "epoch": 965.8119658119658,
      "grad_norm": 8.145536422729492,
      "learning_rate": 4.0341880341880346e-05,
      "loss": 3.9121,
      "step": 113000
    },
    {
      "epoch": 970.0854700854701,
      "grad_norm": 6.233683109283447,
      "learning_rate": 4.02991452991453e-05,
      "loss": 3.9259,
      "step": 113500
    },
    {
      "epoch": 974.3589743589744,
      "grad_norm": 8.074170112609863,
      "learning_rate": 4.025641025641026e-05,
      "loss": 3.9245,
      "step": 114000
    },
    {
      "epoch": 978.6324786324786,
      "grad_norm": 5.164620399475098,
      "learning_rate": 4.021367521367522e-05,
      "loss": 3.913,
      "step": 114500
    },
    {
      "epoch": 982.9059829059829,
      "grad_norm": 5.799078941345215,
      "learning_rate": 4.0170940170940174e-05,
      "loss": 3.9199,
      "step": 115000
    },
    {
      "epoch": 987.1794871794872,
      "grad_norm": 7.7591094970703125,
      "learning_rate": 4.012820512820513e-05,
      "loss": 3.92,
      "step": 115500
    },
    {
      "epoch": 991.4529914529915,
      "grad_norm": 10.754416465759277,
      "learning_rate": 4.008547008547009e-05,
      "loss": 3.9146,
      "step": 116000
    },
    {
      "epoch": 995.7264957264957,
      "grad_norm": 7.450811862945557,
      "learning_rate": 4.0042735042735046e-05,
      "loss": 3.9072,
      "step": 116500
    },
    {
      "epoch": 1000.0,
      "grad_norm": 7.534578323364258,
      "learning_rate": 4e-05,
      "loss": 3.9179,
      "step": 117000
    },
    {
      "epoch": 1004.2735042735043,
      "grad_norm": 16.66469955444336,
      "learning_rate": 3.995726495726496e-05,
      "loss": 3.9141,
      "step": 117500
    },
    {
      "epoch": 1008.5470085470085,
      "grad_norm": 7.125945568084717,
      "learning_rate": 3.991452991452992e-05,
      "loss": 3.9186,
      "step": 118000
    },
    {
      "epoch": 1012.8205128205128,
      "grad_norm": 7.861736297607422,
      "learning_rate": 3.9871794871794875e-05,
      "loss": 3.9229,
      "step": 118500
    },
    {
      "epoch": 1017.0940170940171,
      "grad_norm": 5.922479629516602,
      "learning_rate": 3.9829059829059825e-05,
      "loss": 3.9227,
      "step": 119000
    },
    {
      "epoch": 1021.3675213675214,
      "grad_norm": 6.924526214599609,
      "learning_rate": 3.978632478632478e-05,
      "loss": 3.9177,
      "step": 119500
    },
    {
      "epoch": 1025.6410256410256,
      "grad_norm": 5.243896484375,
      "learning_rate": 3.974358974358974e-05,
      "loss": 3.9148,
      "step": 120000
    },
    {
      "epoch": 1029.91452991453,
      "grad_norm": 10.86159610748291,
      "learning_rate": 3.9700854700854704e-05,
      "loss": 3.9138,
      "step": 120500
    },
    {
      "epoch": 1034.1880341880342,
      "grad_norm": 7.111548900604248,
      "learning_rate": 3.965811965811966e-05,
      "loss": 3.9144,
      "step": 121000
    },
    {
      "epoch": 1038.4615384615386,
      "grad_norm": 6.929369926452637,
      "learning_rate": 3.961538461538462e-05,
      "loss": 3.9139,
      "step": 121500
    },
    {
      "epoch": 1042.7350427350427,
      "grad_norm": 6.078151226043701,
      "learning_rate": 3.9572649572649575e-05,
      "loss": 3.9295,
      "step": 122000
    },
    {
      "epoch": 1047.008547008547,
      "grad_norm": 7.018303394317627,
      "learning_rate": 3.952991452991453e-05,
      "loss": 3.9102,
      "step": 122500
    },
    {
      "epoch": 1051.2820512820513,
      "grad_norm": 4.660222053527832,
      "learning_rate": 3.948717948717949e-05,
      "loss": 3.9117,
      "step": 123000
    },
    {
      "epoch": 1055.5555555555557,
      "grad_norm": 5.0254340171813965,
      "learning_rate": 3.944444444444445e-05,
      "loss": 3.9162,
      "step": 123500
    },
    {
      "epoch": 1059.8290598290598,
      "grad_norm": 6.977751731872559,
      "learning_rate": 3.9401709401709404e-05,
      "loss": 3.9245,
      "step": 124000
    },
    {
      "epoch": 1064.1025641025642,
      "grad_norm": 7.983242034912109,
      "learning_rate": 3.935897435897436e-05,
      "loss": 3.9167,
      "step": 124500
    },
    {
      "epoch": 1068.3760683760684,
      "grad_norm": 5.584043502807617,
      "learning_rate": 3.931623931623932e-05,
      "loss": 3.9082,
      "step": 125000
    },
    {
      "epoch": 1072.6495726495727,
      "grad_norm": 6.646012783050537,
      "learning_rate": 3.9273504273504276e-05,
      "loss": 3.9206,
      "step": 125500
    },
    {
      "epoch": 1076.923076923077,
      "grad_norm": 6.884326457977295,
      "learning_rate": 3.923076923076923e-05,
      "loss": 3.9088,
      "step": 126000
    },
    {
      "epoch": 1081.1965811965813,
      "grad_norm": 6.15372896194458,
      "learning_rate": 3.918803418803419e-05,
      "loss": 3.9167,
      "step": 126500
    },
    {
      "epoch": 1085.4700854700855,
      "grad_norm": 20.107873916625977,
      "learning_rate": 3.914529914529915e-05,
      "loss": 3.9011,
      "step": 127000
    },
    {
      "epoch": 1089.7435897435898,
      "grad_norm": 5.489475727081299,
      "learning_rate": 3.9102564102564105e-05,
      "loss": 3.9093,
      "step": 127500
    },
    {
      "epoch": 1094.017094017094,
      "grad_norm": 6.812025547027588,
      "learning_rate": 3.905982905982906e-05,
      "loss": 3.9111,
      "step": 128000
    },
    {
      "epoch": 1098.2905982905984,
      "grad_norm": 6.0143818855285645,
      "learning_rate": 3.901709401709402e-05,
      "loss": 3.9121,
      "step": 128500
    },
    {
      "epoch": 1102.5641025641025,
      "grad_norm": 5.944926738739014,
      "learning_rate": 3.8974358974358976e-05,
      "loss": 3.9079,
      "step": 129000
    },
    {
      "epoch": 1106.837606837607,
      "grad_norm": 10.64007568359375,
      "learning_rate": 3.8931623931623934e-05,
      "loss": 3.9069,
      "step": 129500
    },
    {
      "epoch": 1111.111111111111,
      "grad_norm": 5.982423305511475,
      "learning_rate": 3.888888888888889e-05,
      "loss": 3.9038,
      "step": 130000
    },
    {
      "epoch": 1115.3846153846155,
      "grad_norm": 6.333535194396973,
      "learning_rate": 3.884615384615385e-05,
      "loss": 3.9193,
      "step": 130500
    },
    {
      "epoch": 1119.6581196581196,
      "grad_norm": 6.67844295501709,
      "learning_rate": 3.8803418803418805e-05,
      "loss": 3.9087,
      "step": 131000
    },
    {
      "epoch": 1123.931623931624,
      "grad_norm": 7.960541725158691,
      "learning_rate": 3.876068376068376e-05,
      "loss": 3.9105,
      "step": 131500
    },
    {
      "epoch": 1128.2051282051282,
      "grad_norm": 5.816122055053711,
      "learning_rate": 3.871794871794872e-05,
      "loss": 3.9058,
      "step": 132000
    },
    {
      "epoch": 1132.4786324786326,
      "grad_norm": 9.33767318725586,
      "learning_rate": 3.867521367521368e-05,
      "loss": 3.9209,
      "step": 132500
    },
    {
      "epoch": 1136.7521367521367,
      "grad_norm": 6.790988922119141,
      "learning_rate": 3.8632478632478634e-05,
      "loss": 3.8998,
      "step": 133000
    },
    {
      "epoch": 1141.025641025641,
      "grad_norm": 5.650710105895996,
      "learning_rate": 3.858974358974359e-05,
      "loss": 3.9121,
      "step": 133500
    },
    {
      "epoch": 1145.2991452991453,
      "grad_norm": 6.9959306716918945,
      "learning_rate": 3.854700854700855e-05,
      "loss": 3.9022,
      "step": 134000
    },
    {
      "epoch": 1149.5726495726497,
      "grad_norm": 5.570718288421631,
      "learning_rate": 3.8504273504273506e-05,
      "loss": 3.9066,
      "step": 134500
    },
    {
      "epoch": 1153.8461538461538,
      "grad_norm": 6.652150630950928,
      "learning_rate": 3.846153846153846e-05,
      "loss": 3.9089,
      "step": 135000
    },
    {
      "epoch": 1158.1196581196582,
      "grad_norm": 5.155848979949951,
      "learning_rate": 3.841880341880342e-05,
      "loss": 3.8993,
      "step": 135500
    },
    {
      "epoch": 1162.3931623931624,
      "grad_norm": 9.795127868652344,
      "learning_rate": 3.837606837606838e-05,
      "loss": 3.9111,
      "step": 136000
    },
    {
      "epoch": 1166.6666666666667,
      "grad_norm": 9.439269065856934,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 3.8982,
      "step": 136500
    },
    {
      "epoch": 1170.940170940171,
      "grad_norm": 9.015193939208984,
      "learning_rate": 3.82905982905983e-05,
      "loss": 3.9115,
      "step": 137000
    },
    {
      "epoch": 1175.2136752136753,
      "grad_norm": 6.734692573547363,
      "learning_rate": 3.824786324786325e-05,
      "loss": 3.9039,
      "step": 137500
    },
    {
      "epoch": 1179.4871794871794,
      "grad_norm": 7.4832892417907715,
      "learning_rate": 3.8205128205128206e-05,
      "loss": 3.9109,
      "step": 138000
    },
    {
      "epoch": 1183.7606837606838,
      "grad_norm": 8.61874008178711,
      "learning_rate": 3.816239316239316e-05,
      "loss": 3.9076,
      "step": 138500
    },
    {
      "epoch": 1188.034188034188,
      "grad_norm": 7.006282329559326,
      "learning_rate": 3.811965811965812e-05,
      "loss": 3.9124,
      "step": 139000
    },
    {
      "epoch": 1192.3076923076924,
      "grad_norm": 6.468670845031738,
      "learning_rate": 3.807692307692308e-05,
      "loss": 3.8984,
      "step": 139500
    },
    {
      "epoch": 1196.5811965811965,
      "grad_norm": 7.2872796058654785,
      "learning_rate": 3.8034188034188035e-05,
      "loss": 3.902,
      "step": 140000
    },
    {
      "epoch": 1200.854700854701,
      "grad_norm": 8.148943901062012,
      "learning_rate": 3.799145299145299e-05,
      "loss": 3.9057,
      "step": 140500
    },
    {
      "epoch": 1205.128205128205,
      "grad_norm": 5.326008319854736,
      "learning_rate": 3.794871794871795e-05,
      "loss": 3.9044,
      "step": 141000
    },
    {
      "epoch": 1209.4017094017095,
      "grad_norm": 8.578001976013184,
      "learning_rate": 3.7905982905982907e-05,
      "loss": 3.9028,
      "step": 141500
    },
    {
      "epoch": 1213.6752136752136,
      "grad_norm": 9.740901947021484,
      "learning_rate": 3.7863247863247864e-05,
      "loss": 3.898,
      "step": 142000
    },
    {
      "epoch": 1217.948717948718,
      "grad_norm": 7.2452850341796875,
      "learning_rate": 3.782051282051282e-05,
      "loss": 3.9036,
      "step": 142500
    },
    {
      "epoch": 1222.2222222222222,
      "grad_norm": 6.877384185791016,
      "learning_rate": 3.777777777777778e-05,
      "loss": 3.8933,
      "step": 143000
    },
    {
      "epoch": 1226.4957264957266,
      "grad_norm": 12.606060028076172,
      "learning_rate": 3.7735042735042735e-05,
      "loss": 3.9141,
      "step": 143500
    },
    {
      "epoch": 1230.7692307692307,
      "grad_norm": 7.142357349395752,
      "learning_rate": 3.769230769230769e-05,
      "loss": 3.8981,
      "step": 144000
    },
    {
      "epoch": 1235.042735042735,
      "grad_norm": 5.758839130401611,
      "learning_rate": 3.764957264957265e-05,
      "loss": 3.8986,
      "step": 144500
    },
    {
      "epoch": 1239.3162393162393,
      "grad_norm": 6.188665866851807,
      "learning_rate": 3.760683760683761e-05,
      "loss": 3.9017,
      "step": 145000
    },
    {
      "epoch": 1243.5897435897436,
      "grad_norm": 6.873031139373779,
      "learning_rate": 3.7564102564102564e-05,
      "loss": 3.9071,
      "step": 145500
    },
    {
      "epoch": 1247.8632478632478,
      "grad_norm": 6.409467697143555,
      "learning_rate": 3.752136752136752e-05,
      "loss": 3.9073,
      "step": 146000
    },
    {
      "epoch": 1252.1367521367522,
      "grad_norm": 9.185282707214355,
      "learning_rate": 3.747863247863248e-05,
      "loss": 3.9019,
      "step": 146500
    },
    {
      "epoch": 1256.4102564102564,
      "grad_norm": 5.982016563415527,
      "learning_rate": 3.7435897435897436e-05,
      "loss": 3.8894,
      "step": 147000
    },
    {
      "epoch": 1260.6837606837607,
      "grad_norm": 7.775376796722412,
      "learning_rate": 3.739316239316239e-05,
      "loss": 3.9023,
      "step": 147500
    },
    {
      "epoch": 1264.957264957265,
      "grad_norm": 8.477492332458496,
      "learning_rate": 3.735042735042735e-05,
      "loss": 3.9033,
      "step": 148000
    },
    {
      "epoch": 1269.2307692307693,
      "grad_norm": 5.378364562988281,
      "learning_rate": 3.730769230769231e-05,
      "loss": 3.9032,
      "step": 148500
    },
    {
      "epoch": 1273.5042735042734,
      "grad_norm": 6.654353618621826,
      "learning_rate": 3.7264957264957265e-05,
      "loss": 3.9083,
      "step": 149000
    },
    {
      "epoch": 1277.7777777777778,
      "grad_norm": 5.791096210479736,
      "learning_rate": 3.722222222222222e-05,
      "loss": 3.8961,
      "step": 149500
    },
    {
      "epoch": 1282.051282051282,
      "grad_norm": 6.453015327453613,
      "learning_rate": 3.717948717948718e-05,
      "loss": 3.9031,
      "step": 150000
    },
    {
      "epoch": 1286.3247863247864,
      "grad_norm": 6.5045013427734375,
      "learning_rate": 3.713675213675214e-05,
      "loss": 3.9043,
      "step": 150500
    },
    {
      "epoch": 1290.5982905982905,
      "grad_norm": 8.266364097595215,
      "learning_rate": 3.70940170940171e-05,
      "loss": 3.9019,
      "step": 151000
    },
    {
      "epoch": 1294.871794871795,
      "grad_norm": 5.924482345581055,
      "learning_rate": 3.705128205128206e-05,
      "loss": 3.8839,
      "step": 151500
    },
    {
      "epoch": 1299.145299145299,
      "grad_norm": 6.050508975982666,
      "learning_rate": 3.7008547008547015e-05,
      "loss": 3.8984,
      "step": 152000
    },
    {
      "epoch": 1303.4188034188035,
      "grad_norm": 9.699873924255371,
      "learning_rate": 3.696581196581197e-05,
      "loss": 3.895,
      "step": 152500
    },
    {
      "epoch": 1307.6923076923076,
      "grad_norm": 6.622016906738281,
      "learning_rate": 3.692307692307693e-05,
      "loss": 3.9053,
      "step": 153000
    },
    {
      "epoch": 1311.965811965812,
      "grad_norm": 8.406899452209473,
      "learning_rate": 3.6880341880341886e-05,
      "loss": 3.8967,
      "step": 153500
    },
    {
      "epoch": 1316.2393162393162,
      "grad_norm": 8.868720054626465,
      "learning_rate": 3.6837606837606844e-05,
      "loss": 3.9027,
      "step": 154000
    },
    {
      "epoch": 1320.5128205128206,
      "grad_norm": 7.1497955322265625,
      "learning_rate": 3.67948717948718e-05,
      "loss": 3.885,
      "step": 154500
    },
    {
      "epoch": 1324.7863247863247,
      "grad_norm": 7.68508768081665,
      "learning_rate": 3.675213675213676e-05,
      "loss": 3.9107,
      "step": 155000
    },
    {
      "epoch": 1329.059829059829,
      "grad_norm": 7.473965167999268,
      "learning_rate": 3.670940170940171e-05,
      "loss": 3.8875,
      "step": 155500
    },
    {
      "epoch": 1333.3333333333333,
      "grad_norm": 5.836130142211914,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 3.8967,
      "step": 156000
    },
    {
      "epoch": 1337.6068376068376,
      "grad_norm": 7.7762274742126465,
      "learning_rate": 3.662393162393162e-05,
      "loss": 3.8976,
      "step": 156500
    },
    {
      "epoch": 1341.8803418803418,
      "grad_norm": 7.14823579788208,
      "learning_rate": 3.658119658119658e-05,
      "loss": 3.8922,
      "step": 157000
    },
    {
      "epoch": 1346.1538461538462,
      "grad_norm": 8.294669151306152,
      "learning_rate": 3.653846153846154e-05,
      "loss": 3.8921,
      "step": 157500
    },
    {
      "epoch": 1350.4273504273503,
      "grad_norm": 7.286296367645264,
      "learning_rate": 3.6495726495726495e-05,
      "loss": 3.8865,
      "step": 158000
    },
    {
      "epoch": 1354.7008547008547,
      "grad_norm": 7.252664089202881,
      "learning_rate": 3.645299145299145e-05,
      "loss": 3.8992,
      "step": 158500
    },
    {
      "epoch": 1358.974358974359,
      "grad_norm": 6.8899688720703125,
      "learning_rate": 3.641025641025641e-05,
      "loss": 3.9107,
      "step": 159000
    },
    {
      "epoch": 1363.2478632478633,
      "grad_norm": 6.62986421585083,
      "learning_rate": 3.6367521367521366e-05,
      "loss": 3.8898,
      "step": 159500
    },
    {
      "epoch": 1367.5213675213674,
      "grad_norm": 11.313315391540527,
      "learning_rate": 3.6324786324786323e-05,
      "loss": 3.8953,
      "step": 160000
    },
    {
      "epoch": 1371.7948717948718,
      "grad_norm": 6.891268253326416,
      "learning_rate": 3.628205128205128e-05,
      "loss": 3.9002,
      "step": 160500
    },
    {
      "epoch": 1376.068376068376,
      "grad_norm": 8.90712833404541,
      "learning_rate": 3.623931623931624e-05,
      "loss": 3.8959,
      "step": 161000
    },
    {
      "epoch": 1380.3418803418804,
      "grad_norm": 8.561731338500977,
      "learning_rate": 3.6196581196581195e-05,
      "loss": 3.8855,
      "step": 161500
    },
    {
      "epoch": 1384.6153846153845,
      "grad_norm": 5.722553730010986,
      "learning_rate": 3.615384615384615e-05,
      "loss": 3.8923,
      "step": 162000
    },
    {
      "epoch": 1388.888888888889,
      "grad_norm": 9.234941482543945,
      "learning_rate": 3.611111111111111e-05,
      "loss": 3.8919,
      "step": 162500
    },
    {
      "epoch": 1393.162393162393,
      "grad_norm": 6.336771488189697,
      "learning_rate": 3.606837606837607e-05,
      "loss": 3.8855,
      "step": 163000
    },
    {
      "epoch": 1397.4358974358975,
      "grad_norm": 5.59092378616333,
      "learning_rate": 3.6025641025641024e-05,
      "loss": 3.8899,
      "step": 163500
    },
    {
      "epoch": 1401.7094017094016,
      "grad_norm": 5.962862014770508,
      "learning_rate": 3.598290598290598e-05,
      "loss": 3.8888,
      "step": 164000
    },
    {
      "epoch": 1405.982905982906,
      "grad_norm": 8.078895568847656,
      "learning_rate": 3.5940170940170945e-05,
      "loss": 3.8941,
      "step": 164500
    },
    {
      "epoch": 1410.2564102564102,
      "grad_norm": 9.226444244384766,
      "learning_rate": 3.58974358974359e-05,
      "loss": 3.8951,
      "step": 165000
    },
    {
      "epoch": 1414.5299145299145,
      "grad_norm": 8.431792259216309,
      "learning_rate": 3.585470085470086e-05,
      "loss": 3.8959,
      "step": 165500
    },
    {
      "epoch": 1418.8034188034187,
      "grad_norm": 6.839963436126709,
      "learning_rate": 3.581196581196582e-05,
      "loss": 3.8908,
      "step": 166000
    },
    {
      "epoch": 1423.076923076923,
      "grad_norm": 5.44870138168335,
      "learning_rate": 3.5769230769230774e-05,
      "loss": 3.8854,
      "step": 166500
    },
    {
      "epoch": 1427.3504273504273,
      "grad_norm": 9.201324462890625,
      "learning_rate": 3.572649572649573e-05,
      "loss": 3.8941,
      "step": 167000
    },
    {
      "epoch": 1431.6239316239316,
      "grad_norm": 6.444371223449707,
      "learning_rate": 3.568376068376069e-05,
      "loss": 3.8781,
      "step": 167500
    },
    {
      "epoch": 1435.8974358974358,
      "grad_norm": 9.065454483032227,
      "learning_rate": 3.5641025641025646e-05,
      "loss": 3.9017,
      "step": 168000
    },
    {
      "epoch": 1440.1709401709402,
      "grad_norm": 7.653104305267334,
      "learning_rate": 3.55982905982906e-05,
      "loss": 3.884,
      "step": 168500
    },
    {
      "epoch": 1444.4444444444443,
      "grad_norm": 8.872462272644043,
      "learning_rate": 3.555555555555556e-05,
      "loss": 3.9052,
      "step": 169000
    },
    {
      "epoch": 1448.7179487179487,
      "grad_norm": 7.8770036697387695,
      "learning_rate": 3.551282051282052e-05,
      "loss": 3.8911,
      "step": 169500
    },
    {
      "epoch": 1452.991452991453,
      "grad_norm": 7.5474348068237305,
      "learning_rate": 3.5470085470085474e-05,
      "loss": 3.8874,
      "step": 170000
    },
    {
      "epoch": 1457.2649572649573,
      "grad_norm": 8.946928024291992,
      "learning_rate": 3.542735042735043e-05,
      "loss": 3.8872,
      "step": 170500
    },
    {
      "epoch": 1461.5384615384614,
      "grad_norm": 6.898177146911621,
      "learning_rate": 3.538461538461539e-05,
      "loss": 3.8968,
      "step": 171000
    },
    {
      "epoch": 1465.8119658119658,
      "grad_norm": 26.157161712646484,
      "learning_rate": 3.5341880341880346e-05,
      "loss": 3.883,
      "step": 171500
    },
    {
      "epoch": 1470.08547008547,
      "grad_norm": 5.657871246337891,
      "learning_rate": 3.52991452991453e-05,
      "loss": 3.8925,
      "step": 172000
    },
    {
      "epoch": 1474.3589743589744,
      "grad_norm": 7.0443644523620605,
      "learning_rate": 3.525641025641026e-05,
      "loss": 3.8937,
      "step": 172500
    },
    {
      "epoch": 1478.6324786324785,
      "grad_norm": 5.250459671020508,
      "learning_rate": 3.521367521367522e-05,
      "loss": 3.8998,
      "step": 173000
    },
    {
      "epoch": 1482.905982905983,
      "grad_norm": 6.8388261795043945,
      "learning_rate": 3.5170940170940175e-05,
      "loss": 3.8843,
      "step": 173500
    },
    {
      "epoch": 1487.179487179487,
      "grad_norm": 6.325049877166748,
      "learning_rate": 3.5128205128205125e-05,
      "loss": 3.8915,
      "step": 174000
    },
    {
      "epoch": 1491.4529914529915,
      "grad_norm": 5.923136234283447,
      "learning_rate": 3.508547008547008e-05,
      "loss": 3.8835,
      "step": 174500
    },
    {
      "epoch": 1495.7264957264956,
      "grad_norm": 7.353410243988037,
      "learning_rate": 3.504273504273504e-05,
      "loss": 3.8862,
      "step": 175000
    },
    {
      "epoch": 1500.0,
      "grad_norm": 8.88832950592041,
      "learning_rate": 3.5e-05,
      "loss": 3.8887,
      "step": 175500
    },
    {
      "epoch": 1504.2735042735044,
      "grad_norm": 7.17301607131958,
      "learning_rate": 3.4957264957264954e-05,
      "loss": 3.8947,
      "step": 176000
    },
    {
      "epoch": 1508.5470085470085,
      "grad_norm": 6.155588626861572,
      "learning_rate": 3.491452991452991e-05,
      "loss": 3.8739,
      "step": 176500
    },
    {
      "epoch": 1512.820512820513,
      "grad_norm": 6.687053203582764,
      "learning_rate": 3.487179487179487e-05,
      "loss": 3.8901,
      "step": 177000
    },
    {
      "epoch": 1517.094017094017,
      "grad_norm": 5.1387739181518555,
      "learning_rate": 3.4829059829059826e-05,
      "loss": 3.8882,
      "step": 177500
    },
    {
      "epoch": 1521.3675213675215,
      "grad_norm": 5.3556108474731445,
      "learning_rate": 3.478632478632479e-05,
      "loss": 3.8989,
      "step": 178000
    },
    {
      "epoch": 1525.6410256410256,
      "grad_norm": 7.139941215515137,
      "learning_rate": 3.474358974358975e-05,
      "loss": 3.8833,
      "step": 178500
    },
    {
      "epoch": 1529.91452991453,
      "grad_norm": 5.826701641082764,
      "learning_rate": 3.4700854700854704e-05,
      "loss": 3.8811,
      "step": 179000
    },
    {
      "epoch": 1534.1880341880342,
      "grad_norm": 8.797523498535156,
      "learning_rate": 3.465811965811966e-05,
      "loss": 3.8855,
      "step": 179500
    },
    {
      "epoch": 1538.4615384615386,
      "grad_norm": 7.352578639984131,
      "learning_rate": 3.461538461538462e-05,
      "loss": 3.8889,
      "step": 180000
    },
    {
      "epoch": 1542.7350427350427,
      "grad_norm": 5.126039028167725,
      "learning_rate": 3.4572649572649576e-05,
      "loss": 3.8762,
      "step": 180500
    },
    {
      "epoch": 1547.008547008547,
      "grad_norm": 6.078268051147461,
      "learning_rate": 3.452991452991453e-05,
      "loss": 3.8891,
      "step": 181000
    },
    {
      "epoch": 1551.2820512820513,
      "grad_norm": 5.789132595062256,
      "learning_rate": 3.448717948717949e-05,
      "loss": 3.8898,
      "step": 181500
    },
    {
      "epoch": 1555.5555555555557,
      "grad_norm": 7.569526195526123,
      "learning_rate": 3.444444444444445e-05,
      "loss": 3.8912,
      "step": 182000
    },
    {
      "epoch": 1559.8290598290598,
      "grad_norm": 5.916326999664307,
      "learning_rate": 3.4401709401709405e-05,
      "loss": 3.8867,
      "step": 182500
    },
    {
      "epoch": 1564.1025641025642,
      "grad_norm": 4.832009792327881,
      "learning_rate": 3.435897435897436e-05,
      "loss": 3.873,
      "step": 183000
    },
    {
      "epoch": 1568.3760683760684,
      "grad_norm": 5.660408020019531,
      "learning_rate": 3.431623931623932e-05,
      "loss": 3.8937,
      "step": 183500
    },
    {
      "epoch": 1572.6495726495727,
      "grad_norm": 6.381187915802002,
      "learning_rate": 3.4273504273504276e-05,
      "loss": 3.8905,
      "step": 184000
    },
    {
      "epoch": 1576.923076923077,
      "grad_norm": 6.137252330780029,
      "learning_rate": 3.4230769230769234e-05,
      "loss": 3.8834,
      "step": 184500
    },
    {
      "epoch": 1581.1965811965813,
      "grad_norm": 5.522212982177734,
      "learning_rate": 3.418803418803419e-05,
      "loss": 3.8862,
      "step": 185000
    },
    {
      "epoch": 1585.4700854700855,
      "grad_norm": 8.1517972946167,
      "learning_rate": 3.414529914529915e-05,
      "loss": 3.8793,
      "step": 185500
    },
    {
      "epoch": 1589.7435897435898,
      "grad_norm": 6.236490726470947,
      "learning_rate": 3.4102564102564105e-05,
      "loss": 3.8773,
      "step": 186000
    },
    {
      "epoch": 1594.017094017094,
      "grad_norm": 8.88163948059082,
      "learning_rate": 3.405982905982906e-05,
      "loss": 3.8862,
      "step": 186500
    },
    {
      "epoch": 1598.2905982905984,
      "grad_norm": 11.103718757629395,
      "learning_rate": 3.401709401709402e-05,
      "loss": 3.89,
      "step": 187000
    },
    {
      "epoch": 1602.5641025641025,
      "grad_norm": 5.66525936126709,
      "learning_rate": 3.397435897435898e-05,
      "loss": 3.8811,
      "step": 187500
    },
    {
      "epoch": 1606.837606837607,
      "grad_norm": 7.1092658042907715,
      "learning_rate": 3.3931623931623934e-05,
      "loss": 3.8826,
      "step": 188000
    },
    {
      "epoch": 1611.111111111111,
      "grad_norm": 8.084446907043457,
      "learning_rate": 3.388888888888889e-05,
      "loss": 3.8982,
      "step": 188500
    },
    {
      "epoch": 1615.3846153846155,
      "grad_norm": 7.755466938018799,
      "learning_rate": 3.384615384615385e-05,
      "loss": 3.8778,
      "step": 189000
    },
    {
      "epoch": 1619.6581196581196,
      "grad_norm": 5.133908271789551,
      "learning_rate": 3.3803418803418806e-05,
      "loss": 3.8882,
      "step": 189500
    },
    {
      "epoch": 1623.931623931624,
      "grad_norm": 6.284000873565674,
      "learning_rate": 3.376068376068376e-05,
      "loss": 3.8728,
      "step": 190000
    },
    {
      "epoch": 1628.2051282051282,
      "grad_norm": 6.059098720550537,
      "learning_rate": 3.371794871794872e-05,
      "loss": 3.8851,
      "step": 190500
    },
    {
      "epoch": 1632.4786324786326,
      "grad_norm": 7.031802654266357,
      "learning_rate": 3.367521367521368e-05,
      "loss": 3.8777,
      "step": 191000
    },
    {
      "epoch": 1636.7521367521367,
      "grad_norm": 7.412784099578857,
      "learning_rate": 3.3632478632478634e-05,
      "loss": 3.8831,
      "step": 191500
    },
    {
      "epoch": 1641.025641025641,
      "grad_norm": 6.855136871337891,
      "learning_rate": 3.358974358974359e-05,
      "loss": 3.8729,
      "step": 192000
    },
    {
      "epoch": 1645.2991452991453,
      "grad_norm": 5.768770694732666,
      "learning_rate": 3.354700854700855e-05,
      "loss": 3.8888,
      "step": 192500
    },
    {
      "epoch": 1649.5726495726497,
      "grad_norm": 5.942259788513184,
      "learning_rate": 3.3504273504273506e-05,
      "loss": 3.8842,
      "step": 193000
    },
    {
      "epoch": 1653.8461538461538,
      "grad_norm": 7.120217800140381,
      "learning_rate": 3.346153846153846e-05,
      "loss": 3.8773,
      "step": 193500
    },
    {
      "epoch": 1658.1196581196582,
      "grad_norm": 6.186959266662598,
      "learning_rate": 3.341880341880342e-05,
      "loss": 3.8821,
      "step": 194000
    },
    {
      "epoch": 1662.3931623931624,
      "grad_norm": 6.908320903778076,
      "learning_rate": 3.337606837606838e-05,
      "loss": 3.8788,
      "step": 194500
    },
    {
      "epoch": 1666.6666666666667,
      "grad_norm": 8.969461441040039,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 3.8883,
      "step": 195000
    },
    {
      "epoch": 1670.940170940171,
      "grad_norm": 6.900783538818359,
      "learning_rate": 3.329059829059829e-05,
      "loss": 3.8796,
      "step": 195500
    },
    {
      "epoch": 1675.2136752136753,
      "grad_norm": 7.78842306137085,
      "learning_rate": 3.324786324786325e-05,
      "loss": 3.8834,
      "step": 196000
    },
    {
      "epoch": 1679.4871794871794,
      "grad_norm": 6.43311071395874,
      "learning_rate": 3.3205128205128207e-05,
      "loss": 3.8702,
      "step": 196500
    },
    {
      "epoch": 1683.7606837606838,
      "grad_norm": 7.339198589324951,
      "learning_rate": 3.3162393162393164e-05,
      "loss": 3.892,
      "step": 197000
    },
    {
      "epoch": 1688.034188034188,
      "grad_norm": 8.448479652404785,
      "learning_rate": 3.311965811965812e-05,
      "loss": 3.8789,
      "step": 197500
    },
    {
      "epoch": 1692.3076923076924,
      "grad_norm": 6.473570346832275,
      "learning_rate": 3.307692307692308e-05,
      "loss": 3.8833,
      "step": 198000
    },
    {
      "epoch": 1696.5811965811965,
      "grad_norm": 6.910654544830322,
      "learning_rate": 3.3034188034188035e-05,
      "loss": 3.8742,
      "step": 198500
    },
    {
      "epoch": 1700.854700854701,
      "grad_norm": 6.645727157592773,
      "learning_rate": 3.299145299145299e-05,
      "loss": 3.8787,
      "step": 199000
    },
    {
      "epoch": 1705.128205128205,
      "grad_norm": 6.086300849914551,
      "learning_rate": 3.294871794871795e-05,
      "loss": 3.8761,
      "step": 199500
    },
    {
      "epoch": 1709.4017094017095,
      "grad_norm": 6.716450214385986,
      "learning_rate": 3.290598290598291e-05,
      "loss": 3.8754,
      "step": 200000
    },
    {
      "epoch": 1713.6752136752136,
      "grad_norm": 8.327093124389648,
      "learning_rate": 3.2863247863247864e-05,
      "loss": 3.8861,
      "step": 200500
    },
    {
      "epoch": 1717.948717948718,
      "grad_norm": 6.359273910522461,
      "learning_rate": 3.282051282051282e-05,
      "loss": 3.8737,
      "step": 201000
    },
    {
      "epoch": 1722.2222222222222,
      "grad_norm": 5.38441801071167,
      "learning_rate": 3.277777777777778e-05,
      "loss": 3.8785,
      "step": 201500
    },
    {
      "epoch": 1726.4957264957266,
      "grad_norm": 7.520225524902344,
      "learning_rate": 3.2735042735042736e-05,
      "loss": 3.8693,
      "step": 202000
    },
    {
      "epoch": 1730.7692307692307,
      "grad_norm": 12.958513259887695,
      "learning_rate": 3.269230769230769e-05,
      "loss": 3.8839,
      "step": 202500
    },
    {
      "epoch": 1735.042735042735,
      "grad_norm": 6.710107803344727,
      "learning_rate": 3.264957264957265e-05,
      "loss": 3.8876,
      "step": 203000
    },
    {
      "epoch": 1739.3162393162393,
      "grad_norm": 7.76682186126709,
      "learning_rate": 3.260683760683761e-05,
      "loss": 3.8861,
      "step": 203500
    },
    {
      "epoch": 1743.5897435897436,
      "grad_norm": 5.995610237121582,
      "learning_rate": 3.2564102564102565e-05,
      "loss": 3.88,
      "step": 204000
    },
    {
      "epoch": 1747.8632478632478,
      "grad_norm": 8.188546180725098,
      "learning_rate": 3.252136752136752e-05,
      "loss": 3.8789,
      "step": 204500
    },
    {
      "epoch": 1752.1367521367522,
      "grad_norm": 8.501296997070312,
      "learning_rate": 3.247863247863248e-05,
      "loss": 3.8848,
      "step": 205000
    },
    {
      "epoch": 1756.4102564102564,
      "grad_norm": 7.217730522155762,
      "learning_rate": 3.2435897435897436e-05,
      "loss": 3.8825,
      "step": 205500
    },
    {
      "epoch": 1760.6837606837607,
      "grad_norm": 5.646606922149658,
      "learning_rate": 3.2393162393162394e-05,
      "loss": 3.8772,
      "step": 206000
    },
    {
      "epoch": 1764.957264957265,
      "grad_norm": 8.068549156188965,
      "learning_rate": 3.235042735042735e-05,
      "loss": 3.8774,
      "step": 206500
    },
    {
      "epoch": 1769.2307692307693,
      "grad_norm": 7.6161065101623535,
      "learning_rate": 3.230769230769231e-05,
      "loss": 3.8741,
      "step": 207000
    },
    {
      "epoch": 1773.5042735042734,
      "grad_norm": 9.595163345336914,
      "learning_rate": 3.2264957264957265e-05,
      "loss": 3.8782,
      "step": 207500
    },
    {
      "epoch": 1777.7777777777778,
      "grad_norm": 9.765974044799805,
      "learning_rate": 3.222222222222223e-05,
      "loss": 3.8729,
      "step": 208000
    },
    {
      "epoch": 1782.051282051282,
      "grad_norm": 9.229324340820312,
      "learning_rate": 3.2179487179487186e-05,
      "loss": 3.886,
      "step": 208500
    },
    {
      "epoch": 1786.3247863247864,
      "grad_norm": 7.125509262084961,
      "learning_rate": 3.2136752136752144e-05,
      "loss": 3.8771,
      "step": 209000
    },
    {
      "epoch": 1790.5982905982905,
      "grad_norm": 7.029506206512451,
      "learning_rate": 3.20940170940171e-05,
      "loss": 3.8672,
      "step": 209500
    },
    {
      "epoch": 1794.871794871795,
      "grad_norm": 8.332058906555176,
      "learning_rate": 3.205128205128206e-05,
      "loss": 3.8817,
      "step": 210000
    },
    {
      "epoch": 1799.145299145299,
      "grad_norm": 6.5019307136535645,
      "learning_rate": 3.200854700854701e-05,
      "loss": 3.8846,
      "step": 210500
    },
    {
      "epoch": 1803.4188034188035,
      "grad_norm": 7.087681293487549,
      "learning_rate": 3.1965811965811966e-05,
      "loss": 3.8814,
      "step": 211000
    },
    {
      "epoch": 1807.6923076923076,
      "grad_norm": 6.2073516845703125,
      "learning_rate": 3.192307692307692e-05,
      "loss": 3.875,
      "step": 211500
    },
    {
      "epoch": 1811.965811965812,
      "grad_norm": 6.740688800811768,
      "learning_rate": 3.188034188034188e-05,
      "loss": 3.8751,
      "step": 212000
    },
    {
      "epoch": 1816.2393162393162,
      "grad_norm": 11.333822250366211,
      "learning_rate": 3.183760683760684e-05,
      "loss": 3.8782,
      "step": 212500
    },
    {
      "epoch": 1820.5128205128206,
      "grad_norm": 6.1607136726379395,
      "learning_rate": 3.1794871794871795e-05,
      "loss": 3.8729,
      "step": 213000
    },
    {
      "epoch": 1824.7863247863247,
      "grad_norm": 6.190780162811279,
      "learning_rate": 3.175213675213675e-05,
      "loss": 3.8725,
      "step": 213500
    },
    {
      "epoch": 1829.059829059829,
      "grad_norm": 7.300971984863281,
      "learning_rate": 3.170940170940171e-05,
      "loss": 3.8789,
      "step": 214000
    },
    {
      "epoch": 1833.3333333333333,
      "grad_norm": 7.274680137634277,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 3.8846,
      "step": 214500
    },
    {
      "epoch": 1837.6068376068376,
      "grad_norm": 7.255353927612305,
      "learning_rate": 3.162393162393162e-05,
      "loss": 3.8741,
      "step": 215000
    },
    {
      "epoch": 1841.8803418803418,
      "grad_norm": 6.6894450187683105,
      "learning_rate": 3.158119658119658e-05,
      "loss": 3.8733,
      "step": 215500
    },
    {
      "epoch": 1846.1538461538462,
      "grad_norm": 5.961180210113525,
      "learning_rate": 3.153846153846154e-05,
      "loss": 3.8779,
      "step": 216000
    },
    {
      "epoch": 1850.4273504273503,
      "grad_norm": 5.566915988922119,
      "learning_rate": 3.1495726495726495e-05,
      "loss": 3.8797,
      "step": 216500
    },
    {
      "epoch": 1854.7008547008547,
      "grad_norm": 6.978036880493164,
      "learning_rate": 3.145299145299145e-05,
      "loss": 3.8812,
      "step": 217000
    },
    {
      "epoch": 1858.974358974359,
      "grad_norm": 5.646541118621826,
      "learning_rate": 3.141025641025641e-05,
      "loss": 3.8731,
      "step": 217500
    },
    {
      "epoch": 1863.2478632478633,
      "grad_norm": 5.884077072143555,
      "learning_rate": 3.136752136752137e-05,
      "loss": 3.8797,
      "step": 218000
    },
    {
      "epoch": 1867.5213675213674,
      "grad_norm": 6.465122699737549,
      "learning_rate": 3.1324786324786324e-05,
      "loss": 3.8554,
      "step": 218500
    },
    {
      "epoch": 1871.7948717948718,
      "grad_norm": 7.561049938201904,
      "learning_rate": 3.128205128205128e-05,
      "loss": 3.8824,
      "step": 219000
    },
    {
      "epoch": 1876.068376068376,
      "grad_norm": 5.614378929138184,
      "learning_rate": 3.123931623931624e-05,
      "loss": 3.8658,
      "step": 219500
    },
    {
      "epoch": 1880.3418803418804,
      "grad_norm": 9.743213653564453,
      "learning_rate": 3.1196581196581195e-05,
      "loss": 3.8757,
      "step": 220000
    },
    {
      "epoch": 1884.6153846153845,
      "grad_norm": 6.706124305725098,
      "learning_rate": 3.115384615384615e-05,
      "loss": 3.8781,
      "step": 220500
    },
    {
      "epoch": 1888.888888888889,
      "grad_norm": 8.171833038330078,
      "learning_rate": 3.111111111111111e-05,
      "loss": 3.8796,
      "step": 221000
    },
    {
      "epoch": 1893.162393162393,
      "grad_norm": 6.09094762802124,
      "learning_rate": 3.1068376068376074e-05,
      "loss": 3.8697,
      "step": 221500
    },
    {
      "epoch": 1897.4358974358975,
      "grad_norm": 9.573050498962402,
      "learning_rate": 3.102564102564103e-05,
      "loss": 3.8758,
      "step": 222000
    },
    {
      "epoch": 1901.7094017094016,
      "grad_norm": 5.917961597442627,
      "learning_rate": 3.098290598290599e-05,
      "loss": 3.8777,
      "step": 222500
    },
    {
      "epoch": 1905.982905982906,
      "grad_norm": 5.593689441680908,
      "learning_rate": 3.0940170940170946e-05,
      "loss": 3.8653,
      "step": 223000
    },
    {
      "epoch": 1910.2564102564102,
      "grad_norm": 6.65146017074585,
      "learning_rate": 3.08974358974359e-05,
      "loss": 3.8767,
      "step": 223500
    },
    {
      "epoch": 1914.5299145299145,
      "grad_norm": 5.921352386474609,
      "learning_rate": 3.085470085470086e-05,
      "loss": 3.8657,
      "step": 224000
    },
    {
      "epoch": 1918.8034188034187,
      "grad_norm": 5.9994025230407715,
      "learning_rate": 3.081196581196582e-05,
      "loss": 3.873,
      "step": 224500
    },
    {
      "epoch": 1923.076923076923,
      "grad_norm": 5.317912578582764,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 3.867,
      "step": 225000
    },
    {
      "epoch": 1927.3504273504273,
      "grad_norm": 8.656668663024902,
      "learning_rate": 3.072649572649573e-05,
      "loss": 3.873,
      "step": 225500
    },
    {
      "epoch": 1931.6239316239316,
      "grad_norm": 7.053510665893555,
      "learning_rate": 3.068376068376069e-05,
      "loss": 3.8715,
      "step": 226000
    },
    {
      "epoch": 1935.8974358974358,
      "grad_norm": 7.458500862121582,
      "learning_rate": 3.0641025641025646e-05,
      "loss": 3.8696,
      "step": 226500
    },
    {
      "epoch": 1940.1709401709402,
      "grad_norm": 9.778066635131836,
      "learning_rate": 3.05982905982906e-05,
      "loss": 3.8756,
      "step": 227000
    },
    {
      "epoch": 1944.4444444444443,
      "grad_norm": 6.306413173675537,
      "learning_rate": 3.055555555555556e-05,
      "loss": 3.8736,
      "step": 227500
    },
    {
      "epoch": 1948.7179487179487,
      "grad_norm": 6.9407267570495605,
      "learning_rate": 3.0512820512820518e-05,
      "loss": 3.8757,
      "step": 228000
    },
    {
      "epoch": 1952.991452991453,
      "grad_norm": 6.672031879425049,
      "learning_rate": 3.0470085470085475e-05,
      "loss": 3.8737,
      "step": 228500
    },
    {
      "epoch": 1957.2649572649573,
      "grad_norm": 6.252605438232422,
      "learning_rate": 3.0427350427350425e-05,
      "loss": 3.8661,
      "step": 229000
    },
    {
      "epoch": 1961.5384615384614,
      "grad_norm": 7.23162841796875,
      "learning_rate": 3.0384615384615382e-05,
      "loss": 3.8791,
      "step": 229500
    },
    {
      "epoch": 1965.8119658119658,
      "grad_norm": 6.171245574951172,
      "learning_rate": 3.034188034188034e-05,
      "loss": 3.8811,
      "step": 230000
    },
    {
      "epoch": 1970.08547008547,
      "grad_norm": 5.427062034606934,
      "learning_rate": 3.0299145299145297e-05,
      "loss": 3.8786,
      "step": 230500
    },
    {
      "epoch": 1974.3589743589744,
      "grad_norm": 8.708120346069336,
      "learning_rate": 3.0256410256410257e-05,
      "loss": 3.8715,
      "step": 231000
    },
    {
      "epoch": 1978.6324786324785,
      "grad_norm": 6.557018756866455,
      "learning_rate": 3.0213675213675215e-05,
      "loss": 3.8695,
      "step": 231500
    },
    {
      "epoch": 1982.905982905983,
      "grad_norm": 5.99291467666626,
      "learning_rate": 3.0170940170940172e-05,
      "loss": 3.8763,
      "step": 232000
    },
    {
      "epoch": 1987.179487179487,
      "grad_norm": 6.44040060043335,
      "learning_rate": 3.012820512820513e-05,
      "loss": 3.8669,
      "step": 232500
    },
    {
      "epoch": 1991.4529914529915,
      "grad_norm": 6.718740463256836,
      "learning_rate": 3.0085470085470086e-05,
      "loss": 3.8741,
      "step": 233000
    },
    {
      "epoch": 1995.7264957264956,
      "grad_norm": 7.087294578552246,
      "learning_rate": 3.0042735042735044e-05,
      "loss": 3.866,
      "step": 233500
    },
    {
      "epoch": 2000.0,
      "grad_norm": 11.214970588684082,
      "learning_rate": 3e-05,
      "loss": 3.8704,
      "step": 234000
    },
    {
      "epoch": 2004.2735042735044,
      "grad_norm": 7.135955810546875,
      "learning_rate": 2.9957264957264958e-05,
      "loss": 3.8636,
      "step": 234500
    },
    {
      "epoch": 2008.5470085470085,
      "grad_norm": 6.994925022125244,
      "learning_rate": 2.9914529914529915e-05,
      "loss": 3.8735,
      "step": 235000
    },
    {
      "epoch": 2012.820512820513,
      "grad_norm": 6.596508502960205,
      "learning_rate": 2.9871794871794872e-05,
      "loss": 3.8614,
      "step": 235500
    },
    {
      "epoch": 2017.094017094017,
      "grad_norm": 6.021636962890625,
      "learning_rate": 2.982905982905983e-05,
      "loss": 3.8752,
      "step": 236000
    },
    {
      "epoch": 2021.3675213675215,
      "grad_norm": 6.063498020172119,
      "learning_rate": 2.9786324786324787e-05,
      "loss": 3.8698,
      "step": 236500
    },
    {
      "epoch": 2025.6410256410256,
      "grad_norm": 6.839400768280029,
      "learning_rate": 2.9743589743589744e-05,
      "loss": 3.8675,
      "step": 237000
    },
    {
      "epoch": 2029.91452991453,
      "grad_norm": 6.324821472167969,
      "learning_rate": 2.97008547008547e-05,
      "loss": 3.8762,
      "step": 237500
    },
    {
      "epoch": 2034.1880341880342,
      "grad_norm": 9.658435821533203,
      "learning_rate": 2.965811965811966e-05,
      "loss": 3.8647,
      "step": 238000
    },
    {
      "epoch": 2038.4615384615386,
      "grad_norm": 4.987521171569824,
      "learning_rate": 2.9615384615384616e-05,
      "loss": 3.8725,
      "step": 238500
    },
    {
      "epoch": 2042.7350427350427,
      "grad_norm": 8.262955665588379,
      "learning_rate": 2.9572649572649573e-05,
      "loss": 3.8765,
      "step": 239000
    },
    {
      "epoch": 2047.008547008547,
      "grad_norm": 5.402341842651367,
      "learning_rate": 2.9529914529914533e-05,
      "loss": 3.8757,
      "step": 239500
    },
    {
      "epoch": 2051.2820512820513,
      "grad_norm": 5.730078220367432,
      "learning_rate": 2.948717948717949e-05,
      "loss": 3.8739,
      "step": 240000
    },
    {
      "epoch": 2055.5555555555557,
      "grad_norm": 7.305825233459473,
      "learning_rate": 2.9444444444444448e-05,
      "loss": 3.8638,
      "step": 240500
    },
    {
      "epoch": 2059.82905982906,
      "grad_norm": 6.6525678634643555,
      "learning_rate": 2.9401709401709405e-05,
      "loss": 3.8773,
      "step": 241000
    },
    {
      "epoch": 2064.102564102564,
      "grad_norm": 7.378720760345459,
      "learning_rate": 2.9358974358974362e-05,
      "loss": 3.8647,
      "step": 241500
    },
    {
      "epoch": 2068.3760683760684,
      "grad_norm": 5.782235622406006,
      "learning_rate": 2.931623931623932e-05,
      "loss": 3.866,
      "step": 242000
    },
    {
      "epoch": 2072.6495726495727,
      "grad_norm": 6.66444206237793,
      "learning_rate": 2.9273504273504277e-05,
      "loss": 3.8702,
      "step": 242500
    },
    {
      "epoch": 2076.923076923077,
      "grad_norm": 6.007595539093018,
      "learning_rate": 2.9230769230769234e-05,
      "loss": 3.8665,
      "step": 243000
    },
    {
      "epoch": 2081.196581196581,
      "grad_norm": 8.48533821105957,
      "learning_rate": 2.918803418803419e-05,
      "loss": 3.8641,
      "step": 243500
    },
    {
      "epoch": 2085.4700854700855,
      "grad_norm": 5.331730365753174,
      "learning_rate": 2.914529914529915e-05,
      "loss": 3.8617,
      "step": 244000
    },
    {
      "epoch": 2089.74358974359,
      "grad_norm": 6.848256587982178,
      "learning_rate": 2.9102564102564106e-05,
      "loss": 3.8749,
      "step": 244500
    },
    {
      "epoch": 2094.017094017094,
      "grad_norm": 6.398602485656738,
      "learning_rate": 2.9059829059829063e-05,
      "loss": 3.8668,
      "step": 245000
    },
    {
      "epoch": 2098.290598290598,
      "grad_norm": 5.058210849761963,
      "learning_rate": 2.901709401709402e-05,
      "loss": 3.8732,
      "step": 245500
    },
    {
      "epoch": 2102.5641025641025,
      "grad_norm": 5.580383777618408,
      "learning_rate": 2.8974358974358977e-05,
      "loss": 3.8814,
      "step": 246000
    },
    {
      "epoch": 2106.837606837607,
      "grad_norm": 7.429088115692139,
      "learning_rate": 2.8931623931623934e-05,
      "loss": 3.8701,
      "step": 246500
    },
    {
      "epoch": 2111.1111111111113,
      "grad_norm": 5.976251125335693,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 3.8667,
      "step": 247000
    },
    {
      "epoch": 2115.3846153846152,
      "grad_norm": 6.612605571746826,
      "learning_rate": 2.8846153846153845e-05,
      "loss": 3.8664,
      "step": 247500
    },
    {
      "epoch": 2119.6581196581196,
      "grad_norm": 9.209565162658691,
      "learning_rate": 2.8803418803418803e-05,
      "loss": 3.8643,
      "step": 248000
    },
    {
      "epoch": 2123.931623931624,
      "grad_norm": 6.357812881469727,
      "learning_rate": 2.876068376068376e-05,
      "loss": 3.8759,
      "step": 248500
    },
    {
      "epoch": 2128.2051282051284,
      "grad_norm": 5.879672527313232,
      "learning_rate": 2.8717948717948717e-05,
      "loss": 3.8716,
      "step": 249000
    },
    {
      "epoch": 2132.4786324786323,
      "grad_norm": 7.216097831726074,
      "learning_rate": 2.8675213675213674e-05,
      "loss": 3.8712,
      "step": 249500
    },
    {
      "epoch": 2136.7521367521367,
      "grad_norm": 5.483220100402832,
      "learning_rate": 2.863247863247863e-05,
      "loss": 3.8661,
      "step": 250000
    },
    {
      "epoch": 2141.025641025641,
      "grad_norm": 6.336112022399902,
      "learning_rate": 2.858974358974359e-05,
      "loss": 3.8761,
      "step": 250500
    },
    {
      "epoch": 2145.2991452991455,
      "grad_norm": 6.196169853210449,
      "learning_rate": 2.8547008547008546e-05,
      "loss": 3.866,
      "step": 251000
    },
    {
      "epoch": 2149.5726495726494,
      "grad_norm": 8.791023254394531,
      "learning_rate": 2.8504273504273503e-05,
      "loss": 3.8709,
      "step": 251500
    },
    {
      "epoch": 2153.846153846154,
      "grad_norm": 4.934384346008301,
      "learning_rate": 2.846153846153846e-05,
      "loss": 3.8712,
      "step": 252000
    },
    {
      "epoch": 2158.119658119658,
      "grad_norm": 6.6350531578063965,
      "learning_rate": 2.8418803418803418e-05,
      "loss": 3.8676,
      "step": 252500
    },
    {
      "epoch": 2162.3931623931626,
      "grad_norm": 6.321859359741211,
      "learning_rate": 2.8376068376068378e-05,
      "loss": 3.8669,
      "step": 253000
    },
    {
      "epoch": 2166.6666666666665,
      "grad_norm": 6.579145908355713,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 3.8703,
      "step": 253500
    },
    {
      "epoch": 2170.940170940171,
      "grad_norm": 9.954607009887695,
      "learning_rate": 2.8290598290598293e-05,
      "loss": 3.863,
      "step": 254000
    },
    {
      "epoch": 2175.2136752136753,
      "grad_norm": 5.303163051605225,
      "learning_rate": 2.824786324786325e-05,
      "loss": 3.8646,
      "step": 254500
    },
    {
      "epoch": 2179.4871794871797,
      "grad_norm": 4.604946613311768,
      "learning_rate": 2.8205128205128207e-05,
      "loss": 3.8651,
      "step": 255000
    },
    {
      "epoch": 2183.7606837606836,
      "grad_norm": 6.886598587036133,
      "learning_rate": 2.8162393162393164e-05,
      "loss": 3.8714,
      "step": 255500
    },
    {
      "epoch": 2188.034188034188,
      "grad_norm": 7.55396032333374,
      "learning_rate": 2.811965811965812e-05,
      "loss": 3.8742,
      "step": 256000
    },
    {
      "epoch": 2192.3076923076924,
      "grad_norm": 7.90762186050415,
      "learning_rate": 2.807692307692308e-05,
      "loss": 3.8725,
      "step": 256500
    },
    {
      "epoch": 2196.5811965811968,
      "grad_norm": 6.729964733123779,
      "learning_rate": 2.8034188034188036e-05,
      "loss": 3.8671,
      "step": 257000
    },
    {
      "epoch": 2200.8547008547007,
      "grad_norm": 6.17934513092041,
      "learning_rate": 2.7991452991452993e-05,
      "loss": 3.8688,
      "step": 257500
    },
    {
      "epoch": 2205.128205128205,
      "grad_norm": 7.386281967163086,
      "learning_rate": 2.794871794871795e-05,
      "loss": 3.8624,
      "step": 258000
    },
    {
      "epoch": 2209.4017094017095,
      "grad_norm": 6.762639045715332,
      "learning_rate": 2.7905982905982907e-05,
      "loss": 3.8548,
      "step": 258500
    },
    {
      "epoch": 2213.675213675214,
      "grad_norm": 6.185686111450195,
      "learning_rate": 2.7863247863247865e-05,
      "loss": 3.8671,
      "step": 259000
    },
    {
      "epoch": 2217.948717948718,
      "grad_norm": 7.579869270324707,
      "learning_rate": 2.7820512820512822e-05,
      "loss": 3.8666,
      "step": 259500
    },
    {
      "epoch": 2222.222222222222,
      "grad_norm": 6.825736045837402,
      "learning_rate": 2.777777777777778e-05,
      "loss": 3.8698,
      "step": 260000
    },
    {
      "epoch": 2226.4957264957266,
      "grad_norm": 5.1796956062316895,
      "learning_rate": 2.7735042735042736e-05,
      "loss": 3.87,
      "step": 260500
    },
    {
      "epoch": 2230.769230769231,
      "grad_norm": 6.446352481842041,
      "learning_rate": 2.7692307692307694e-05,
      "loss": 3.8703,
      "step": 261000
    },
    {
      "epoch": 2235.042735042735,
      "grad_norm": 5.971834659576416,
      "learning_rate": 2.7649572649572654e-05,
      "loss": 3.8674,
      "step": 261500
    },
    {
      "epoch": 2239.3162393162393,
      "grad_norm": 7.060756683349609,
      "learning_rate": 2.760683760683761e-05,
      "loss": 3.8719,
      "step": 262000
    },
    {
      "epoch": 2243.5897435897436,
      "grad_norm": 6.387053966522217,
      "learning_rate": 2.756410256410257e-05,
      "loss": 3.8611,
      "step": 262500
    },
    {
      "epoch": 2247.863247863248,
      "grad_norm": 6.359095573425293,
      "learning_rate": 2.7521367521367526e-05,
      "loss": 3.8606,
      "step": 263000
    },
    {
      "epoch": 2252.136752136752,
      "grad_norm": 6.435451030731201,
      "learning_rate": 2.7478632478632483e-05,
      "loss": 3.8723,
      "step": 263500
    },
    {
      "epoch": 2256.4102564102564,
      "grad_norm": 5.12763786315918,
      "learning_rate": 2.743589743589744e-05,
      "loss": 3.8654,
      "step": 264000
    },
    {
      "epoch": 2260.6837606837607,
      "grad_norm": 8.139139175415039,
      "learning_rate": 2.7393162393162397e-05,
      "loss": 3.8678,
      "step": 264500
    },
    {
      "epoch": 2264.957264957265,
      "grad_norm": 6.118929386138916,
      "learning_rate": 2.7350427350427355e-05,
      "loss": 3.8638,
      "step": 265000
    },
    {
      "epoch": 2269.230769230769,
      "grad_norm": 6.97443962097168,
      "learning_rate": 2.7307692307692305e-05,
      "loss": 3.8741,
      "step": 265500
    },
    {
      "epoch": 2273.5042735042734,
      "grad_norm": 6.208436965942383,
      "learning_rate": 2.7264957264957262e-05,
      "loss": 3.8582,
      "step": 266000
    },
    {
      "epoch": 2277.777777777778,
      "grad_norm": 7.378619194030762,
      "learning_rate": 2.7222222222222223e-05,
      "loss": 3.861,
      "step": 266500
    },
    {
      "epoch": 2282.051282051282,
      "grad_norm": 8.57574462890625,
      "learning_rate": 2.717948717948718e-05,
      "loss": 3.8687,
      "step": 267000
    },
    {
      "epoch": 2286.324786324786,
      "grad_norm": 7.401900768280029,
      "learning_rate": 2.7136752136752137e-05,
      "loss": 3.8648,
      "step": 267500
    },
    {
      "epoch": 2290.5982905982905,
      "grad_norm": 8.652878761291504,
      "learning_rate": 2.7094017094017094e-05,
      "loss": 3.8687,
      "step": 268000
    },
    {
      "epoch": 2294.871794871795,
      "grad_norm": 5.2012481689453125,
      "learning_rate": 2.705128205128205e-05,
      "loss": 3.8612,
      "step": 268500
    },
    {
      "epoch": 2299.1452991452993,
      "grad_norm": 10.328161239624023,
      "learning_rate": 2.700854700854701e-05,
      "loss": 3.8714,
      "step": 269000
    },
    {
      "epoch": 2303.4188034188032,
      "grad_norm": 8.59821891784668,
      "learning_rate": 2.6965811965811966e-05,
      "loss": 3.8706,
      "step": 269500
    },
    {
      "epoch": 2307.6923076923076,
      "grad_norm": 5.995849609375,
      "learning_rate": 2.6923076923076923e-05,
      "loss": 3.8673,
      "step": 270000
    },
    {
      "epoch": 2311.965811965812,
      "grad_norm": 6.707541465759277,
      "learning_rate": 2.688034188034188e-05,
      "loss": 3.8659,
      "step": 270500
    },
    {
      "epoch": 2316.2393162393164,
      "grad_norm": 5.857332229614258,
      "learning_rate": 2.6837606837606838e-05,
      "loss": 3.8586,
      "step": 271000
    },
    {
      "epoch": 2320.5128205128203,
      "grad_norm": 7.37634801864624,
      "learning_rate": 2.6794871794871795e-05,
      "loss": 3.8727,
      "step": 271500
    },
    {
      "epoch": 2324.7863247863247,
      "grad_norm": 7.949588298797607,
      "learning_rate": 2.6752136752136752e-05,
      "loss": 3.8751,
      "step": 272000
    },
    {
      "epoch": 2329.059829059829,
      "grad_norm": 7.231234073638916,
      "learning_rate": 2.670940170940171e-05,
      "loss": 3.8672,
      "step": 272500
    },
    {
      "epoch": 2333.3333333333335,
      "grad_norm": 6.0908708572387695,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 3.8592,
      "step": 273000
    },
    {
      "epoch": 2337.6068376068374,
      "grad_norm": 9.237683296203613,
      "learning_rate": 2.6623931623931624e-05,
      "loss": 3.8611,
      "step": 273500
    },
    {
      "epoch": 2341.880341880342,
      "grad_norm": 7.281806468963623,
      "learning_rate": 2.658119658119658e-05,
      "loss": 3.855,
      "step": 274000
    },
    {
      "epoch": 2346.153846153846,
      "grad_norm": 9.162506103515625,
      "learning_rate": 2.6538461538461538e-05,
      "loss": 3.866,
      "step": 274500
    },
    {
      "epoch": 2350.4273504273506,
      "grad_norm": 8.60576343536377,
      "learning_rate": 2.64957264957265e-05,
      "loss": 3.8591,
      "step": 275000
    },
    {
      "epoch": 2354.7008547008545,
      "grad_norm": 6.212117671966553,
      "learning_rate": 2.6452991452991456e-05,
      "loss": 3.8673,
      "step": 275500
    },
    {
      "epoch": 2358.974358974359,
      "grad_norm": 7.315773010253906,
      "learning_rate": 2.6410256410256413e-05,
      "loss": 3.8614,
      "step": 276000
    },
    {
      "epoch": 2363.2478632478633,
      "grad_norm": 6.560557842254639,
      "learning_rate": 2.636752136752137e-05,
      "loss": 3.8744,
      "step": 276500
    },
    {
      "epoch": 2367.5213675213677,
      "grad_norm": 6.4314680099487305,
      "learning_rate": 2.6324786324786328e-05,
      "loss": 3.8604,
      "step": 277000
    },
    {
      "epoch": 2371.7948717948716,
      "grad_norm": 6.43577766418457,
      "learning_rate": 2.6282051282051285e-05,
      "loss": 3.8633,
      "step": 277500
    },
    {
      "epoch": 2376.068376068376,
      "grad_norm": 7.562428951263428,
      "learning_rate": 2.6239316239316242e-05,
      "loss": 3.8639,
      "step": 278000
    },
    {
      "epoch": 2380.3418803418804,
      "grad_norm": 7.317022800445557,
      "learning_rate": 2.61965811965812e-05,
      "loss": 3.8621,
      "step": 278500
    },
    {
      "epoch": 2384.6153846153848,
      "grad_norm": 6.664881229400635,
      "learning_rate": 2.6153846153846157e-05,
      "loss": 3.8579,
      "step": 279000
    },
    {
      "epoch": 2388.8888888888887,
      "grad_norm": 6.91467809677124,
      "learning_rate": 2.6111111111111114e-05,
      "loss": 3.865,
      "step": 279500
    },
    {
      "epoch": 2393.162393162393,
      "grad_norm": 6.073693752288818,
      "learning_rate": 2.606837606837607e-05,
      "loss": 3.8701,
      "step": 280000
    },
    {
      "epoch": 2397.4358974358975,
      "grad_norm": 5.824352741241455,
      "learning_rate": 2.6025641025641028e-05,
      "loss": 3.8554,
      "step": 280500
    },
    {
      "epoch": 2401.709401709402,
      "grad_norm": 8.229446411132812,
      "learning_rate": 2.5982905982905985e-05,
      "loss": 3.8623,
      "step": 281000
    },
    {
      "epoch": 2405.982905982906,
      "grad_norm": 9.294300079345703,
      "learning_rate": 2.5940170940170943e-05,
      "loss": 3.8695,
      "step": 281500
    },
    {
      "epoch": 2410.25641025641,
      "grad_norm": 6.727072715759277,
      "learning_rate": 2.58974358974359e-05,
      "loss": 3.8608,
      "step": 282000
    },
    {
      "epoch": 2414.5299145299145,
      "grad_norm": 8.375951766967773,
      "learning_rate": 2.5854700854700857e-05,
      "loss": 3.8717,
      "step": 282500
    },
    {
      "epoch": 2418.803418803419,
      "grad_norm": 8.253819465637207,
      "learning_rate": 2.5811965811965814e-05,
      "loss": 3.8604,
      "step": 283000
    },
    {
      "epoch": 2423.076923076923,
      "grad_norm": 6.689769268035889,
      "learning_rate": 2.5769230769230768e-05,
      "loss": 3.8621,
      "step": 283500
    },
    {
      "epoch": 2427.3504273504273,
      "grad_norm": 5.315905570983887,
      "learning_rate": 2.5726495726495725e-05,
      "loss": 3.8722,
      "step": 284000
    },
    {
      "epoch": 2431.6239316239316,
      "grad_norm": 7.004143238067627,
      "learning_rate": 2.5683760683760682e-05,
      "loss": 3.8661,
      "step": 284500
    },
    {
      "epoch": 2435.897435897436,
      "grad_norm": 5.58003044128418,
      "learning_rate": 2.564102564102564e-05,
      "loss": 3.8542,
      "step": 285000
    },
    {
      "epoch": 2440.17094017094,
      "grad_norm": 9.202375411987305,
      "learning_rate": 2.5598290598290597e-05,
      "loss": 3.8535,
      "step": 285500
    },
    {
      "epoch": 2444.4444444444443,
      "grad_norm": 7.963063716888428,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 3.8589,
      "step": 286000
    },
    {
      "epoch": 2448.7179487179487,
      "grad_norm": 5.71821403503418,
      "learning_rate": 2.551282051282051e-05,
      "loss": 3.8674,
      "step": 286500
    },
    {
      "epoch": 2452.991452991453,
      "grad_norm": 6.239468574523926,
      "learning_rate": 2.547008547008547e-05,
      "loss": 3.8566,
      "step": 287000
    },
    {
      "epoch": 2457.264957264957,
      "grad_norm": 6.313135147094727,
      "learning_rate": 2.5427350427350426e-05,
      "loss": 3.8589,
      "step": 287500
    },
    {
      "epoch": 2461.5384615384614,
      "grad_norm": 13.461230278015137,
      "learning_rate": 2.5384615384615383e-05,
      "loss": 3.8688,
      "step": 288000
    },
    {
      "epoch": 2465.811965811966,
      "grad_norm": 6.837730884552002,
      "learning_rate": 2.5341880341880344e-05,
      "loss": 3.8623,
      "step": 288500
    },
    {
      "epoch": 2470.08547008547,
      "grad_norm": 8.42053508758545,
      "learning_rate": 2.52991452991453e-05,
      "loss": 3.8649,
      "step": 289000
    },
    {
      "epoch": 2474.358974358974,
      "grad_norm": 6.05740213394165,
      "learning_rate": 2.5256410256410258e-05,
      "loss": 3.8653,
      "step": 289500
    },
    {
      "epoch": 2478.6324786324785,
      "grad_norm": 11.736297607421875,
      "learning_rate": 2.5213675213675215e-05,
      "loss": 3.8604,
      "step": 290000
    },
    {
      "epoch": 2482.905982905983,
      "grad_norm": 8.421445846557617,
      "learning_rate": 2.5170940170940172e-05,
      "loss": 3.8586,
      "step": 290500
    },
    {
      "epoch": 2487.1794871794873,
      "grad_norm": 6.8439507484436035,
      "learning_rate": 2.512820512820513e-05,
      "loss": 3.8547,
      "step": 291000
    },
    {
      "epoch": 2491.4529914529912,
      "grad_norm": 6.929172992706299,
      "learning_rate": 2.5085470085470087e-05,
      "loss": 3.8595,
      "step": 291500
    },
    {
      "epoch": 2495.7264957264956,
      "grad_norm": 8.34686279296875,
      "learning_rate": 2.5042735042735044e-05,
      "loss": 3.8589,
      "step": 292000
    },
    {
      "epoch": 2500.0,
      "grad_norm": 8.32435417175293,
      "learning_rate": 2.5e-05,
      "loss": 3.8612,
      "step": 292500
    },
    {
      "epoch": 2504.2735042735044,
      "grad_norm": 7.806347370147705,
      "learning_rate": 2.495726495726496e-05,
      "loss": 3.8559,
      "step": 293000
    },
    {
      "epoch": 2508.5470085470088,
      "grad_norm": 8.284542083740234,
      "learning_rate": 2.4914529914529916e-05,
      "loss": 3.8724,
      "step": 293500
    },
    {
      "epoch": 2512.8205128205127,
      "grad_norm": 4.711062431335449,
      "learning_rate": 2.4871794871794873e-05,
      "loss": 3.8604,
      "step": 294000
    },
    {
      "epoch": 2517.094017094017,
      "grad_norm": 5.5187225341796875,
      "learning_rate": 2.482905982905983e-05,
      "loss": 3.8643,
      "step": 294500
    },
    {
      "epoch": 2521.3675213675215,
      "grad_norm": 10.870867729187012,
      "learning_rate": 2.4786324786324787e-05,
      "loss": 3.8712,
      "step": 295000
    },
    {
      "epoch": 2525.641025641026,
      "grad_norm": 8.128747940063477,
      "learning_rate": 2.4743589743589744e-05,
      "loss": 3.8572,
      "step": 295500
    },
    {
      "epoch": 2529.91452991453,
      "grad_norm": 4.801506996154785,
      "learning_rate": 2.47008547008547e-05,
      "loss": 3.8533,
      "step": 296000
    },
    {
      "epoch": 2534.188034188034,
      "grad_norm": 10.674232482910156,
      "learning_rate": 2.465811965811966e-05,
      "loss": 3.8564,
      "step": 296500
    },
    {
      "epoch": 2538.4615384615386,
      "grad_norm": 7.190107822418213,
      "learning_rate": 2.461538461538462e-05,
      "loss": 3.868,
      "step": 297000
    },
    {
      "epoch": 2542.735042735043,
      "grad_norm": 6.911982536315918,
      "learning_rate": 2.4572649572649573e-05,
      "loss": 3.8441,
      "step": 297500
    },
    {
      "epoch": 2547.008547008547,
      "grad_norm": 5.5095038414001465,
      "learning_rate": 2.452991452991453e-05,
      "loss": 3.8637,
      "step": 298000
    },
    {
      "epoch": 2551.2820512820513,
      "grad_norm": 7.9434356689453125,
      "learning_rate": 2.4487179487179488e-05,
      "loss": 3.8547,
      "step": 298500
    },
    {
      "epoch": 2555.5555555555557,
      "grad_norm": 5.157257080078125,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 3.8583,
      "step": 299000
    },
    {
      "epoch": 2559.82905982906,
      "grad_norm": 9.203621864318848,
      "learning_rate": 2.4401709401709402e-05,
      "loss": 3.8622,
      "step": 299500
    },
    {
      "epoch": 2564.102564102564,
      "grad_norm": 10.79738712310791,
      "learning_rate": 2.435897435897436e-05,
      "loss": 3.8541,
      "step": 300000
    },
    {
      "epoch": 2568.3760683760684,
      "grad_norm": 8.502331733703613,
      "learning_rate": 2.4316239316239317e-05,
      "loss": 3.8625,
      "step": 300500
    },
    {
      "epoch": 2572.6495726495727,
      "grad_norm": 7.642268180847168,
      "learning_rate": 2.4273504273504274e-05,
      "loss": 3.8543,
      "step": 301000
    },
    {
      "epoch": 2576.923076923077,
      "grad_norm": 6.5482401847839355,
      "learning_rate": 2.423076923076923e-05,
      "loss": 3.8615,
      "step": 301500
    },
    {
      "epoch": 2581.196581196581,
      "grad_norm": 9.769577980041504,
      "learning_rate": 2.4188034188034188e-05,
      "loss": 3.864,
      "step": 302000
    },
    {
      "epoch": 2585.4700854700855,
      "grad_norm": 5.978809833526611,
      "learning_rate": 2.4145299145299145e-05,
      "loss": 3.8568,
      "step": 302500
    },
    {
      "epoch": 2589.74358974359,
      "grad_norm": 7.2167816162109375,
      "learning_rate": 2.4102564102564103e-05,
      "loss": 3.8616,
      "step": 303000
    },
    {
      "epoch": 2594.017094017094,
      "grad_norm": 6.761388301849365,
      "learning_rate": 2.4059829059829063e-05,
      "loss": 3.8582,
      "step": 303500
    },
    {
      "epoch": 2598.290598290598,
      "grad_norm": 7.131763458251953,
      "learning_rate": 2.401709401709402e-05,
      "loss": 3.8631,
      "step": 304000
    },
    {
      "epoch": 2602.5641025641025,
      "grad_norm": 15.352784156799316,
      "learning_rate": 2.3974358974358978e-05,
      "loss": 3.8526,
      "step": 304500
    },
    {
      "epoch": 2606.837606837607,
      "grad_norm": 6.448829174041748,
      "learning_rate": 2.3931623931623935e-05,
      "loss": 3.8557,
      "step": 305000
    },
    {
      "epoch": 2611.1111111111113,
      "grad_norm": 5.656983852386475,
      "learning_rate": 2.3888888888888892e-05,
      "loss": 3.8694,
      "step": 305500
    },
    {
      "epoch": 2615.3846153846152,
      "grad_norm": 6.085046768188477,
      "learning_rate": 2.384615384615385e-05,
      "loss": 3.8618,
      "step": 306000
    },
    {
      "epoch": 2619.6581196581196,
      "grad_norm": 6.973981857299805,
      "learning_rate": 2.3803418803418803e-05,
      "loss": 3.8668,
      "step": 306500
    },
    {
      "epoch": 2623.931623931624,
      "grad_norm": 5.274648666381836,
      "learning_rate": 2.376068376068376e-05,
      "loss": 3.8604,
      "step": 307000
    },
    {
      "epoch": 2628.2051282051284,
      "grad_norm": 5.859671115875244,
      "learning_rate": 2.3717948717948718e-05,
      "loss": 3.8618,
      "step": 307500
    },
    {
      "epoch": 2632.4786324786323,
      "grad_norm": 4.485450267791748,
      "learning_rate": 2.3675213675213675e-05,
      "loss": 3.8626,
      "step": 308000
    },
    {
      "epoch": 2636.7521367521367,
      "grad_norm": 5.470067024230957,
      "learning_rate": 2.3632478632478632e-05,
      "loss": 3.8635,
      "step": 308500
    },
    {
      "epoch": 2641.025641025641,
      "grad_norm": 6.5475592613220215,
      "learning_rate": 2.358974358974359e-05,
      "loss": 3.8648,
      "step": 309000
    },
    {
      "epoch": 2645.2991452991455,
      "grad_norm": 7.974522590637207,
      "learning_rate": 2.3547008547008546e-05,
      "loss": 3.8654,
      "step": 309500
    },
    {
      "epoch": 2649.5726495726494,
      "grad_norm": 6.925262451171875,
      "learning_rate": 2.3504273504273504e-05,
      "loss": 3.8583,
      "step": 310000
    },
    {
      "epoch": 2653.846153846154,
      "grad_norm": 9.865472793579102,
      "learning_rate": 2.3461538461538464e-05,
      "loss": 3.8591,
      "step": 310500
    },
    {
      "epoch": 2658.119658119658,
      "grad_norm": 5.245920181274414,
      "learning_rate": 2.341880341880342e-05,
      "loss": 3.8537,
      "step": 311000
    },
    {
      "epoch": 2662.3931623931626,
      "grad_norm": 7.3860249519348145,
      "learning_rate": 2.337606837606838e-05,
      "loss": 3.8633,
      "step": 311500
    },
    {
      "epoch": 2666.6666666666665,
      "grad_norm": 7.148440837860107,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 3.8617,
      "step": 312000
    },
    {
      "epoch": 2670.940170940171,
      "grad_norm": 5.6234917640686035,
      "learning_rate": 2.3290598290598293e-05,
      "loss": 3.8631,
      "step": 312500
    },
    {
      "epoch": 2675.2136752136753,
      "grad_norm": 6.267797470092773,
      "learning_rate": 2.324786324786325e-05,
      "loss": 3.8642,
      "step": 313000
    },
    {
      "epoch": 2679.4871794871797,
      "grad_norm": 5.403331756591797,
      "learning_rate": 2.3205128205128207e-05,
      "loss": 3.8641,
      "step": 313500
    },
    {
      "epoch": 2683.7606837606836,
      "grad_norm": 7.670177459716797,
      "learning_rate": 2.3162393162393165e-05,
      "loss": 3.867,
      "step": 314000
    },
    {
      "epoch": 2688.034188034188,
      "grad_norm": 6.533986568450928,
      "learning_rate": 2.3119658119658122e-05,
      "loss": 3.86,
      "step": 314500
    },
    {
      "epoch": 2692.3076923076924,
      "grad_norm": 8.16472053527832,
      "learning_rate": 2.307692307692308e-05,
      "loss": 3.8507,
      "step": 315000
    },
    {
      "epoch": 2696.5811965811968,
      "grad_norm": 7.703214645385742,
      "learning_rate": 2.3034188034188033e-05,
      "loss": 3.8611,
      "step": 315500
    },
    {
      "epoch": 2700.8547008547007,
      "grad_norm": 4.493631839752197,
      "learning_rate": 2.299145299145299e-05,
      "loss": 3.8643,
      "step": 316000
    },
    {
      "epoch": 2705.128205128205,
      "grad_norm": 7.96334171295166,
      "learning_rate": 2.2948717948717947e-05,
      "loss": 3.8642,
      "step": 316500
    },
    {
      "epoch": 2709.4017094017095,
      "grad_norm": 7.058523178100586,
      "learning_rate": 2.2905982905982905e-05,
      "loss": 3.8622,
      "step": 317000
    },
    {
      "epoch": 2713.675213675214,
      "grad_norm": 8.571613311767578,
      "learning_rate": 2.2863247863247865e-05,
      "loss": 3.853,
      "step": 317500
    },
    {
      "epoch": 2717.948717948718,
      "grad_norm": 5.830678939819336,
      "learning_rate": 2.2820512820512822e-05,
      "loss": 3.861,
      "step": 318000
    },
    {
      "epoch": 2722.222222222222,
      "grad_norm": 10.849736213684082,
      "learning_rate": 2.277777777777778e-05,
      "loss": 3.8706,
      "step": 318500
    },
    {
      "epoch": 2726.4957264957266,
      "grad_norm": 5.8316755294799805,
      "learning_rate": 2.2735042735042737e-05,
      "loss": 3.8543,
      "step": 319000
    },
    {
      "epoch": 2730.769230769231,
      "grad_norm": 12.347990036010742,
      "learning_rate": 2.2692307692307694e-05,
      "loss": 3.8535,
      "step": 319500
    },
    {
      "epoch": 2735.042735042735,
      "grad_norm": 7.6253862380981445,
      "learning_rate": 2.264957264957265e-05,
      "loss": 3.8545,
      "step": 320000
    },
    {
      "epoch": 2739.3162393162393,
      "grad_norm": 6.5087690353393555,
      "learning_rate": 2.260683760683761e-05,
      "loss": 3.8554,
      "step": 320500
    },
    {
      "epoch": 2743.5897435897436,
      "grad_norm": 5.88273811340332,
      "learning_rate": 2.2564102564102566e-05,
      "loss": 3.8625,
      "step": 321000
    },
    {
      "epoch": 2747.863247863248,
      "grad_norm": 6.2084455490112305,
      "learning_rate": 2.2521367521367523e-05,
      "loss": 3.8689,
      "step": 321500
    },
    {
      "epoch": 2752.136752136752,
      "grad_norm": 6.206622123718262,
      "learning_rate": 2.247863247863248e-05,
      "loss": 3.8669,
      "step": 322000
    },
    {
      "epoch": 2756.4102564102564,
      "grad_norm": 7.245204925537109,
      "learning_rate": 2.2435897435897437e-05,
      "loss": 3.8679,
      "step": 322500
    },
    {
      "epoch": 2760.6837606837607,
      "grad_norm": 9.1981201171875,
      "learning_rate": 2.2393162393162394e-05,
      "loss": 3.8627,
      "step": 323000
    },
    {
      "epoch": 2764.957264957265,
      "grad_norm": 5.100063800811768,
      "learning_rate": 2.235042735042735e-05,
      "loss": 3.8489,
      "step": 323500
    },
    {
      "epoch": 2769.230769230769,
      "grad_norm": 6.808504581451416,
      "learning_rate": 2.230769230769231e-05,
      "loss": 3.8599,
      "step": 324000
    },
    {
      "epoch": 2773.5042735042734,
      "grad_norm": 7.894433498382568,
      "learning_rate": 2.2264957264957266e-05,
      "loss": 3.8573,
      "step": 324500
    },
    {
      "epoch": 2777.777777777778,
      "grad_norm": 5.641517639160156,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 3.8619,
      "step": 325000
    },
    {
      "epoch": 2782.051282051282,
      "grad_norm": 6.035812854766846,
      "learning_rate": 2.217948717948718e-05,
      "loss": 3.8574,
      "step": 325500
    },
    {
      "epoch": 2786.324786324786,
      "grad_norm": 5.882615566253662,
      "learning_rate": 2.2136752136752138e-05,
      "loss": 3.8527,
      "step": 326000
    },
    {
      "epoch": 2790.5982905982905,
      "grad_norm": 7.51320219039917,
      "learning_rate": 2.2094017094017095e-05,
      "loss": 3.8604,
      "step": 326500
    },
    {
      "epoch": 2794.871794871795,
      "grad_norm": 6.741795539855957,
      "learning_rate": 2.2051282051282052e-05,
      "loss": 3.8594,
      "step": 327000
    },
    {
      "epoch": 2799.1452991452993,
      "grad_norm": 6.02520227432251,
      "learning_rate": 2.200854700854701e-05,
      "loss": 3.8726,
      "step": 327500
    },
    {
      "epoch": 2803.4188034188032,
      "grad_norm": 8.686217308044434,
      "learning_rate": 2.1965811965811967e-05,
      "loss": 3.8587,
      "step": 328000
    },
    {
      "epoch": 2807.6923076923076,
      "grad_norm": 7.8267822265625,
      "learning_rate": 2.1923076923076924e-05,
      "loss": 3.8633,
      "step": 328500
    },
    {
      "epoch": 2811.965811965812,
      "grad_norm": 8.240510940551758,
      "learning_rate": 2.188034188034188e-05,
      "loss": 3.8516,
      "step": 329000
    },
    {
      "epoch": 2816.2393162393164,
      "grad_norm": 7.2918701171875,
      "learning_rate": 2.1837606837606838e-05,
      "loss": 3.8636,
      "step": 329500
    },
    {
      "epoch": 2820.5128205128203,
      "grad_norm": 5.735622406005859,
      "learning_rate": 2.1794871794871795e-05,
      "loss": 3.8468,
      "step": 330000
    },
    {
      "epoch": 2824.7863247863247,
      "grad_norm": 6.1149678230285645,
      "learning_rate": 2.1752136752136753e-05,
      "loss": 3.866,
      "step": 330500
    },
    {
      "epoch": 2829.059829059829,
      "grad_norm": 5.670619010925293,
      "learning_rate": 2.170940170940171e-05,
      "loss": 3.8562,
      "step": 331000
    },
    {
      "epoch": 2833.3333333333335,
      "grad_norm": 4.85557746887207,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 3.8565,
      "step": 331500
    },
    {
      "epoch": 2837.6068376068374,
      "grad_norm": 6.961161136627197,
      "learning_rate": 2.1623931623931624e-05,
      "loss": 3.8573,
      "step": 332000
    },
    {
      "epoch": 2841.880341880342,
      "grad_norm": 9.815033912658691,
      "learning_rate": 2.1581196581196585e-05,
      "loss": 3.8627,
      "step": 332500
    },
    {
      "epoch": 2846.153846153846,
      "grad_norm": 8.303600311279297,
      "learning_rate": 2.1538461538461542e-05,
      "loss": 3.8645,
      "step": 333000
    },
    {
      "epoch": 2850.4273504273506,
      "grad_norm": 4.857261657714844,
      "learning_rate": 2.14957264957265e-05,
      "loss": 3.851,
      "step": 333500
    },
    {
      "epoch": 2854.7008547008545,
      "grad_norm": 12.051545143127441,
      "learning_rate": 2.1452991452991453e-05,
      "loss": 3.8644,
      "step": 334000
    },
    {
      "epoch": 2858.974358974359,
      "grad_norm": 10.225120544433594,
      "learning_rate": 2.141025641025641e-05,
      "loss": 3.8658,
      "step": 334500
    },
    {
      "epoch": 2863.2478632478633,
      "grad_norm": 6.1884541511535645,
      "learning_rate": 2.1367521367521368e-05,
      "loss": 3.8655,
      "step": 335000
    },
    {
      "epoch": 2867.5213675213677,
      "grad_norm": 11.169172286987305,
      "learning_rate": 2.1324786324786325e-05,
      "loss": 3.8621,
      "step": 335500
    },
    {
      "epoch": 2871.7948717948716,
      "grad_norm": 9.75446891784668,
      "learning_rate": 2.1282051282051282e-05,
      "loss": 3.8637,
      "step": 336000
    },
    {
      "epoch": 2876.068376068376,
      "grad_norm": 6.920517921447754,
      "learning_rate": 2.123931623931624e-05,
      "loss": 3.8684,
      "step": 336500
    },
    {
      "epoch": 2880.3418803418804,
      "grad_norm": 6.169353008270264,
      "learning_rate": 2.1196581196581196e-05,
      "loss": 3.8533,
      "step": 337000
    },
    {
      "epoch": 2884.6153846153848,
      "grad_norm": 8.48220157623291,
      "learning_rate": 2.1153846153846154e-05,
      "loss": 3.8609,
      "step": 337500
    },
    {
      "epoch": 2888.8888888888887,
      "grad_norm": 7.79032039642334,
      "learning_rate": 2.111111111111111e-05,
      "loss": 3.8573,
      "step": 338000
    },
    {
      "epoch": 2893.162393162393,
      "grad_norm": 4.829476833343506,
      "learning_rate": 2.1068376068376068e-05,
      "loss": 3.8538,
      "step": 338500
    },
    {
      "epoch": 2897.4358974358975,
      "grad_norm": 5.711863994598389,
      "learning_rate": 2.102564102564103e-05,
      "loss": 3.8596,
      "step": 339000
    },
    {
      "epoch": 2901.709401709402,
      "grad_norm": 6.200038433074951,
      "learning_rate": 2.0982905982905986e-05,
      "loss": 3.8639,
      "step": 339500
    },
    {
      "epoch": 2905.982905982906,
      "grad_norm": 5.417143821716309,
      "learning_rate": 2.0940170940170943e-05,
      "loss": 3.8562,
      "step": 340000
    },
    {
      "epoch": 2910.25641025641,
      "grad_norm": 6.8271484375,
      "learning_rate": 2.08974358974359e-05,
      "loss": 3.855,
      "step": 340500
    },
    {
      "epoch": 2914.5299145299145,
      "grad_norm": 6.570103645324707,
      "learning_rate": 2.0854700854700857e-05,
      "loss": 3.8627,
      "step": 341000
    },
    {
      "epoch": 2918.803418803419,
      "grad_norm": 9.404302597045898,
      "learning_rate": 2.0811965811965815e-05,
      "loss": 3.8574,
      "step": 341500
    },
    {
      "epoch": 2923.076923076923,
      "grad_norm": 5.9764790534973145,
      "learning_rate": 2.0769230769230772e-05,
      "loss": 3.8504,
      "step": 342000
    },
    {
      "epoch": 2927.3504273504273,
      "grad_norm": 5.822232246398926,
      "learning_rate": 2.072649572649573e-05,
      "loss": 3.8534,
      "step": 342500
    },
    {
      "epoch": 2931.6239316239316,
      "grad_norm": 4.652793884277344,
      "learning_rate": 2.0683760683760683e-05,
      "loss": 3.8597,
      "step": 343000
    },
    {
      "epoch": 2935.897435897436,
      "grad_norm": 6.255268573760986,
      "learning_rate": 2.064102564102564e-05,
      "loss": 3.861,
      "step": 343500
    },
    {
      "epoch": 2940.17094017094,
      "grad_norm": 9.47851848602295,
      "learning_rate": 2.0598290598290597e-05,
      "loss": 3.8656,
      "step": 344000
    },
    {
      "epoch": 2944.4444444444443,
      "grad_norm": 6.88186502456665,
      "learning_rate": 2.0555555555555555e-05,
      "loss": 3.857,
      "step": 344500
    },
    {
      "epoch": 2948.7179487179487,
      "grad_norm": 4.295712947845459,
      "learning_rate": 2.0512820512820512e-05,
      "loss": 3.8464,
      "step": 345000
    },
    {
      "epoch": 2952.991452991453,
      "grad_norm": 15.724485397338867,
      "learning_rate": 2.047008547008547e-05,
      "loss": 3.8607,
      "step": 345500
    },
    {
      "epoch": 2957.264957264957,
      "grad_norm": 5.0391950607299805,
      "learning_rate": 2.042735042735043e-05,
      "loss": 3.8599,
      "step": 346000
    },
    {
      "epoch": 2961.5384615384614,
      "grad_norm": 7.620805740356445,
      "learning_rate": 2.0384615384615387e-05,
      "loss": 3.8442,
      "step": 346500
    },
    {
      "epoch": 2965.811965811966,
      "grad_norm": 12.486639976501465,
      "learning_rate": 2.0341880341880344e-05,
      "loss": 3.8644,
      "step": 347000
    },
    {
      "epoch": 2970.08547008547,
      "grad_norm": 7.22880220413208,
      "learning_rate": 2.02991452991453e-05,
      "loss": 3.8604,
      "step": 347500
    },
    {
      "epoch": 2974.358974358974,
      "grad_norm": 6.857851505279541,
      "learning_rate": 2.025641025641026e-05,
      "loss": 3.8441,
      "step": 348000
    },
    {
      "epoch": 2978.6324786324785,
      "grad_norm": 5.265714168548584,
      "learning_rate": 2.0213675213675216e-05,
      "loss": 3.8581,
      "step": 348500
    },
    {
      "epoch": 2982.905982905983,
      "grad_norm": 6.481839179992676,
      "learning_rate": 2.0170940170940173e-05,
      "loss": 3.8517,
      "step": 349000
    },
    {
      "epoch": 2987.1794871794873,
      "grad_norm": 9.195462226867676,
      "learning_rate": 2.012820512820513e-05,
      "loss": 3.8473,
      "step": 349500
    },
    {
      "epoch": 2991.4529914529912,
      "grad_norm": 4.550374984741211,
      "learning_rate": 2.0085470085470087e-05,
      "loss": 3.8624,
      "step": 350000
    },
    {
      "epoch": 2995.7264957264956,
      "grad_norm": 5.385314464569092,
      "learning_rate": 2.0042735042735044e-05,
      "loss": 3.856,
      "step": 350500
    },
    {
      "epoch": 3000.0,
      "grad_norm": 8.396604537963867,
      "learning_rate": 2e-05,
      "loss": 3.8495,
      "step": 351000
    },
    {
      "epoch": 3004.2735042735044,
      "grad_norm": 6.7072930335998535,
      "learning_rate": 1.995726495726496e-05,
      "loss": 3.8637,
      "step": 351500
    },
    {
      "epoch": 3008.5470085470088,
      "grad_norm": 7.061611652374268,
      "learning_rate": 1.9914529914529913e-05,
      "loss": 3.8597,
      "step": 352000
    },
    {
      "epoch": 3012.8205128205127,
      "grad_norm": 6.69920015335083,
      "learning_rate": 1.987179487179487e-05,
      "loss": 3.8544,
      "step": 352500
    },
    {
      "epoch": 3017.094017094017,
      "grad_norm": 10.246750831604004,
      "learning_rate": 1.982905982905983e-05,
      "loss": 3.8441,
      "step": 353000
    },
    {
      "epoch": 3021.3675213675215,
      "grad_norm": 6.517632961273193,
      "learning_rate": 1.9786324786324788e-05,
      "loss": 3.8588,
      "step": 353500
    },
    {
      "epoch": 3025.641025641026,
      "grad_norm": 4.799968719482422,
      "learning_rate": 1.9743589743589745e-05,
      "loss": 3.854,
      "step": 354000
    },
    {
      "epoch": 3029.91452991453,
      "grad_norm": 5.7914323806762695,
      "learning_rate": 1.9700854700854702e-05,
      "loss": 3.8516,
      "step": 354500
    },
    {
      "epoch": 3034.188034188034,
      "grad_norm": 8.032977104187012,
      "learning_rate": 1.965811965811966e-05,
      "loss": 3.8519,
      "step": 355000
    },
    {
      "epoch": 3038.4615384615386,
      "grad_norm": 5.932280540466309,
      "learning_rate": 1.9615384615384617e-05,
      "loss": 3.8555,
      "step": 355500
    },
    {
      "epoch": 3042.735042735043,
      "grad_norm": 9.540348052978516,
      "learning_rate": 1.9572649572649574e-05,
      "loss": 3.8523,
      "step": 356000
    },
    {
      "epoch": 3047.008547008547,
      "grad_norm": 9.124935150146484,
      "learning_rate": 1.952991452991453e-05,
      "loss": 3.8543,
      "step": 356500
    },
    {
      "epoch": 3051.2820512820513,
      "grad_norm": 6.51146936416626,
      "learning_rate": 1.9487179487179488e-05,
      "loss": 3.86,
      "step": 357000
    },
    {
      "epoch": 3055.5555555555557,
      "grad_norm": 6.195018291473389,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 3.8497,
      "step": 357500
    },
    {
      "epoch": 3059.82905982906,
      "grad_norm": 6.819456577301025,
      "learning_rate": 1.9401709401709403e-05,
      "loss": 3.8461,
      "step": 358000
    },
    {
      "epoch": 3064.102564102564,
      "grad_norm": 6.375602722167969,
      "learning_rate": 1.935897435897436e-05,
      "loss": 3.8539,
      "step": 358500
    },
    {
      "epoch": 3068.3760683760684,
      "grad_norm": 5.85556697845459,
      "learning_rate": 1.9316239316239317e-05,
      "loss": 3.8516,
      "step": 359000
    },
    {
      "epoch": 3072.6495726495727,
      "grad_norm": 7.5133376121521,
      "learning_rate": 1.9273504273504274e-05,
      "loss": 3.8454,
      "step": 359500
    },
    {
      "epoch": 3076.923076923077,
      "grad_norm": 6.226598739624023,
      "learning_rate": 1.923076923076923e-05,
      "loss": 3.8605,
      "step": 360000
    },
    {
      "epoch": 3081.196581196581,
      "grad_norm": 5.999598979949951,
      "learning_rate": 1.918803418803419e-05,
      "loss": 3.866,
      "step": 360500
    },
    {
      "epoch": 3085.4700854700855,
      "grad_norm": 6.716316223144531,
      "learning_rate": 1.914529914529915e-05,
      "loss": 3.8644,
      "step": 361000
    },
    {
      "epoch": 3089.74358974359,
      "grad_norm": 5.418787956237793,
      "learning_rate": 1.9102564102564103e-05,
      "loss": 3.8526,
      "step": 361500
    },
    {
      "epoch": 3094.017094017094,
      "grad_norm": 9.597931861877441,
      "learning_rate": 1.905982905982906e-05,
      "loss": 3.8479,
      "step": 362000
    },
    {
      "epoch": 3098.290598290598,
      "grad_norm": 6.719906330108643,
      "learning_rate": 1.9017094017094017e-05,
      "loss": 3.8527,
      "step": 362500
    },
    {
      "epoch": 3102.5641025641025,
      "grad_norm": 7.110659599304199,
      "learning_rate": 1.8974358974358975e-05,
      "loss": 3.8602,
      "step": 363000
    },
    {
      "epoch": 3106.837606837607,
      "grad_norm": 6.062603950500488,
      "learning_rate": 1.8931623931623932e-05,
      "loss": 3.8424,
      "step": 363500
    },
    {
      "epoch": 3111.1111111111113,
      "grad_norm": 6.878049850463867,
      "learning_rate": 1.888888888888889e-05,
      "loss": 3.8623,
      "step": 364000
    },
    {
      "epoch": 3115.3846153846152,
      "grad_norm": 7.405776023864746,
      "learning_rate": 1.8846153846153846e-05,
      "loss": 3.8564,
      "step": 364500
    },
    {
      "epoch": 3119.6581196581196,
      "grad_norm": 7.478275775909424,
      "learning_rate": 1.8803418803418804e-05,
      "loss": 3.8524,
      "step": 365000
    },
    {
      "epoch": 3123.931623931624,
      "grad_norm": 6.4103827476501465,
      "learning_rate": 1.876068376068376e-05,
      "loss": 3.8515,
      "step": 365500
    },
    {
      "epoch": 3128.2051282051284,
      "grad_norm": 5.361420631408691,
      "learning_rate": 1.8717948717948718e-05,
      "loss": 3.8482,
      "step": 366000
    },
    {
      "epoch": 3132.4786324786323,
      "grad_norm": 7.295760631561279,
      "learning_rate": 1.8675213675213675e-05,
      "loss": 3.8499,
      "step": 366500
    },
    {
      "epoch": 3136.7521367521367,
      "grad_norm": 7.630984783172607,
      "learning_rate": 1.8632478632478632e-05,
      "loss": 3.8494,
      "step": 367000
    },
    {
      "epoch": 3141.025641025641,
      "grad_norm": 6.309628486633301,
      "learning_rate": 1.858974358974359e-05,
      "loss": 3.8501,
      "step": 367500
    },
    {
      "epoch": 3145.2991452991455,
      "grad_norm": 9.645255088806152,
      "learning_rate": 1.854700854700855e-05,
      "loss": 3.8576,
      "step": 368000
    },
    {
      "epoch": 3149.5726495726494,
      "grad_norm": 5.927445888519287,
      "learning_rate": 1.8504273504273507e-05,
      "loss": 3.8565,
      "step": 368500
    },
    {
      "epoch": 3153.846153846154,
      "grad_norm": 5.363542556762695,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 3.8504,
      "step": 369000
    },
    {
      "epoch": 3158.119658119658,
      "grad_norm": 6.049560070037842,
      "learning_rate": 1.8418803418803422e-05,
      "loss": 3.8613,
      "step": 369500
    },
    {
      "epoch": 3162.3931623931626,
      "grad_norm": 8.530417442321777,
      "learning_rate": 1.837606837606838e-05,
      "loss": 3.8608,
      "step": 370000
    },
    {
      "epoch": 3166.6666666666665,
      "grad_norm": 7.311332702636719,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 3.8462,
      "step": 370500
    },
    {
      "epoch": 3170.940170940171,
      "grad_norm": 6.6852803230285645,
      "learning_rate": 1.829059829059829e-05,
      "loss": 3.8535,
      "step": 371000
    },
    {
      "epoch": 3175.2136752136753,
      "grad_norm": 9.531510353088379,
      "learning_rate": 1.8247863247863247e-05,
      "loss": 3.854,
      "step": 371500
    },
    {
      "epoch": 3179.4871794871797,
      "grad_norm": 4.949696063995361,
      "learning_rate": 1.8205128205128204e-05,
      "loss": 3.8507,
      "step": 372000
    },
    {
      "epoch": 3183.7606837606836,
      "grad_norm": 8.61506175994873,
      "learning_rate": 1.8162393162393162e-05,
      "loss": 3.8549,
      "step": 372500
    },
    {
      "epoch": 3188.034188034188,
      "grad_norm": 7.114767551422119,
      "learning_rate": 1.811965811965812e-05,
      "loss": 3.8601,
      "step": 373000
    },
    {
      "epoch": 3192.3076923076924,
      "grad_norm": 6.688547611236572,
      "learning_rate": 1.8076923076923076e-05,
      "loss": 3.8563,
      "step": 373500
    },
    {
      "epoch": 3196.5811965811968,
      "grad_norm": 6.980363368988037,
      "learning_rate": 1.8034188034188033e-05,
      "loss": 3.8513,
      "step": 374000
    },
    {
      "epoch": 3200.8547008547007,
      "grad_norm": 5.147256851196289,
      "learning_rate": 1.799145299145299e-05,
      "loss": 3.8628,
      "step": 374500
    },
    {
      "epoch": 3205.128205128205,
      "grad_norm": 5.936110973358154,
      "learning_rate": 1.794871794871795e-05,
      "loss": 3.8509,
      "step": 375000
    },
    {
      "epoch": 3209.4017094017095,
      "grad_norm": 5.455055236816406,
      "learning_rate": 1.790598290598291e-05,
      "loss": 3.8509,
      "step": 375500
    },
    {
      "epoch": 3213.675213675214,
      "grad_norm": 7.8206024169921875,
      "learning_rate": 1.7863247863247866e-05,
      "loss": 3.8582,
      "step": 376000
    },
    {
      "epoch": 3217.948717948718,
      "grad_norm": 6.004284381866455,
      "learning_rate": 1.7820512820512823e-05,
      "loss": 3.8547,
      "step": 376500
    },
    {
      "epoch": 3222.222222222222,
      "grad_norm": 5.479804992675781,
      "learning_rate": 1.777777777777778e-05,
      "loss": 3.8609,
      "step": 377000
    },
    {
      "epoch": 3226.4957264957266,
      "grad_norm": 7.262368679046631,
      "learning_rate": 1.7735042735042737e-05,
      "loss": 3.848,
      "step": 377500
    },
    {
      "epoch": 3230.769230769231,
      "grad_norm": 6.98955774307251,
      "learning_rate": 1.7692307692307694e-05,
      "loss": 3.8638,
      "step": 378000
    },
    {
      "epoch": 3235.042735042735,
      "grad_norm": 5.8812360763549805,
      "learning_rate": 1.764957264957265e-05,
      "loss": 3.8502,
      "step": 378500
    },
    {
      "epoch": 3239.3162393162393,
      "grad_norm": 5.297847747802734,
      "learning_rate": 1.760683760683761e-05,
      "loss": 3.8554,
      "step": 379000
    },
    {
      "epoch": 3243.5897435897436,
      "grad_norm": 7.217306137084961,
      "learning_rate": 1.7564102564102563e-05,
      "loss": 3.8526,
      "step": 379500
    },
    {
      "epoch": 3247.863247863248,
      "grad_norm": 4.359228610992432,
      "learning_rate": 1.752136752136752e-05,
      "loss": 3.848,
      "step": 380000
    },
    {
      "epoch": 3252.136752136752,
      "grad_norm": 5.573180198669434,
      "learning_rate": 1.7478632478632477e-05,
      "loss": 3.8466,
      "step": 380500
    },
    {
      "epoch": 3256.4102564102564,
      "grad_norm": 8.470503807067871,
      "learning_rate": 1.7435897435897434e-05,
      "loss": 3.8599,
      "step": 381000
    },
    {
      "epoch": 3260.6837606837607,
      "grad_norm": 5.606305122375488,
      "learning_rate": 1.7393162393162395e-05,
      "loss": 3.8518,
      "step": 381500
    },
    {
      "epoch": 3264.957264957265,
      "grad_norm": 6.517281532287598,
      "learning_rate": 1.7350427350427352e-05,
      "loss": 3.8622,
      "step": 382000
    },
    {
      "epoch": 3269.230769230769,
      "grad_norm": 8.267162322998047,
      "learning_rate": 1.730769230769231e-05,
      "loss": 3.8669,
      "step": 382500
    },
    {
      "epoch": 3273.5042735042734,
      "grad_norm": 6.343186855316162,
      "learning_rate": 1.7264957264957267e-05,
      "loss": 3.8484,
      "step": 383000
    },
    {
      "epoch": 3277.777777777778,
      "grad_norm": 6.45223331451416,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 3.8514,
      "step": 383500
    },
    {
      "epoch": 3282.051282051282,
      "grad_norm": 5.5138115882873535,
      "learning_rate": 1.717948717948718e-05,
      "loss": 3.8399,
      "step": 384000
    },
    {
      "epoch": 3286.324786324786,
      "grad_norm": 6.189694881439209,
      "learning_rate": 1.7136752136752138e-05,
      "loss": 3.8513,
      "step": 384500
    },
    {
      "epoch": 3290.5982905982905,
      "grad_norm": 7.56158971786499,
      "learning_rate": 1.7094017094017095e-05,
      "loss": 3.8468,
      "step": 385000
    },
    {
      "epoch": 3294.871794871795,
      "grad_norm": 7.235848426818848,
      "learning_rate": 1.7051282051282053e-05,
      "loss": 3.8532,
      "step": 385500
    },
    {
      "epoch": 3299.1452991452993,
      "grad_norm": 9.488948822021484,
      "learning_rate": 1.700854700854701e-05,
      "loss": 3.8366,
      "step": 386000
    },
    {
      "epoch": 3303.4188034188032,
      "grad_norm": 5.616906642913818,
      "learning_rate": 1.6965811965811967e-05,
      "loss": 3.856,
      "step": 386500
    },
    {
      "epoch": 3307.6923076923076,
      "grad_norm": 6.502200126647949,
      "learning_rate": 1.6923076923076924e-05,
      "loss": 3.8569,
      "step": 387000
    },
    {
      "epoch": 3311.965811965812,
      "grad_norm": 5.7400126457214355,
      "learning_rate": 1.688034188034188e-05,
      "loss": 3.8553,
      "step": 387500
    },
    {
      "epoch": 3316.2393162393164,
      "grad_norm": 7.441009044647217,
      "learning_rate": 1.683760683760684e-05,
      "loss": 3.8538,
      "step": 388000
    },
    {
      "epoch": 3320.5128205128203,
      "grad_norm": 7.49617862701416,
      "learning_rate": 1.6794871794871796e-05,
      "loss": 3.85,
      "step": 388500
    },
    {
      "epoch": 3324.7863247863247,
      "grad_norm": 6.313830852508545,
      "learning_rate": 1.6752136752136753e-05,
      "loss": 3.8564,
      "step": 389000
    },
    {
      "epoch": 3329.059829059829,
      "grad_norm": 6.115009307861328,
      "learning_rate": 1.670940170940171e-05,
      "loss": 3.8509,
      "step": 389500
    },
    {
      "epoch": 3333.3333333333335,
      "grad_norm": 8.299777030944824,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 3.8555,
      "step": 390000
    },
    {
      "epoch": 3337.6068376068374,
      "grad_norm": 7.325772762298584,
      "learning_rate": 1.6623931623931625e-05,
      "loss": 3.8462,
      "step": 390500
    },
    {
      "epoch": 3341.880341880342,
      "grad_norm": 6.932294845581055,
      "learning_rate": 1.6581196581196582e-05,
      "loss": 3.8532,
      "step": 391000
    },
    {
      "epoch": 3346.153846153846,
      "grad_norm": 6.603904724121094,
      "learning_rate": 1.653846153846154e-05,
      "loss": 3.8539,
      "step": 391500
    },
    {
      "epoch": 3350.4273504273506,
      "grad_norm": 6.1440019607543945,
      "learning_rate": 1.6495726495726496e-05,
      "loss": 3.8556,
      "step": 392000
    },
    {
      "epoch": 3354.7008547008545,
      "grad_norm": 5.5215067863464355,
      "learning_rate": 1.6452991452991454e-05,
      "loss": 3.8438,
      "step": 392500
    },
    {
      "epoch": 3358.974358974359,
      "grad_norm": 6.929416656494141,
      "learning_rate": 1.641025641025641e-05,
      "loss": 3.856,
      "step": 393000
    },
    {
      "epoch": 3363.2478632478633,
      "grad_norm": 7.513930797576904,
      "learning_rate": 1.6367521367521368e-05,
      "loss": 3.8515,
      "step": 393500
    },
    {
      "epoch": 3367.5213675213677,
      "grad_norm": 7.244807720184326,
      "learning_rate": 1.6324786324786325e-05,
      "loss": 3.8497,
      "step": 394000
    },
    {
      "epoch": 3371.7948717948716,
      "grad_norm": 7.17394495010376,
      "learning_rate": 1.6282051282051282e-05,
      "loss": 3.8573,
      "step": 394500
    },
    {
      "epoch": 3376.068376068376,
      "grad_norm": 8.23774528503418,
      "learning_rate": 1.623931623931624e-05,
      "loss": 3.851,
      "step": 395000
    },
    {
      "epoch": 3380.3418803418804,
      "grad_norm": 6.420203685760498,
      "learning_rate": 1.6196581196581197e-05,
      "loss": 3.841,
      "step": 395500
    },
    {
      "epoch": 3384.6153846153848,
      "grad_norm": 8.274726867675781,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 3.8608,
      "step": 396000
    },
    {
      "epoch": 3388.8888888888887,
      "grad_norm": 6.546872615814209,
      "learning_rate": 1.6111111111111115e-05,
      "loss": 3.8561,
      "step": 396500
    },
    {
      "epoch": 3393.162393162393,
      "grad_norm": 6.935685157775879,
      "learning_rate": 1.6068376068376072e-05,
      "loss": 3.8477,
      "step": 397000
    },
    {
      "epoch": 3397.4358974358975,
      "grad_norm": 8.0576810836792,
      "learning_rate": 1.602564102564103e-05,
      "loss": 3.8535,
      "step": 397500
    },
    {
      "epoch": 3401.709401709402,
      "grad_norm": 5.641271114349365,
      "learning_rate": 1.5982905982905983e-05,
      "loss": 3.8512,
      "step": 398000
    },
    {
      "epoch": 3405.982905982906,
      "grad_norm": 6.888087272644043,
      "learning_rate": 1.594017094017094e-05,
      "loss": 3.8539,
      "step": 398500
    },
    {
      "epoch": 3410.25641025641,
      "grad_norm": 5.891284465789795,
      "learning_rate": 1.5897435897435897e-05,
      "loss": 3.853,
      "step": 399000
    },
    {
      "epoch": 3414.5299145299145,
      "grad_norm": 6.204245567321777,
      "learning_rate": 1.5854700854700854e-05,
      "loss": 3.8582,
      "step": 399500
    },
    {
      "epoch": 3418.803418803419,
      "grad_norm": 6.263969421386719,
      "learning_rate": 1.581196581196581e-05,
      "loss": 3.8507,
      "step": 400000
    },
    {
      "epoch": 3423.076923076923,
      "grad_norm": 7.989259243011475,
      "learning_rate": 1.576923076923077e-05,
      "loss": 3.8534,
      "step": 400500
    },
    {
      "epoch": 3427.3504273504273,
      "grad_norm": 7.099576473236084,
      "learning_rate": 1.5726495726495726e-05,
      "loss": 3.8517,
      "step": 401000
    },
    {
      "epoch": 3431.6239316239316,
      "grad_norm": 6.467569351196289,
      "learning_rate": 1.5683760683760683e-05,
      "loss": 3.8552,
      "step": 401500
    },
    {
      "epoch": 3435.897435897436,
      "grad_norm": 5.820825099945068,
      "learning_rate": 1.564102564102564e-05,
      "loss": 3.8468,
      "step": 402000
    },
    {
      "epoch": 3440.17094017094,
      "grad_norm": 7.614930152893066,
      "learning_rate": 1.5598290598290598e-05,
      "loss": 3.8537,
      "step": 402500
    },
    {
      "epoch": 3444.4444444444443,
      "grad_norm": 7.543700218200684,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 3.8605,
      "step": 403000
    },
    {
      "epoch": 3448.7179487179487,
      "grad_norm": 7.982395172119141,
      "learning_rate": 1.5512820512820516e-05,
      "loss": 3.8536,
      "step": 403500
    },
    {
      "epoch": 3452.991452991453,
      "grad_norm": 6.990199089050293,
      "learning_rate": 1.5470085470085473e-05,
      "loss": 3.8531,
      "step": 404000
    },
    {
      "epoch": 3457.264957264957,
      "grad_norm": 7.568807601928711,
      "learning_rate": 1.542735042735043e-05,
      "loss": 3.8576,
      "step": 404500
    },
    {
      "epoch": 3461.5384615384614,
      "grad_norm": 4.933117866516113,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 3.8455,
      "step": 405000
    },
    {
      "epoch": 3465.811965811966,
      "grad_norm": 6.901890754699707,
      "learning_rate": 1.5341880341880344e-05,
      "loss": 3.8614,
      "step": 405500
    },
    {
      "epoch": 3470.08547008547,
      "grad_norm": 13.573233604431152,
      "learning_rate": 1.52991452991453e-05,
      "loss": 3.8529,
      "step": 406000
    },
    {
      "epoch": 3474.358974358974,
      "grad_norm": 6.279968738555908,
      "learning_rate": 1.5256410256410259e-05,
      "loss": 3.8563,
      "step": 406500
    },
    {
      "epoch": 3478.6324786324785,
      "grad_norm": 7.122865200042725,
      "learning_rate": 1.5213675213675213e-05,
      "loss": 3.8416,
      "step": 407000
    },
    {
      "epoch": 3482.905982905983,
      "grad_norm": 5.355694770812988,
      "learning_rate": 1.517094017094017e-05,
      "loss": 3.8591,
      "step": 407500
    },
    {
      "epoch": 3487.1794871794873,
      "grad_norm": 5.557507514953613,
      "learning_rate": 1.5128205128205129e-05,
      "loss": 3.8499,
      "step": 408000
    },
    {
      "epoch": 3491.4529914529912,
      "grad_norm": 5.988241672515869,
      "learning_rate": 1.5085470085470086e-05,
      "loss": 3.8498,
      "step": 408500
    },
    {
      "epoch": 3495.7264957264956,
      "grad_norm": 6.719196796417236,
      "learning_rate": 1.5042735042735043e-05,
      "loss": 3.8479,
      "step": 409000
    },
    {
      "epoch": 3500.0,
      "grad_norm": 10.312884330749512,
      "learning_rate": 1.5e-05,
      "loss": 3.8569,
      "step": 409500
    },
    {
      "epoch": 3504.2735042735044,
      "grad_norm": 6.005336761474609,
      "learning_rate": 1.4957264957264958e-05,
      "loss": 3.849,
      "step": 410000
    },
    {
      "epoch": 3508.5470085470088,
      "grad_norm": 6.683414459228516,
      "learning_rate": 1.4914529914529915e-05,
      "loss": 3.8555,
      "step": 410500
    },
    {
      "epoch": 3512.8205128205127,
      "grad_norm": 9.191061973571777,
      "learning_rate": 1.4871794871794872e-05,
      "loss": 3.8525,
      "step": 411000
    },
    {
      "epoch": 3517.094017094017,
      "grad_norm": 5.524645805358887,
      "learning_rate": 1.482905982905983e-05,
      "loss": 3.8535,
      "step": 411500
    },
    {
      "epoch": 3521.3675213675215,
      "grad_norm": 6.529521942138672,
      "learning_rate": 1.4786324786324786e-05,
      "loss": 3.8491,
      "step": 412000
    },
    {
      "epoch": 3525.641025641026,
      "grad_norm": 6.49983024597168,
      "learning_rate": 1.4743589743589745e-05,
      "loss": 3.8449,
      "step": 412500
    },
    {
      "epoch": 3529.91452991453,
      "grad_norm": 7.191836833953857,
      "learning_rate": 1.4700854700854703e-05,
      "loss": 3.8557,
      "step": 413000
    },
    {
      "epoch": 3534.188034188034,
      "grad_norm": 8.031668663024902,
      "learning_rate": 1.465811965811966e-05,
      "loss": 3.8532,
      "step": 413500
    },
    {
      "epoch": 3538.4615384615386,
      "grad_norm": 6.926084518432617,
      "learning_rate": 1.4615384615384617e-05,
      "loss": 3.8586,
      "step": 414000
    },
    {
      "epoch": 3542.735042735043,
      "grad_norm": 7.655279159545898,
      "learning_rate": 1.4572649572649574e-05,
      "loss": 3.8537,
      "step": 414500
    },
    {
      "epoch": 3547.008547008547,
      "grad_norm": 6.367990970611572,
      "learning_rate": 1.4529914529914531e-05,
      "loss": 3.8505,
      "step": 415000
    },
    {
      "epoch": 3551.2820512820513,
      "grad_norm": 6.49528694152832,
      "learning_rate": 1.4487179487179489e-05,
      "loss": 3.854,
      "step": 415500
    },
    {
      "epoch": 3555.5555555555557,
      "grad_norm": 9.477978706359863,
      "learning_rate": 1.4444444444444444e-05,
      "loss": 3.8605,
      "step": 416000
    },
    {
      "epoch": 3559.82905982906,
      "grad_norm": 9.770669937133789,
      "learning_rate": 1.4401709401709401e-05,
      "loss": 3.8626,
      "step": 416500
    },
    {
      "epoch": 3564.102564102564,
      "grad_norm": 5.238129615783691,
      "learning_rate": 1.4358974358974359e-05,
      "loss": 3.8533,
      "step": 417000
    },
    {
      "epoch": 3568.3760683760684,
      "grad_norm": 8.187969207763672,
      "learning_rate": 1.4316239316239316e-05,
      "loss": 3.866,
      "step": 417500
    },
    {
      "epoch": 3572.6495726495727,
      "grad_norm": 5.009241580963135,
      "learning_rate": 1.4273504273504273e-05,
      "loss": 3.8537,
      "step": 418000
    },
    {
      "epoch": 3576.923076923077,
      "grad_norm": 7.501357555389404,
      "learning_rate": 1.423076923076923e-05,
      "loss": 3.8454,
      "step": 418500
    },
    {
      "epoch": 3581.196581196581,
      "grad_norm": 9.080608367919922,
      "learning_rate": 1.4188034188034189e-05,
      "loss": 3.8493,
      "step": 419000
    },
    {
      "epoch": 3585.4700854700855,
      "grad_norm": 6.36716365814209,
      "learning_rate": 1.4145299145299146e-05,
      "loss": 3.8622,
      "step": 419500
    },
    {
      "epoch": 3589.74358974359,
      "grad_norm": 6.451291561126709,
      "learning_rate": 1.4102564102564104e-05,
      "loss": 3.8472,
      "step": 420000
    },
    {
      "epoch": 3594.017094017094,
      "grad_norm": 7.09987211227417,
      "learning_rate": 1.405982905982906e-05,
      "loss": 3.8435,
      "step": 420500
    },
    {
      "epoch": 3598.290598290598,
      "grad_norm": 6.092393398284912,
      "learning_rate": 1.4017094017094018e-05,
      "loss": 3.8383,
      "step": 421000
    },
    {
      "epoch": 3602.5641025641025,
      "grad_norm": 9.721609115600586,
      "learning_rate": 1.3974358974358975e-05,
      "loss": 3.8557,
      "step": 421500
    },
    {
      "epoch": 3606.837606837607,
      "grad_norm": 6.599653720855713,
      "learning_rate": 1.3931623931623932e-05,
      "loss": 3.857,
      "step": 422000
    },
    {
      "epoch": 3611.1111111111113,
      "grad_norm": 5.623654365539551,
      "learning_rate": 1.388888888888889e-05,
      "loss": 3.8549,
      "step": 422500
    },
    {
      "epoch": 3615.3846153846152,
      "grad_norm": 5.264936923980713,
      "learning_rate": 1.3846153846153847e-05,
      "loss": 3.8486,
      "step": 423000
    },
    {
      "epoch": 3619.6581196581196,
      "grad_norm": 12.146076202392578,
      "learning_rate": 1.3803418803418806e-05,
      "loss": 3.8516,
      "step": 423500
    },
    {
      "epoch": 3623.931623931624,
      "grad_norm": 7.2667131423950195,
      "learning_rate": 1.3760683760683763e-05,
      "loss": 3.8427,
      "step": 424000
    },
    {
      "epoch": 3628.2051282051284,
      "grad_norm": 6.230648517608643,
      "learning_rate": 1.371794871794872e-05,
      "loss": 3.8484,
      "step": 424500
    },
    {
      "epoch": 3632.4786324786323,
      "grad_norm": 6.1032891273498535,
      "learning_rate": 1.3675213675213677e-05,
      "loss": 3.8464,
      "step": 425000
    },
    {
      "epoch": 3636.7521367521367,
      "grad_norm": 6.8046112060546875,
      "learning_rate": 1.3632478632478631e-05,
      "loss": 3.8545,
      "step": 425500
    },
    {
      "epoch": 3641.025641025641,
      "grad_norm": 9.017399787902832,
      "learning_rate": 1.358974358974359e-05,
      "loss": 3.8475,
      "step": 426000
    },
    {
      "epoch": 3645.2991452991455,
      "grad_norm": 6.909700870513916,
      "learning_rate": 1.3547008547008547e-05,
      "loss": 3.8437,
      "step": 426500
    },
    {
      "epoch": 3649.5726495726494,
      "grad_norm": 4.94766092300415,
      "learning_rate": 1.3504273504273504e-05,
      "loss": 3.8491,
      "step": 427000
    },
    {
      "epoch": 3653.846153846154,
      "grad_norm": 11.660079002380371,
      "learning_rate": 1.3461538461538462e-05,
      "loss": 3.8523,
      "step": 427500
    },
    {
      "epoch": 3658.119658119658,
      "grad_norm": 5.143978595733643,
      "learning_rate": 1.3418803418803419e-05,
      "loss": 3.8546,
      "step": 428000
    },
    {
      "epoch": 3662.3931623931626,
      "grad_norm": 6.332979202270508,
      "learning_rate": 1.3376068376068376e-05,
      "loss": 3.8546,
      "step": 428500
    },
    {
      "epoch": 3666.6666666666665,
      "grad_norm": 7.0860137939453125,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 3.8504,
      "step": 429000
    },
    {
      "epoch": 3670.940170940171,
      "grad_norm": 8.792435646057129,
      "learning_rate": 1.329059829059829e-05,
      "loss": 3.8478,
      "step": 429500
    },
    {
      "epoch": 3675.2136752136753,
      "grad_norm": 6.044615268707275,
      "learning_rate": 1.324786324786325e-05,
      "loss": 3.8554,
      "step": 430000
    },
    {
      "epoch": 3679.4871794871797,
      "grad_norm": 6.889280796051025,
      "learning_rate": 1.3205128205128207e-05,
      "loss": 3.85,
      "step": 430500
    },
    {
      "epoch": 3683.7606837606836,
      "grad_norm": 6.094766616821289,
      "learning_rate": 1.3162393162393164e-05,
      "loss": 3.8579,
      "step": 431000
    },
    {
      "epoch": 3688.034188034188,
      "grad_norm": 8.56273365020752,
      "learning_rate": 1.3119658119658121e-05,
      "loss": 3.8463,
      "step": 431500
    },
    {
      "epoch": 3692.3076923076924,
      "grad_norm": 6.469424724578857,
      "learning_rate": 1.3076923076923078e-05,
      "loss": 3.8486,
      "step": 432000
    },
    {
      "epoch": 3696.5811965811968,
      "grad_norm": 8.361885070800781,
      "learning_rate": 1.3034188034188035e-05,
      "loss": 3.8511,
      "step": 432500
    },
    {
      "epoch": 3700.8547008547007,
      "grad_norm": 7.424774169921875,
      "learning_rate": 1.2991452991452993e-05,
      "loss": 3.8483,
      "step": 433000
    },
    {
      "epoch": 3705.128205128205,
      "grad_norm": 5.735393524169922,
      "learning_rate": 1.294871794871795e-05,
      "loss": 3.8533,
      "step": 433500
    },
    {
      "epoch": 3709.4017094017095,
      "grad_norm": 6.034005641937256,
      "learning_rate": 1.2905982905982907e-05,
      "loss": 3.8453,
      "step": 434000
    },
    {
      "epoch": 3713.675213675214,
      "grad_norm": 5.555027484893799,
      "learning_rate": 1.2863247863247863e-05,
      "loss": 3.8637,
      "step": 434500
    },
    {
      "epoch": 3717.948717948718,
      "grad_norm": 8.3613862991333,
      "learning_rate": 1.282051282051282e-05,
      "loss": 3.851,
      "step": 435000
    },
    {
      "epoch": 3722.222222222222,
      "grad_norm": 6.269801616668701,
      "learning_rate": 1.2777777777777777e-05,
      "loss": 3.8531,
      "step": 435500
    },
    {
      "epoch": 3726.4957264957266,
      "grad_norm": 7.4358720779418945,
      "learning_rate": 1.2735042735042734e-05,
      "loss": 3.8537,
      "step": 436000
    },
    {
      "epoch": 3730.769230769231,
      "grad_norm": 6.359580993652344,
      "learning_rate": 1.2692307692307691e-05,
      "loss": 3.8441,
      "step": 436500
    },
    {
      "epoch": 3735.042735042735,
      "grad_norm": 5.817246437072754,
      "learning_rate": 1.264957264957265e-05,
      "loss": 3.8494,
      "step": 437000
    },
    {
      "epoch": 3739.3162393162393,
      "grad_norm": 7.078254222869873,
      "learning_rate": 1.2606837606837608e-05,
      "loss": 3.857,
      "step": 437500
    },
    {
      "epoch": 3743.5897435897436,
      "grad_norm": 7.004785060882568,
      "learning_rate": 1.2564102564102565e-05,
      "loss": 3.8435,
      "step": 438000
    },
    {
      "epoch": 3747.863247863248,
      "grad_norm": 6.757535457611084,
      "learning_rate": 1.2521367521367522e-05,
      "loss": 3.8495,
      "step": 438500
    },
    {
      "epoch": 3752.136752136752,
      "grad_norm": 6.386906147003174,
      "learning_rate": 1.247863247863248e-05,
      "loss": 3.8449,
      "step": 439000
    },
    {
      "epoch": 3756.4102564102564,
      "grad_norm": 7.615799903869629,
      "learning_rate": 1.2435897435897436e-05,
      "loss": 3.8507,
      "step": 439500
    },
    {
      "epoch": 3760.6837606837607,
      "grad_norm": 6.391145706176758,
      "learning_rate": 1.2393162393162394e-05,
      "loss": 3.8512,
      "step": 440000
    },
    {
      "epoch": 3764.957264957265,
      "grad_norm": 5.683253288269043,
      "learning_rate": 1.235042735042735e-05,
      "loss": 3.847,
      "step": 440500
    },
    {
      "epoch": 3769.230769230769,
      "grad_norm": 7.603121757507324,
      "learning_rate": 1.230769230769231e-05,
      "loss": 3.8507,
      "step": 441000
    },
    {
      "epoch": 3773.5042735042734,
      "grad_norm": 8.47082805633545,
      "learning_rate": 1.2264957264957265e-05,
      "loss": 3.8467,
      "step": 441500
    },
    {
      "epoch": 3777.777777777778,
      "grad_norm": 6.000853061676025,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 3.8526,
      "step": 442000
    },
    {
      "epoch": 3782.051282051282,
      "grad_norm": 6.251121520996094,
      "learning_rate": 1.217948717948718e-05,
      "loss": 3.85,
      "step": 442500
    },
    {
      "epoch": 3786.324786324786,
      "grad_norm": 4.694450378417969,
      "learning_rate": 1.2136752136752137e-05,
      "loss": 3.853,
      "step": 443000
    },
    {
      "epoch": 3790.5982905982905,
      "grad_norm": 7.273486614227295,
      "learning_rate": 1.2094017094017094e-05,
      "loss": 3.8446,
      "step": 443500
    },
    {
      "epoch": 3794.871794871795,
      "grad_norm": 6.735748767852783,
      "learning_rate": 1.2051282051282051e-05,
      "loss": 3.8559,
      "step": 444000
    },
    {
      "epoch": 3799.1452991452993,
      "grad_norm": 6.024905204772949,
      "learning_rate": 1.200854700854701e-05,
      "loss": 3.8409,
      "step": 444500
    },
    {
      "epoch": 3803.4188034188032,
      "grad_norm": 6.525893688201904,
      "learning_rate": 1.1965811965811967e-05,
      "loss": 3.8564,
      "step": 445000
    },
    {
      "epoch": 3807.6923076923076,
      "grad_norm": 7.616283893585205,
      "learning_rate": 1.1923076923076925e-05,
      "loss": 3.8486,
      "step": 445500
    },
    {
      "epoch": 3811.965811965812,
      "grad_norm": 6.062371730804443,
      "learning_rate": 1.188034188034188e-05,
      "loss": 3.8606,
      "step": 446000
    },
    {
      "epoch": 3816.2393162393164,
      "grad_norm": 9.83065128326416,
      "learning_rate": 1.1837606837606837e-05,
      "loss": 3.8435,
      "step": 446500
    },
    {
      "epoch": 3820.5128205128203,
      "grad_norm": 6.806593894958496,
      "learning_rate": 1.1794871794871795e-05,
      "loss": 3.8453,
      "step": 447000
    },
    {
      "epoch": 3824.7863247863247,
      "grad_norm": 6.584982395172119,
      "learning_rate": 1.1752136752136752e-05,
      "loss": 3.8532,
      "step": 447500
    },
    {
      "epoch": 3829.059829059829,
      "grad_norm": 6.341406345367432,
      "learning_rate": 1.170940170940171e-05,
      "loss": 3.8593,
      "step": 448000
    },
    {
      "epoch": 3833.3333333333335,
      "grad_norm": 8.329391479492188,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 3.8459,
      "step": 448500
    },
    {
      "epoch": 3837.6068376068374,
      "grad_norm": 7.854916572570801,
      "learning_rate": 1.1623931623931625e-05,
      "loss": 3.8408,
      "step": 449000
    },
    {
      "epoch": 3841.880341880342,
      "grad_norm": 7.967125415802002,
      "learning_rate": 1.1581196581196582e-05,
      "loss": 3.851,
      "step": 449500
    },
    {
      "epoch": 3846.153846153846,
      "grad_norm": 8.06904411315918,
      "learning_rate": 1.153846153846154e-05,
      "loss": 3.8483,
      "step": 450000
    },
    {
      "epoch": 3850.4273504273506,
      "grad_norm": 7.747043609619141,
      "learning_rate": 1.1495726495726495e-05,
      "loss": 3.846,
      "step": 450500
    },
    {
      "epoch": 3854.7008547008545,
      "grad_norm": 5.73163366317749,
      "learning_rate": 1.1452991452991452e-05,
      "loss": 3.8424,
      "step": 451000
    },
    {
      "epoch": 3858.974358974359,
      "grad_norm": 8.675545692443848,
      "learning_rate": 1.1410256410256411e-05,
      "loss": 3.8469,
      "step": 451500
    },
    {
      "epoch": 3863.2478632478633,
      "grad_norm": 8.099806785583496,
      "learning_rate": 1.1367521367521368e-05,
      "loss": 3.8568,
      "step": 452000
    },
    {
      "epoch": 3867.5213675213677,
      "grad_norm": 7.3365373611450195,
      "learning_rate": 1.1324786324786326e-05,
      "loss": 3.852,
      "step": 452500
    },
    {
      "epoch": 3871.7948717948716,
      "grad_norm": 7.339315891265869,
      "learning_rate": 1.1282051282051283e-05,
      "loss": 3.8526,
      "step": 453000
    },
    {
      "epoch": 3876.068376068376,
      "grad_norm": 6.541483402252197,
      "learning_rate": 1.123931623931624e-05,
      "loss": 3.8595,
      "step": 453500
    },
    {
      "epoch": 3880.3418803418804,
      "grad_norm": 5.479032039642334,
      "learning_rate": 1.1196581196581197e-05,
      "loss": 3.8472,
      "step": 454000
    },
    {
      "epoch": 3884.6153846153848,
      "grad_norm": 5.29816198348999,
      "learning_rate": 1.1153846153846154e-05,
      "loss": 3.8571,
      "step": 454500
    },
    {
      "epoch": 3888.8888888888887,
      "grad_norm": 6.973989486694336,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 3.8386,
      "step": 455000
    },
    {
      "epoch": 3893.162393162393,
      "grad_norm": 5.029973030090332,
      "learning_rate": 1.1068376068376069e-05,
      "loss": 3.8486,
      "step": 455500
    },
    {
      "epoch": 3897.4358974358975,
      "grad_norm": 7.338024616241455,
      "learning_rate": 1.1025641025641026e-05,
      "loss": 3.8503,
      "step": 456000
    },
    {
      "epoch": 3901.709401709402,
      "grad_norm": 7.377981662750244,
      "learning_rate": 1.0982905982905983e-05,
      "loss": 3.8363,
      "step": 456500
    },
    {
      "epoch": 3905.982905982906,
      "grad_norm": 5.040859222412109,
      "learning_rate": 1.094017094017094e-05,
      "loss": 3.8551,
      "step": 457000
    },
    {
      "epoch": 3910.25641025641,
      "grad_norm": 6.898927211761475,
      "learning_rate": 1.0897435897435898e-05,
      "loss": 3.8433,
      "step": 457500
    },
    {
      "epoch": 3914.5299145299145,
      "grad_norm": 6.101895332336426,
      "learning_rate": 1.0854700854700855e-05,
      "loss": 3.8466,
      "step": 458000
    },
    {
      "epoch": 3918.803418803419,
      "grad_norm": 6.6303863525390625,
      "learning_rate": 1.0811965811965812e-05,
      "loss": 3.8474,
      "step": 458500
    },
    {
      "epoch": 3923.076923076923,
      "grad_norm": 5.579093933105469,
      "learning_rate": 1.0769230769230771e-05,
      "loss": 3.858,
      "step": 459000
    },
    {
      "epoch": 3927.3504273504273,
      "grad_norm": 6.5895795822143555,
      "learning_rate": 1.0726495726495727e-05,
      "loss": 3.8518,
      "step": 459500
    },
    {
      "epoch": 3931.6239316239316,
      "grad_norm": 7.521353244781494,
      "learning_rate": 1.0683760683760684e-05,
      "loss": 3.8399,
      "step": 460000
    },
    {
      "epoch": 3935.897435897436,
      "grad_norm": 6.716864585876465,
      "learning_rate": 1.0641025641025641e-05,
      "loss": 3.8468,
      "step": 460500
    },
    {
      "epoch": 3940.17094017094,
      "grad_norm": 6.2452006340026855,
      "learning_rate": 1.0598290598290598e-05,
      "loss": 3.853,
      "step": 461000
    },
    {
      "epoch": 3944.4444444444443,
      "grad_norm": 9.542932510375977,
      "learning_rate": 1.0555555555555555e-05,
      "loss": 3.8445,
      "step": 461500
    },
    {
      "epoch": 3948.7179487179487,
      "grad_norm": 4.866786003112793,
      "learning_rate": 1.0512820512820514e-05,
      "loss": 3.8449,
      "step": 462000
    },
    {
      "epoch": 3952.991452991453,
      "grad_norm": 6.518831729888916,
      "learning_rate": 1.0470085470085471e-05,
      "loss": 3.8508,
      "step": 462500
    },
    {
      "epoch": 3957.264957264957,
      "grad_norm": 6.583685874938965,
      "learning_rate": 1.0427350427350429e-05,
      "loss": 3.8487,
      "step": 463000
    },
    {
      "epoch": 3961.5384615384614,
      "grad_norm": 7.289849758148193,
      "learning_rate": 1.0384615384615386e-05,
      "loss": 3.8543,
      "step": 463500
    },
    {
      "epoch": 3965.811965811966,
      "grad_norm": 6.885251998901367,
      "learning_rate": 1.0341880341880341e-05,
      "loss": 3.8562,
      "step": 464000
    },
    {
      "epoch": 3970.08547008547,
      "grad_norm": 7.307628631591797,
      "learning_rate": 1.0299145299145299e-05,
      "loss": 3.8468,
      "step": 464500
    },
    {
      "epoch": 3974.358974358974,
      "grad_norm": 7.320564270019531,
      "learning_rate": 1.0256410256410256e-05,
      "loss": 3.8569,
      "step": 465000
    },
    {
      "epoch": 3978.6324786324785,
      "grad_norm": 7.158496856689453,
      "learning_rate": 1.0213675213675215e-05,
      "loss": 3.8476,
      "step": 465500
    },
    {
      "epoch": 3982.905982905983,
      "grad_norm": 6.689944744110107,
      "learning_rate": 1.0170940170940172e-05,
      "loss": 3.8452,
      "step": 466000
    },
    {
      "epoch": 3987.1794871794873,
      "grad_norm": 4.817671775817871,
      "learning_rate": 1.012820512820513e-05,
      "loss": 3.8395,
      "step": 466500
    },
    {
      "epoch": 3991.4529914529912,
      "grad_norm": 7.670632362365723,
      "learning_rate": 1.0085470085470086e-05,
      "loss": 3.849,
      "step": 467000
    },
    {
      "epoch": 3995.7264957264956,
      "grad_norm": 8.051830291748047,
      "learning_rate": 1.0042735042735044e-05,
      "loss": 3.8433,
      "step": 467500
    },
    {
      "epoch": 4000.0,
      "grad_norm": 7.990854263305664,
      "learning_rate": 1e-05,
      "loss": 3.8411,
      "step": 468000
    },
    {
      "epoch": 4004.2735042735044,
      "grad_norm": 6.633859634399414,
      "learning_rate": 9.957264957264956e-06,
      "loss": 3.8455,
      "step": 468500
    },
    {
      "epoch": 4008.5470085470088,
      "grad_norm": 5.935342788696289,
      "learning_rate": 9.914529914529915e-06,
      "loss": 3.8476,
      "step": 469000
    },
    {
      "epoch": 4012.8205128205127,
      "grad_norm": 5.352187633514404,
      "learning_rate": 9.871794871794872e-06,
      "loss": 3.8534,
      "step": 469500
    },
    {
      "epoch": 4017.094017094017,
      "grad_norm": 12.515803337097168,
      "learning_rate": 9.82905982905983e-06,
      "loss": 3.8502,
      "step": 470000
    },
    {
      "epoch": 4021.3675213675215,
      "grad_norm": 6.151679039001465,
      "learning_rate": 9.786324786324787e-06,
      "loss": 3.8442,
      "step": 470500
    },
    {
      "epoch": 4025.641025641026,
      "grad_norm": 9.864907264709473,
      "learning_rate": 9.743589743589744e-06,
      "loss": 3.8506,
      "step": 471000
    },
    {
      "epoch": 4029.91452991453,
      "grad_norm": 4.823155403137207,
      "learning_rate": 9.700854700854701e-06,
      "loss": 3.8479,
      "step": 471500
    },
    {
      "epoch": 4034.188034188034,
      "grad_norm": 7.867710113525391,
      "learning_rate": 9.658119658119659e-06,
      "loss": 3.845,
      "step": 472000
    },
    {
      "epoch": 4038.4615384615386,
      "grad_norm": 6.080876350402832,
      "learning_rate": 9.615384615384616e-06,
      "loss": 3.8353,
      "step": 472500
    },
    {
      "epoch": 4042.735042735043,
      "grad_norm": 9.039155960083008,
      "learning_rate": 9.572649572649575e-06,
      "loss": 3.8498,
      "step": 473000
    },
    {
      "epoch": 4047.008547008547,
      "grad_norm": 8.933002471923828,
      "learning_rate": 9.52991452991453e-06,
      "loss": 3.8484,
      "step": 473500
    },
    {
      "epoch": 4051.2820512820513,
      "grad_norm": 6.083435535430908,
      "learning_rate": 9.487179487179487e-06,
      "loss": 3.8454,
      "step": 474000
    },
    {
      "epoch": 4055.5555555555557,
      "grad_norm": 8.517407417297363,
      "learning_rate": 9.444444444444445e-06,
      "loss": 3.8517,
      "step": 474500
    },
    {
      "epoch": 4059.82905982906,
      "grad_norm": 8.399150848388672,
      "learning_rate": 9.401709401709402e-06,
      "loss": 3.8433,
      "step": 475000
    },
    {
      "epoch": 4064.102564102564,
      "grad_norm": 7.30482292175293,
      "learning_rate": 9.358974358974359e-06,
      "loss": 3.8465,
      "step": 475500
    },
    {
      "epoch": 4068.3760683760684,
      "grad_norm": 6.135570049285889,
      "learning_rate": 9.316239316239316e-06,
      "loss": 3.8588,
      "step": 476000
    },
    {
      "epoch": 4072.6495726495727,
      "grad_norm": 6.676504135131836,
      "learning_rate": 9.273504273504275e-06,
      "loss": 3.8454,
      "step": 476500
    },
    {
      "epoch": 4076.923076923077,
      "grad_norm": 6.301239967346191,
      "learning_rate": 9.230769230769232e-06,
      "loss": 3.8494,
      "step": 477000
    },
    {
      "epoch": 4081.196581196581,
      "grad_norm": 20.820589065551758,
      "learning_rate": 9.18803418803419e-06,
      "loss": 3.8474,
      "step": 477500
    },
    {
      "epoch": 4085.4700854700855,
      "grad_norm": 8.610608100891113,
      "learning_rate": 9.145299145299145e-06,
      "loss": 3.8414,
      "step": 478000
    },
    {
      "epoch": 4089.74358974359,
      "grad_norm": 10.76124095916748,
      "learning_rate": 9.102564102564102e-06,
      "loss": 3.8506,
      "step": 478500
    },
    {
      "epoch": 4094.017094017094,
      "grad_norm": 4.380560398101807,
      "learning_rate": 9.05982905982906e-06,
      "loss": 3.8405,
      "step": 479000
    },
    {
      "epoch": 4098.290598290599,
      "grad_norm": 7.427516937255859,
      "learning_rate": 9.017094017094017e-06,
      "loss": 3.8378,
      "step": 479500
    },
    {
      "epoch": 4102.5641025641025,
      "grad_norm": 5.143411636352539,
      "learning_rate": 8.974358974358976e-06,
      "loss": 3.8348,
      "step": 480000
    },
    {
      "epoch": 4106.8376068376065,
      "grad_norm": 7.349700927734375,
      "learning_rate": 8.931623931623933e-06,
      "loss": 3.8534,
      "step": 480500
    },
    {
      "epoch": 4111.111111111111,
      "grad_norm": 7.049888610839844,
      "learning_rate": 8.88888888888889e-06,
      "loss": 3.8484,
      "step": 481000
    },
    {
      "epoch": 4115.384615384615,
      "grad_norm": 7.871098041534424,
      "learning_rate": 8.846153846153847e-06,
      "loss": 3.8384,
      "step": 481500
    },
    {
      "epoch": 4119.65811965812,
      "grad_norm": 5.5265350341796875,
      "learning_rate": 8.803418803418804e-06,
      "loss": 3.8479,
      "step": 482000
    },
    {
      "epoch": 4123.931623931624,
      "grad_norm": 6.836963176727295,
      "learning_rate": 8.76068376068376e-06,
      "loss": 3.8517,
      "step": 482500
    },
    {
      "epoch": 4128.205128205128,
      "grad_norm": 8.835548400878906,
      "learning_rate": 8.717948717948717e-06,
      "loss": 3.8498,
      "step": 483000
    },
    {
      "epoch": 4132.478632478633,
      "grad_norm": 5.0384416580200195,
      "learning_rate": 8.675213675213676e-06,
      "loss": 3.8419,
      "step": 483500
    },
    {
      "epoch": 4136.752136752137,
      "grad_norm": 6.797217845916748,
      "learning_rate": 8.632478632478633e-06,
      "loss": 3.847,
      "step": 484000
    },
    {
      "epoch": 4141.025641025641,
      "grad_norm": 8.482881546020508,
      "learning_rate": 8.58974358974359e-06,
      "loss": 3.8456,
      "step": 484500
    },
    {
      "epoch": 4145.2991452991455,
      "grad_norm": 6.980646133422852,
      "learning_rate": 8.547008547008548e-06,
      "loss": 3.8432,
      "step": 485000
    },
    {
      "epoch": 4149.572649572649,
      "grad_norm": 6.8039374351501465,
      "learning_rate": 8.504273504273505e-06,
      "loss": 3.8405,
      "step": 485500
    },
    {
      "epoch": 4153.846153846154,
      "grad_norm": 6.978059768676758,
      "learning_rate": 8.461538461538462e-06,
      "loss": 3.8439,
      "step": 486000
    },
    {
      "epoch": 4158.119658119658,
      "grad_norm": 11.098405838012695,
      "learning_rate": 8.41880341880342e-06,
      "loss": 3.8413,
      "step": 486500
    },
    {
      "epoch": 4162.393162393162,
      "grad_norm": 7.759696960449219,
      "learning_rate": 8.376068376068377e-06,
      "loss": 3.8409,
      "step": 487000
    },
    {
      "epoch": 4166.666666666667,
      "grad_norm": 6.480846881866455,
      "learning_rate": 8.333333333333334e-06,
      "loss": 3.8576,
      "step": 487500
    },
    {
      "epoch": 4170.940170940171,
      "grad_norm": 9.127996444702148,
      "learning_rate": 8.290598290598291e-06,
      "loss": 3.8446,
      "step": 488000
    },
    {
      "epoch": 4175.213675213675,
      "grad_norm": 4.873786449432373,
      "learning_rate": 8.247863247863248e-06,
      "loss": 3.8393,
      "step": 488500
    },
    {
      "epoch": 4179.48717948718,
      "grad_norm": 6.336849689483643,
      "learning_rate": 8.205128205128205e-06,
      "loss": 3.8408,
      "step": 489000
    },
    {
      "epoch": 4183.760683760684,
      "grad_norm": 5.848801136016846,
      "learning_rate": 8.162393162393163e-06,
      "loss": 3.85,
      "step": 489500
    },
    {
      "epoch": 4188.034188034188,
      "grad_norm": 6.730717658996582,
      "learning_rate": 8.11965811965812e-06,
      "loss": 3.8541,
      "step": 490000
    },
    {
      "epoch": 4192.307692307692,
      "grad_norm": 7.44318151473999,
      "learning_rate": 8.076923076923077e-06,
      "loss": 3.8516,
      "step": 490500
    },
    {
      "epoch": 4196.581196581196,
      "grad_norm": 7.861567497253418,
      "learning_rate": 8.034188034188036e-06,
      "loss": 3.8458,
      "step": 491000
    },
    {
      "epoch": 4200.854700854701,
      "grad_norm": 5.321564197540283,
      "learning_rate": 7.991452991452991e-06,
      "loss": 3.854,
      "step": 491500
    },
    {
      "epoch": 4205.128205128205,
      "grad_norm": 8.721431732177734,
      "learning_rate": 7.948717948717949e-06,
      "loss": 3.8563,
      "step": 492000
    },
    {
      "epoch": 4209.401709401709,
      "grad_norm": 7.994133949279785,
      "learning_rate": 7.905982905982906e-06,
      "loss": 3.8475,
      "step": 492500
    },
    {
      "epoch": 4213.675213675214,
      "grad_norm": 7.642183780670166,
      "learning_rate": 7.863247863247863e-06,
      "loss": 3.8429,
      "step": 493000
    },
    {
      "epoch": 4217.948717948718,
      "grad_norm": 6.540603160858154,
      "learning_rate": 7.82051282051282e-06,
      "loss": 3.842,
      "step": 493500
    },
    {
      "epoch": 4222.222222222223,
      "grad_norm": 5.606776714324951,
      "learning_rate": 7.777777777777777e-06,
      "loss": 3.8406,
      "step": 494000
    },
    {
      "epoch": 4226.495726495727,
      "grad_norm": 7.523338794708252,
      "learning_rate": 7.735042735042736e-06,
      "loss": 3.8532,
      "step": 494500
    },
    {
      "epoch": 4230.7692307692305,
      "grad_norm": 5.840158939361572,
      "learning_rate": 7.692307692307694e-06,
      "loss": 3.8466,
      "step": 495000
    },
    {
      "epoch": 4235.042735042735,
      "grad_norm": 5.668903350830078,
      "learning_rate": 7.64957264957265e-06,
      "loss": 3.8384,
      "step": 495500
    },
    {
      "epoch": 4239.316239316239,
      "grad_norm": 5.5582051277160645,
      "learning_rate": 7.606837606837606e-06,
      "loss": 3.8509,
      "step": 496000
    },
    {
      "epoch": 4243.589743589743,
      "grad_norm": 8.793195724487305,
      "learning_rate": 7.564102564102564e-06,
      "loss": 3.8481,
      "step": 496500
    },
    {
      "epoch": 4247.863247863248,
      "grad_norm": 7.683248043060303,
      "learning_rate": 7.521367521367522e-06,
      "loss": 3.843,
      "step": 497000
    },
    {
      "epoch": 4252.136752136752,
      "grad_norm": 5.265697479248047,
      "learning_rate": 7.478632478632479e-06,
      "loss": 3.8445,
      "step": 497500
    },
    {
      "epoch": 4256.410256410257,
      "grad_norm": 6.615379333496094,
      "learning_rate": 7.435897435897436e-06,
      "loss": 3.8472,
      "step": 498000
    },
    {
      "epoch": 4260.683760683761,
      "grad_norm": 9.452568054199219,
      "learning_rate": 7.393162393162393e-06,
      "loss": 3.8496,
      "step": 498500
    },
    {
      "epoch": 4264.957264957265,
      "grad_norm": 6.567836761474609,
      "learning_rate": 7.350427350427351e-06,
      "loss": 3.8494,
      "step": 499000
    },
    {
      "epoch": 4269.2307692307695,
      "grad_norm": 5.279902935028076,
      "learning_rate": 7.3076923076923085e-06,
      "loss": 3.8469,
      "step": 499500
    },
    {
      "epoch": 4273.504273504273,
      "grad_norm": 7.650031566619873,
      "learning_rate": 7.264957264957266e-06,
      "loss": 3.8488,
      "step": 500000
    },
    {
      "epoch": 4277.777777777777,
      "grad_norm": 6.1715874671936035,
      "learning_rate": 7.222222222222222e-06,
      "loss": 3.8497,
      "step": 500500
    },
    {
      "epoch": 4282.051282051282,
      "grad_norm": 7.5032806396484375,
      "learning_rate": 7.179487179487179e-06,
      "loss": 3.8468,
      "step": 501000
    },
    {
      "epoch": 4286.324786324786,
      "grad_norm": 8.59352970123291,
      "learning_rate": 7.1367521367521365e-06,
      "loss": 3.849,
      "step": 501500
    },
    {
      "epoch": 4290.598290598291,
      "grad_norm": 4.920035362243652,
      "learning_rate": 7.0940170940170945e-06,
      "loss": 3.8489,
      "step": 502000
    },
    {
      "epoch": 4294.871794871795,
      "grad_norm": 5.724334716796875,
      "learning_rate": 7.051282051282052e-06,
      "loss": 3.845,
      "step": 502500
    },
    {
      "epoch": 4299.145299145299,
      "grad_norm": 7.1237664222717285,
      "learning_rate": 7.008547008547009e-06,
      "loss": 3.838,
      "step": 503000
    },
    {
      "epoch": 4303.418803418804,
      "grad_norm": 7.011996746063232,
      "learning_rate": 6.965811965811966e-06,
      "loss": 3.8454,
      "step": 503500
    },
    {
      "epoch": 4307.692307692308,
      "grad_norm": 5.114162921905518,
      "learning_rate": 6.923076923076923e-06,
      "loss": 3.8484,
      "step": 504000
    },
    {
      "epoch": 4311.965811965812,
      "grad_norm": 8.249841690063477,
      "learning_rate": 6.8803418803418814e-06,
      "loss": 3.841,
      "step": 504500
    },
    {
      "epoch": 4316.239316239316,
      "grad_norm": 6.710742473602295,
      "learning_rate": 6.837606837606839e-06,
      "loss": 3.8459,
      "step": 505000
    },
    {
      "epoch": 4320.51282051282,
      "grad_norm": 7.293354034423828,
      "learning_rate": 6.794871794871795e-06,
      "loss": 3.8532,
      "step": 505500
    },
    {
      "epoch": 4324.786324786325,
      "grad_norm": 6.666970729827881,
      "learning_rate": 6.752136752136752e-06,
      "loss": 3.8328,
      "step": 506000
    },
    {
      "epoch": 4329.059829059829,
      "grad_norm": 6.474170207977295,
      "learning_rate": 6.7094017094017094e-06,
      "loss": 3.8483,
      "step": 506500
    },
    {
      "epoch": 4333.333333333333,
      "grad_norm": 8.660614967346191,
      "learning_rate": 6.666666666666667e-06,
      "loss": 3.847,
      "step": 507000
    },
    {
      "epoch": 4337.606837606838,
      "grad_norm": 6.338700294494629,
      "learning_rate": 6.623931623931625e-06,
      "loss": 3.842,
      "step": 507500
    },
    {
      "epoch": 4341.880341880342,
      "grad_norm": 7.529707908630371,
      "learning_rate": 6.581196581196582e-06,
      "loss": 3.8468,
      "step": 508000
    },
    {
      "epoch": 4346.153846153846,
      "grad_norm": 9.676480293273926,
      "learning_rate": 6.538461538461539e-06,
      "loss": 3.843,
      "step": 508500
    },
    {
      "epoch": 4350.427350427351,
      "grad_norm": 6.432507514953613,
      "learning_rate": 6.495726495726496e-06,
      "loss": 3.842,
      "step": 509000
    },
    {
      "epoch": 4354.7008547008545,
      "grad_norm": 6.093167304992676,
      "learning_rate": 6.4529914529914535e-06,
      "loss": 3.8433,
      "step": 509500
    },
    {
      "epoch": 4358.974358974359,
      "grad_norm": 7.0412492752075195,
      "learning_rate": 6.41025641025641e-06,
      "loss": 3.8546,
      "step": 510000
    },
    {
      "epoch": 4363.247863247863,
      "grad_norm": 7.025289058685303,
      "learning_rate": 6.367521367521367e-06,
      "loss": 3.848,
      "step": 510500
    },
    {
      "epoch": 4367.521367521367,
      "grad_norm": 5.447882175445557,
      "learning_rate": 6.324786324786325e-06,
      "loss": 3.8459,
      "step": 511000
    },
    {
      "epoch": 4371.794871794872,
      "grad_norm": 5.902449131011963,
      "learning_rate": 6.282051282051282e-06,
      "loss": 3.8403,
      "step": 511500
    },
    {
      "epoch": 4376.068376068376,
      "grad_norm": 6.200092315673828,
      "learning_rate": 6.23931623931624e-06,
      "loss": 3.8441,
      "step": 512000
    },
    {
      "epoch": 4380.34188034188,
      "grad_norm": 6.027736663818359,
      "learning_rate": 6.196581196581197e-06,
      "loss": 3.8489,
      "step": 512500
    },
    {
      "epoch": 4384.615384615385,
      "grad_norm": 6.230316162109375,
      "learning_rate": 6.153846153846155e-06,
      "loss": 3.847,
      "step": 513000
    },
    {
      "epoch": 4388.888888888889,
      "grad_norm": 8.125582695007324,
      "learning_rate": 6.111111111111111e-06,
      "loss": 3.8505,
      "step": 513500
    },
    {
      "epoch": 4393.1623931623935,
      "grad_norm": 7.084765911102295,
      "learning_rate": 6.0683760683760684e-06,
      "loss": 3.8381,
      "step": 514000
    },
    {
      "epoch": 4397.4358974358975,
      "grad_norm": 5.140542030334473,
      "learning_rate": 6.025641025641026e-06,
      "loss": 3.8462,
      "step": 514500
    },
    {
      "epoch": 4401.709401709401,
      "grad_norm": 5.792162895202637,
      "learning_rate": 5.982905982905984e-06,
      "loss": 3.8598,
      "step": 515000
    },
    {
      "epoch": 4405.982905982906,
      "grad_norm": 12.55586051940918,
      "learning_rate": 5.94017094017094e-06,
      "loss": 3.85,
      "step": 515500
    },
    {
      "epoch": 4410.25641025641,
      "grad_norm": 8.537337303161621,
      "learning_rate": 5.897435897435897e-06,
      "loss": 3.8381,
      "step": 516000
    },
    {
      "epoch": 4414.529914529914,
      "grad_norm": 6.999202728271484,
      "learning_rate": 5.854700854700855e-06,
      "loss": 3.8476,
      "step": 516500
    },
    {
      "epoch": 4418.803418803419,
      "grad_norm": 7.162222385406494,
      "learning_rate": 5.8119658119658126e-06,
      "loss": 3.8422,
      "step": 517000
    },
    {
      "epoch": 4423.076923076923,
      "grad_norm": 6.899818420410156,
      "learning_rate": 5.76923076923077e-06,
      "loss": 3.8463,
      "step": 517500
    },
    {
      "epoch": 4427.350427350428,
      "grad_norm": 8.371417999267578,
      "learning_rate": 5.726495726495726e-06,
      "loss": 3.8432,
      "step": 518000
    },
    {
      "epoch": 4431.623931623932,
      "grad_norm": 6.64849853515625,
      "learning_rate": 5.683760683760684e-06,
      "loss": 3.8544,
      "step": 518500
    },
    {
      "epoch": 4435.897435897436,
      "grad_norm": 6.379806041717529,
      "learning_rate": 5.641025641025641e-06,
      "loss": 3.8409,
      "step": 519000
    },
    {
      "epoch": 4440.17094017094,
      "grad_norm": 7.0602617263793945,
      "learning_rate": 5.598290598290599e-06,
      "loss": 3.8467,
      "step": 519500
    },
    {
      "epoch": 4444.444444444444,
      "grad_norm": 13.088540077209473,
      "learning_rate": 5.555555555555556e-06,
      "loss": 3.8474,
      "step": 520000
    },
    {
      "epoch": 4448.717948717948,
      "grad_norm": 9.349185943603516,
      "learning_rate": 5.512820512820513e-06,
      "loss": 3.8611,
      "step": 520500
    },
    {
      "epoch": 4452.991452991453,
      "grad_norm": 7.535775184631348,
      "learning_rate": 5.47008547008547e-06,
      "loss": 3.8427,
      "step": 521000
    },
    {
      "epoch": 4457.264957264957,
      "grad_norm": 9.061532974243164,
      "learning_rate": 5.4273504273504275e-06,
      "loss": 3.841,
      "step": 521500
    },
    {
      "epoch": 4461.538461538462,
      "grad_norm": 8.493703842163086,
      "learning_rate": 5.3846153846153855e-06,
      "loss": 3.8621,
      "step": 522000
    },
    {
      "epoch": 4465.811965811966,
      "grad_norm": 5.543925762176514,
      "learning_rate": 5.341880341880342e-06,
      "loss": 3.8438,
      "step": 522500
    },
    {
      "epoch": 4470.08547008547,
      "grad_norm": 7.57453727722168,
      "learning_rate": 5.299145299145299e-06,
      "loss": 3.8368,
      "step": 523000
    },
    {
      "epoch": 4474.358974358975,
      "grad_norm": 6.53264045715332,
      "learning_rate": 5.256410256410257e-06,
      "loss": 3.8507,
      "step": 523500
    },
    {
      "epoch": 4478.6324786324785,
      "grad_norm": 6.649225234985352,
      "learning_rate": 5.213675213675214e-06,
      "loss": 3.8525,
      "step": 524000
    },
    {
      "epoch": 4482.9059829059825,
      "grad_norm": 6.092637062072754,
      "learning_rate": 5.170940170940171e-06,
      "loss": 3.8454,
      "step": 524500
    },
    {
      "epoch": 4487.179487179487,
      "grad_norm": 5.084811210632324,
      "learning_rate": 5.128205128205128e-06,
      "loss": 3.8495,
      "step": 525000
    },
    {
      "epoch": 4491.452991452991,
      "grad_norm": 6.27247953414917,
      "learning_rate": 5.085470085470086e-06,
      "loss": 3.8525,
      "step": 525500
    },
    {
      "epoch": 4495.726495726496,
      "grad_norm": 6.361040115356445,
      "learning_rate": 5.042735042735043e-06,
      "loss": 3.859,
      "step": 526000
    },
    {
      "epoch": 4500.0,
      "grad_norm": 8.421643257141113,
      "learning_rate": 5e-06,
      "loss": 3.8517,
      "step": 526500
    },
    {
      "epoch": 4504.273504273504,
      "grad_norm": 15.384143829345703,
      "learning_rate": 4.957264957264958e-06,
      "loss": 3.8377,
      "step": 527000
    },
    {
      "epoch": 4508.547008547009,
      "grad_norm": 6.468230247497559,
      "learning_rate": 4.914529914529915e-06,
      "loss": 3.8538,
      "step": 527500
    },
    {
      "epoch": 4512.820512820513,
      "grad_norm": 5.241490364074707,
      "learning_rate": 4.871794871794872e-06,
      "loss": 3.8542,
      "step": 528000
    },
    {
      "epoch": 4517.0940170940175,
      "grad_norm": 5.069799423217773,
      "learning_rate": 4.829059829059829e-06,
      "loss": 3.8499,
      "step": 528500
    },
    {
      "epoch": 4521.3675213675215,
      "grad_norm": 7.352242469787598,
      "learning_rate": 4.786324786324787e-06,
      "loss": 3.8505,
      "step": 529000
    },
    {
      "epoch": 4525.641025641025,
      "grad_norm": 6.170353889465332,
      "learning_rate": 4.743589743589744e-06,
      "loss": 3.8478,
      "step": 529500
    },
    {
      "epoch": 4529.91452991453,
      "grad_norm": 6.833329200744629,
      "learning_rate": 4.700854700854701e-06,
      "loss": 3.8537,
      "step": 530000
    },
    {
      "epoch": 4534.188034188034,
      "grad_norm": 5.70191764831543,
      "learning_rate": 4.658119658119658e-06,
      "loss": 3.8362,
      "step": 530500
    },
    {
      "epoch": 4538.461538461538,
      "grad_norm": 18.580772399902344,
      "learning_rate": 4.615384615384616e-06,
      "loss": 3.8604,
      "step": 531000
    },
    {
      "epoch": 4542.735042735043,
      "grad_norm": 8.051780700683594,
      "learning_rate": 4.5726495726495725e-06,
      "loss": 3.8423,
      "step": 531500
    },
    {
      "epoch": 4547.008547008547,
      "grad_norm": 6.396999835968018,
      "learning_rate": 4.52991452991453e-06,
      "loss": 3.845,
      "step": 532000
    },
    {
      "epoch": 4551.282051282052,
      "grad_norm": 5.383274078369141,
      "learning_rate": 4.487179487179488e-06,
      "loss": 3.8409,
      "step": 532500
    },
    {
      "epoch": 4555.555555555556,
      "grad_norm": 6.34213399887085,
      "learning_rate": 4.444444444444445e-06,
      "loss": 3.8502,
      "step": 533000
    },
    {
      "epoch": 4559.82905982906,
      "grad_norm": 5.733541011810303,
      "learning_rate": 4.401709401709402e-06,
      "loss": 3.8531,
      "step": 533500
    },
    {
      "epoch": 4564.102564102564,
      "grad_norm": 7.373795032501221,
      "learning_rate": 4.3589743589743586e-06,
      "loss": 3.8516,
      "step": 534000
    },
    {
      "epoch": 4568.376068376068,
      "grad_norm": 5.846916675567627,
      "learning_rate": 4.316239316239317e-06,
      "loss": 3.8369,
      "step": 534500
    },
    {
      "epoch": 4572.649572649572,
      "grad_norm": 9.565171241760254,
      "learning_rate": 4.273504273504274e-06,
      "loss": 3.8574,
      "step": 535000
    },
    {
      "epoch": 4576.923076923077,
      "grad_norm": 6.141088008880615,
      "learning_rate": 4.230769230769231e-06,
      "loss": 3.8541,
      "step": 535500
    },
    {
      "epoch": 4581.196581196581,
      "grad_norm": 6.939499855041504,
      "learning_rate": 4.188034188034188e-06,
      "loss": 3.8502,
      "step": 536000
    },
    {
      "epoch": 4585.470085470086,
      "grad_norm": 6.014461994171143,
      "learning_rate": 4.1452991452991455e-06,
      "loss": 3.8319,
      "step": 536500
    },
    {
      "epoch": 4589.74358974359,
      "grad_norm": 5.230937480926514,
      "learning_rate": 4.102564102564103e-06,
      "loss": 3.8522,
      "step": 537000
    },
    {
      "epoch": 4594.017094017094,
      "grad_norm": 9.360339164733887,
      "learning_rate": 4.05982905982906e-06,
      "loss": 3.8458,
      "step": 537500
    },
    {
      "epoch": 4598.290598290599,
      "grad_norm": 5.8308424949646,
      "learning_rate": 4.017094017094018e-06,
      "loss": 3.8412,
      "step": 538000
    },
    {
      "epoch": 4602.5641025641025,
      "grad_norm": 8.280487060546875,
      "learning_rate": 3.974358974358974e-06,
      "loss": 3.8538,
      "step": 538500
    },
    {
      "epoch": 4606.8376068376065,
      "grad_norm": 6.994662761688232,
      "learning_rate": 3.9316239316239315e-06,
      "loss": 3.8441,
      "step": 539000
    },
    {
      "epoch": 4611.111111111111,
      "grad_norm": 7.949836730957031,
      "learning_rate": 3.888888888888889e-06,
      "loss": 3.8386,
      "step": 539500
    },
    {
      "epoch": 4615.384615384615,
      "grad_norm": 7.966054439544678,
      "learning_rate": 3.846153846153847e-06,
      "loss": 3.8461,
      "step": 540000
    },
    {
      "epoch": 4619.65811965812,
      "grad_norm": 5.867963790893555,
      "learning_rate": 3.803418803418803e-06,
      "loss": 3.8486,
      "step": 540500
    },
    {
      "epoch": 4623.931623931624,
      "grad_norm": 6.818374156951904,
      "learning_rate": 3.760683760683761e-06,
      "loss": 3.8475,
      "step": 541000
    },
    {
      "epoch": 4628.205128205128,
      "grad_norm": 7.99022102355957,
      "learning_rate": 3.717948717948718e-06,
      "loss": 3.8455,
      "step": 541500
    },
    {
      "epoch": 4632.478632478633,
      "grad_norm": 5.67010498046875,
      "learning_rate": 3.6752136752136756e-06,
      "loss": 3.8408,
      "step": 542000
    },
    {
      "epoch": 4636.752136752137,
      "grad_norm": 10.551667213439941,
      "learning_rate": 3.632478632478633e-06,
      "loss": 3.8539,
      "step": 542500
    },
    {
      "epoch": 4641.025641025641,
      "grad_norm": 7.266312122344971,
      "learning_rate": 3.5897435897435896e-06,
      "loss": 3.8516,
      "step": 543000
    },
    {
      "epoch": 4645.2991452991455,
      "grad_norm": 8.503974914550781,
      "learning_rate": 3.5470085470085473e-06,
      "loss": 3.8506,
      "step": 543500
    },
    {
      "epoch": 4649.572649572649,
      "grad_norm": 9.503233909606934,
      "learning_rate": 3.5042735042735045e-06,
      "loss": 3.8487,
      "step": 544000
    },
    {
      "epoch": 4653.846153846154,
      "grad_norm": 7.3025736808776855,
      "learning_rate": 3.4615384615384617e-06,
      "loss": 3.8453,
      "step": 544500
    },
    {
      "epoch": 4658.119658119658,
      "grad_norm": 6.519083499908447,
      "learning_rate": 3.4188034188034193e-06,
      "loss": 3.8472,
      "step": 545000
    },
    {
      "epoch": 4662.393162393162,
      "grad_norm": 5.914863109588623,
      "learning_rate": 3.376068376068376e-06,
      "loss": 3.8423,
      "step": 545500
    },
    {
      "epoch": 4666.666666666667,
      "grad_norm": 6.853129863739014,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 3.8404,
      "step": 546000
    },
    {
      "epoch": 4670.940170940171,
      "grad_norm": 8.000950813293457,
      "learning_rate": 3.290598290598291e-06,
      "loss": 3.8532,
      "step": 546500
    },
    {
      "epoch": 4675.213675213675,
      "grad_norm": 5.900455474853516,
      "learning_rate": 3.247863247863248e-06,
      "loss": 3.8422,
      "step": 547000
    },
    {
      "epoch": 4679.48717948718,
      "grad_norm": 5.605871200561523,
      "learning_rate": 3.205128205128205e-06,
      "loss": 3.8393,
      "step": 547500
    }
  ],
  "logging_steps": 500,
  "max_steps": 585000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5000,
  "save_steps": 14800,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.004194598020608e+16,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
