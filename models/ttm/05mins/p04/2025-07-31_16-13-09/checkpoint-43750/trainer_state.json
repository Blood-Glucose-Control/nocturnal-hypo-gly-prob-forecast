{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 50.0,
  "eval_steps": 500,
  "global_step": 43750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 5.58757209777832,
      "learning_rate": 4.942857142857143e-05,
      "loss": 8.3936,
      "step": 500
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 7.424070835113525,
      "learning_rate": 4.885714285714286e-05,
      "loss": 7.8113,
      "step": 1000
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 5.702758312225342,
      "learning_rate": 4.828571428571429e-05,
      "loss": 7.6395,
      "step": 1500
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 10.908013343811035,
      "learning_rate": 4.771428571428572e-05,
      "loss": 7.5339,
      "step": 2000
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 16.614065170288086,
      "learning_rate": 4.714285714285714e-05,
      "loss": 7.4883,
      "step": 2500
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 7.391665458679199,
      "learning_rate": 4.6571428571428575e-05,
      "loss": 7.5124,
      "step": 3000
    },
    {
      "epoch": 4.0,
      "grad_norm": 14.042767524719238,
      "learning_rate": 4.600000000000001e-05,
      "loss": 7.421,
      "step": 3500
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 8.219549179077148,
      "learning_rate": 4.542857142857143e-05,
      "loss": 7.4981,
      "step": 4000
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 6.5239667892456055,
      "learning_rate": 4.485714285714286e-05,
      "loss": 7.3364,
      "step": 4500
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 13.767611503601074,
      "learning_rate": 4.428571428571428e-05,
      "loss": 7.3963,
      "step": 5000
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 16.534208297729492,
      "learning_rate": 4.371428571428572e-05,
      "loss": 7.3777,
      "step": 5500
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 5.128609657287598,
      "learning_rate": 4.314285714285715e-05,
      "loss": 7.3864,
      "step": 6000
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 8.081981658935547,
      "learning_rate": 4.257142857142857e-05,
      "loss": 7.3211,
      "step": 6500
    },
    {
      "epoch": 8.0,
      "grad_norm": 16.891752243041992,
      "learning_rate": 4.2e-05,
      "loss": 7.3583,
      "step": 7000
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 5.876380920410156,
      "learning_rate": 4.1428571428571437e-05,
      "loss": 7.3571,
      "step": 7500
    },
    {
      "epoch": 9.142857142857142,
      "grad_norm": 13.81696605682373,
      "learning_rate": 4.085714285714286e-05,
      "loss": 7.3521,
      "step": 8000
    },
    {
      "epoch": 9.714285714285714,
      "grad_norm": 7.024388790130615,
      "learning_rate": 4.028571428571429e-05,
      "loss": 7.3149,
      "step": 8500
    },
    {
      "epoch": 10.285714285714286,
      "grad_norm": 19.330591201782227,
      "learning_rate": 3.971428571428571e-05,
      "loss": 7.2599,
      "step": 9000
    },
    {
      "epoch": 10.857142857142858,
      "grad_norm": 22.623178482055664,
      "learning_rate": 3.9142857142857145e-05,
      "loss": 7.3115,
      "step": 9500
    },
    {
      "epoch": 11.428571428571429,
      "grad_norm": 9.652050971984863,
      "learning_rate": 3.857142857142858e-05,
      "loss": 7.3311,
      "step": 10000
    },
    {
      "epoch": 12.0,
      "grad_norm": 12.707389831542969,
      "learning_rate": 3.8e-05,
      "loss": 7.2679,
      "step": 10500
    },
    {
      "epoch": 12.571428571428571,
      "grad_norm": 11.45791244506836,
      "learning_rate": 3.742857142857143e-05,
      "loss": 7.3437,
      "step": 11000
    },
    {
      "epoch": 13.142857142857142,
      "grad_norm": 7.195952892303467,
      "learning_rate": 3.685714285714286e-05,
      "loss": 7.2447,
      "step": 11500
    },
    {
      "epoch": 13.714285714285714,
      "grad_norm": 5.836419105529785,
      "learning_rate": 3.628571428571429e-05,
      "loss": 7.2974,
      "step": 12000
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 21.791732788085938,
      "learning_rate": 3.571428571428572e-05,
      "loss": 7.2562,
      "step": 12500
    },
    {
      "epoch": 14.857142857142858,
      "grad_norm": 13.387743949890137,
      "learning_rate": 3.514285714285714e-05,
      "loss": 7.2819,
      "step": 13000
    },
    {
      "epoch": 15.428571428571429,
      "grad_norm": 7.799625873565674,
      "learning_rate": 3.4571428571428574e-05,
      "loss": 7.286,
      "step": 13500
    },
    {
      "epoch": 16.0,
      "grad_norm": 9.646977424621582,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 7.2803,
      "step": 14000
    },
    {
      "epoch": 16.571428571428573,
      "grad_norm": 32.57575225830078,
      "learning_rate": 3.342857142857143e-05,
      "loss": 7.2405,
      "step": 14500
    },
    {
      "epoch": 17.142857142857142,
      "grad_norm": 19.019855499267578,
      "learning_rate": 3.285714285714286e-05,
      "loss": 7.2914,
      "step": 15000
    },
    {
      "epoch": 17.714285714285715,
      "grad_norm": 5.528715133666992,
      "learning_rate": 3.228571428571428e-05,
      "loss": 7.309,
      "step": 15500
    },
    {
      "epoch": 18.285714285714285,
      "grad_norm": 5.394944667816162,
      "learning_rate": 3.1714285714285715e-05,
      "loss": 7.2597,
      "step": 16000
    },
    {
      "epoch": 18.857142857142858,
      "grad_norm": 10.446709632873535,
      "learning_rate": 3.114285714285715e-05,
      "loss": 7.218,
      "step": 16500
    },
    {
      "epoch": 19.428571428571427,
      "grad_norm": 14.233480453491211,
      "learning_rate": 3.057142857142857e-05,
      "loss": 7.2513,
      "step": 17000
    },
    {
      "epoch": 20.0,
      "grad_norm": 9.860044479370117,
      "learning_rate": 3e-05,
      "loss": 7.2765,
      "step": 17500
    },
    {
      "epoch": 20.571428571428573,
      "grad_norm": 6.343319892883301,
      "learning_rate": 2.9428571428571426e-05,
      "loss": 7.2252,
      "step": 18000
    },
    {
      "epoch": 21.142857142857142,
      "grad_norm": 16.30672264099121,
      "learning_rate": 2.885714285714286e-05,
      "loss": 7.2347,
      "step": 18500
    },
    {
      "epoch": 21.714285714285715,
      "grad_norm": 11.967723846435547,
      "learning_rate": 2.8285714285714287e-05,
      "loss": 7.237,
      "step": 19000
    },
    {
      "epoch": 22.285714285714285,
      "grad_norm": 9.713333129882812,
      "learning_rate": 2.7714285714285716e-05,
      "loss": 7.2786,
      "step": 19500
    },
    {
      "epoch": 22.857142857142858,
      "grad_norm": 8.335389137268066,
      "learning_rate": 2.714285714285714e-05,
      "loss": 7.2919,
      "step": 20000
    },
    {
      "epoch": 23.428571428571427,
      "grad_norm": 17.66240119934082,
      "learning_rate": 2.6571428571428576e-05,
      "loss": 7.2363,
      "step": 20500
    },
    {
      "epoch": 24.0,
      "grad_norm": 8.231350898742676,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 7.2489,
      "step": 21000
    },
    {
      "epoch": 24.571428571428573,
      "grad_norm": 19.843929290771484,
      "learning_rate": 2.542857142857143e-05,
      "loss": 7.2507,
      "step": 21500
    },
    {
      "epoch": 25.142857142857142,
      "grad_norm": 5.437232494354248,
      "learning_rate": 2.485714285714286e-05,
      "loss": 7.2508,
      "step": 22000
    },
    {
      "epoch": 25.714285714285715,
      "grad_norm": 11.1651029586792,
      "learning_rate": 2.4285714285714288e-05,
      "loss": 7.2306,
      "step": 22500
    },
    {
      "epoch": 26.285714285714285,
      "grad_norm": 8.333647727966309,
      "learning_rate": 2.3714285714285717e-05,
      "loss": 7.279,
      "step": 23000
    },
    {
      "epoch": 26.857142857142858,
      "grad_norm": 20.937885284423828,
      "learning_rate": 2.3142857142857145e-05,
      "loss": 7.2012,
      "step": 23500
    },
    {
      "epoch": 27.428571428571427,
      "grad_norm": 12.546097755432129,
      "learning_rate": 2.257142857142857e-05,
      "loss": 7.2319,
      "step": 24000
    },
    {
      "epoch": 28.0,
      "grad_norm": 8.278054237365723,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 7.27,
      "step": 24500
    },
    {
      "epoch": 28.571428571428573,
      "grad_norm": 22.423219680786133,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 7.2602,
      "step": 25000
    },
    {
      "epoch": 29.142857142857142,
      "grad_norm": 22.440433502197266,
      "learning_rate": 2.0857142857142857e-05,
      "loss": 7.2441,
      "step": 25500
    },
    {
      "epoch": 29.714285714285715,
      "grad_norm": 7.129939556121826,
      "learning_rate": 2.0285714285714286e-05,
      "loss": 7.2252,
      "step": 26000
    },
    {
      "epoch": 30.285714285714285,
      "grad_norm": 14.615425109863281,
      "learning_rate": 1.9714285714285714e-05,
      "loss": 7.2273,
      "step": 26500
    },
    {
      "epoch": 30.857142857142858,
      "grad_norm": 7.901130199432373,
      "learning_rate": 1.9142857142857143e-05,
      "loss": 7.25,
      "step": 27000
    },
    {
      "epoch": 31.428571428571427,
      "grad_norm": 5.554603576660156,
      "learning_rate": 1.8571428571428572e-05,
      "loss": 7.1943,
      "step": 27500
    },
    {
      "epoch": 32.0,
      "grad_norm": 12.093487739562988,
      "learning_rate": 1.8e-05,
      "loss": 7.2264,
      "step": 28000
    },
    {
      "epoch": 32.57142857142857,
      "grad_norm": 12.546833038330078,
      "learning_rate": 1.742857142857143e-05,
      "loss": 7.1989,
      "step": 28500
    },
    {
      "epoch": 33.142857142857146,
      "grad_norm": 6.758516311645508,
      "learning_rate": 1.6857142857142858e-05,
      "loss": 7.2847,
      "step": 29000
    },
    {
      "epoch": 33.714285714285715,
      "grad_norm": 17.785158157348633,
      "learning_rate": 1.6285714285714287e-05,
      "loss": 7.2055,
      "step": 29500
    },
    {
      "epoch": 34.285714285714285,
      "grad_norm": 9.165641784667969,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 7.2129,
      "step": 30000
    },
    {
      "epoch": 34.857142857142854,
      "grad_norm": 20.76511001586914,
      "learning_rate": 1.5142857142857144e-05,
      "loss": 7.2616,
      "step": 30500
    },
    {
      "epoch": 35.42857142857143,
      "grad_norm": 15.01212215423584,
      "learning_rate": 1.4571428571428573e-05,
      "loss": 7.2367,
      "step": 31000
    },
    {
      "epoch": 36.0,
      "grad_norm": 18.20206069946289,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 7.2112,
      "step": 31500
    },
    {
      "epoch": 36.57142857142857,
      "grad_norm": 31.022611618041992,
      "learning_rate": 1.3428571428571429e-05,
      "loss": 7.2234,
      "step": 32000
    },
    {
      "epoch": 37.142857142857146,
      "grad_norm": 4.970174789428711,
      "learning_rate": 1.2857142857142857e-05,
      "loss": 7.2427,
      "step": 32500
    },
    {
      "epoch": 37.714285714285715,
      "grad_norm": 6.399604320526123,
      "learning_rate": 1.2285714285714286e-05,
      "loss": 7.2349,
      "step": 33000
    },
    {
      "epoch": 38.285714285714285,
      "grad_norm": 26.749292373657227,
      "learning_rate": 1.1714285714285715e-05,
      "loss": 7.1798,
      "step": 33500
    },
    {
      "epoch": 38.857142857142854,
      "grad_norm": 15.788073539733887,
      "learning_rate": 1.1142857142857143e-05,
      "loss": 7.2036,
      "step": 34000
    },
    {
      "epoch": 39.42857142857143,
      "grad_norm": 20.096532821655273,
      "learning_rate": 1.0571428571428572e-05,
      "loss": 7.2839,
      "step": 34500
    },
    {
      "epoch": 40.0,
      "grad_norm": 12.579891204833984,
      "learning_rate": 1e-05,
      "loss": 7.2045,
      "step": 35000
    },
    {
      "epoch": 40.57142857142857,
      "grad_norm": 7.6992878913879395,
      "learning_rate": 9.42857142857143e-06,
      "loss": 7.2219,
      "step": 35500
    },
    {
      "epoch": 41.142857142857146,
      "grad_norm": 21.573102951049805,
      "learning_rate": 8.857142857142857e-06,
      "loss": 7.2812,
      "step": 36000
    },
    {
      "epoch": 41.714285714285715,
      "grad_norm": 7.441547870635986,
      "learning_rate": 8.285714285714285e-06,
      "loss": 7.1722,
      "step": 36500
    },
    {
      "epoch": 42.285714285714285,
      "grad_norm": 10.349288940429688,
      "learning_rate": 7.714285714285714e-06,
      "loss": 7.2106,
      "step": 37000
    },
    {
      "epoch": 42.857142857142854,
      "grad_norm": 10.044301986694336,
      "learning_rate": 7.142857142857143e-06,
      "loss": 7.2001,
      "step": 37500
    },
    {
      "epoch": 43.42857142857143,
      "grad_norm": 8.19778823852539,
      "learning_rate": 6.5714285714285714e-06,
      "loss": 7.246,
      "step": 38000
    },
    {
      "epoch": 44.0,
      "grad_norm": 7.752064228057861,
      "learning_rate": 6e-06,
      "loss": 7.2265,
      "step": 38500
    },
    {
      "epoch": 44.57142857142857,
      "grad_norm": 6.317877292633057,
      "learning_rate": 5.428571428571429e-06,
      "loss": 7.2468,
      "step": 39000
    },
    {
      "epoch": 45.142857142857146,
      "grad_norm": 14.00491714477539,
      "learning_rate": 4.857142857142858e-06,
      "loss": 7.1944,
      "step": 39500
    },
    {
      "epoch": 45.714285714285715,
      "grad_norm": 20.586681365966797,
      "learning_rate": 4.285714285714286e-06,
      "loss": 7.2327,
      "step": 40000
    },
    {
      "epoch": 46.285714285714285,
      "grad_norm": 12.588866233825684,
      "learning_rate": 3.7142857142857146e-06,
      "loss": 7.1623,
      "step": 40500
    },
    {
      "epoch": 46.857142857142854,
      "grad_norm": 9.922075271606445,
      "learning_rate": 3.1428571428571433e-06,
      "loss": 7.2617,
      "step": 41000
    },
    {
      "epoch": 47.42857142857143,
      "grad_norm": 6.454334259033203,
      "learning_rate": 2.5714285714285716e-06,
      "loss": 7.2172,
      "step": 41500
    },
    {
      "epoch": 48.0,
      "grad_norm": 20.39872932434082,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 7.2255,
      "step": 42000
    },
    {
      "epoch": 48.57142857142857,
      "grad_norm": 19.511459350585938,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 7.2639,
      "step": 42500
    },
    {
      "epoch": 49.142857142857146,
      "grad_norm": 7.848310947418213,
      "learning_rate": 8.571428571428572e-07,
      "loss": 7.1935,
      "step": 43000
    },
    {
      "epoch": 49.714285714285715,
      "grad_norm": 11.429476737976074,
      "learning_rate": 2.8571428571428575e-07,
      "loss": 7.2156,
      "step": 43500
    }
  ],
  "logging_steps": 500,
  "max_steps": 43750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 109500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1862824785120000.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
