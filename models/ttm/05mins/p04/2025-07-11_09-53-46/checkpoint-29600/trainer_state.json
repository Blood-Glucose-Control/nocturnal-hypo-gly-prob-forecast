{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 252.991452991453,
  "eval_steps": 500,
  "global_step": 29600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.273504273504273,
      "grad_norm": 3.1177163124084473,
      "learning_rate": 4.995726495726496e-05,
      "loss": 5.1253,
      "step": 500
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 4.9700751304626465,
      "learning_rate": 4.991452991452992e-05,
      "loss": 4.7893,
      "step": 1000
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 5.357769966125488,
      "learning_rate": 4.9871794871794874e-05,
      "loss": 4.6697,
      "step": 1500
    },
    {
      "epoch": 17.094017094017094,
      "grad_norm": 6.760709285736084,
      "learning_rate": 4.982905982905983e-05,
      "loss": 4.5894,
      "step": 2000
    },
    {
      "epoch": 21.367521367521366,
      "grad_norm": 7.849138259887695,
      "learning_rate": 4.978632478632479e-05,
      "loss": 4.5275,
      "step": 2500
    },
    {
      "epoch": 25.641025641025642,
      "grad_norm": 13.399908065795898,
      "learning_rate": 4.9743589743589746e-05,
      "loss": 4.4858,
      "step": 3000
    },
    {
      "epoch": 29.914529914529915,
      "grad_norm": 5.951698303222656,
      "learning_rate": 4.97008547008547e-05,
      "loss": 4.4704,
      "step": 3500
    },
    {
      "epoch": 34.18803418803419,
      "grad_norm": 12.644576072692871,
      "learning_rate": 4.965811965811966e-05,
      "loss": 4.4274,
      "step": 4000
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 7.810548305511475,
      "learning_rate": 4.961538461538462e-05,
      "loss": 4.4039,
      "step": 4500
    },
    {
      "epoch": 42.73504273504273,
      "grad_norm": 11.115514755249023,
      "learning_rate": 4.9572649572649575e-05,
      "loss": 4.3992,
      "step": 5000
    },
    {
      "epoch": 47.00854700854701,
      "grad_norm": 7.523601055145264,
      "learning_rate": 4.952991452991453e-05,
      "loss": 4.3591,
      "step": 5500
    },
    {
      "epoch": 51.282051282051285,
      "grad_norm": 5.32051944732666,
      "learning_rate": 4.948717948717949e-05,
      "loss": 4.3436,
      "step": 6000
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 5.7435197830200195,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 4.3226,
      "step": 6500
    },
    {
      "epoch": 59.82905982905983,
      "grad_norm": 15.95846939086914,
      "learning_rate": 4.94017094017094e-05,
      "loss": 4.311,
      "step": 7000
    },
    {
      "epoch": 64.1025641025641,
      "grad_norm": 9.219342231750488,
      "learning_rate": 4.935897435897436e-05,
      "loss": 4.3008,
      "step": 7500
    },
    {
      "epoch": 68.37606837606837,
      "grad_norm": 6.521927833557129,
      "learning_rate": 4.931623931623932e-05,
      "loss": 4.2793,
      "step": 8000
    },
    {
      "epoch": 72.64957264957265,
      "grad_norm": 10.859046936035156,
      "learning_rate": 4.927350427350428e-05,
      "loss": 4.2699,
      "step": 8500
    },
    {
      "epoch": 76.92307692307692,
      "grad_norm": 6.567059516906738,
      "learning_rate": 4.923076923076924e-05,
      "loss": 4.2601,
      "step": 9000
    },
    {
      "epoch": 81.19658119658119,
      "grad_norm": 10.567413330078125,
      "learning_rate": 4.918803418803419e-05,
      "loss": 4.248,
      "step": 9500
    },
    {
      "epoch": 85.47008547008546,
      "grad_norm": 7.6750688552856445,
      "learning_rate": 4.9145299145299147e-05,
      "loss": 4.2358,
      "step": 10000
    },
    {
      "epoch": 89.74358974358974,
      "grad_norm": 8.19306755065918,
      "learning_rate": 4.9102564102564104e-05,
      "loss": 4.2315,
      "step": 10500
    },
    {
      "epoch": 94.01709401709402,
      "grad_norm": 6.8381524085998535,
      "learning_rate": 4.905982905982906e-05,
      "loss": 4.2126,
      "step": 11000
    },
    {
      "epoch": 98.2905982905983,
      "grad_norm": 6.028937339782715,
      "learning_rate": 4.901709401709402e-05,
      "loss": 4.2104,
      "step": 11500
    },
    {
      "epoch": 102.56410256410257,
      "grad_norm": 6.542298316955566,
      "learning_rate": 4.8974358974358975e-05,
      "loss": 4.1973,
      "step": 12000
    },
    {
      "epoch": 106.83760683760684,
      "grad_norm": 7.324919700622559,
      "learning_rate": 4.893162393162393e-05,
      "loss": 4.2057,
      "step": 12500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 10.700521469116211,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.1824,
      "step": 13000
    },
    {
      "epoch": 115.38461538461539,
      "grad_norm": 10.119913101196289,
      "learning_rate": 4.884615384615385e-05,
      "loss": 4.1825,
      "step": 13500
    },
    {
      "epoch": 119.65811965811966,
      "grad_norm": 6.354224681854248,
      "learning_rate": 4.8803418803418804e-05,
      "loss": 4.181,
      "step": 14000
    },
    {
      "epoch": 123.93162393162393,
      "grad_norm": 10.041450500488281,
      "learning_rate": 4.876068376068376e-05,
      "loss": 4.1606,
      "step": 14500
    },
    {
      "epoch": 128.2051282051282,
      "grad_norm": 8.247559547424316,
      "learning_rate": 4.871794871794872e-05,
      "loss": 4.1579,
      "step": 15000
    },
    {
      "epoch": 132.47863247863248,
      "grad_norm": 7.573277950286865,
      "learning_rate": 4.8675213675213676e-05,
      "loss": 4.1568,
      "step": 15500
    },
    {
      "epoch": 136.75213675213675,
      "grad_norm": 8.592423439025879,
      "learning_rate": 4.863247863247863e-05,
      "loss": 4.1464,
      "step": 16000
    },
    {
      "epoch": 141.02564102564102,
      "grad_norm": 7.11668062210083,
      "learning_rate": 4.858974358974359e-05,
      "loss": 4.139,
      "step": 16500
    },
    {
      "epoch": 145.2991452991453,
      "grad_norm": 6.893136978149414,
      "learning_rate": 4.854700854700855e-05,
      "loss": 4.1334,
      "step": 17000
    },
    {
      "epoch": 149.57264957264957,
      "grad_norm": 8.224817276000977,
      "learning_rate": 4.8504273504273505e-05,
      "loss": 4.1382,
      "step": 17500
    },
    {
      "epoch": 153.84615384615384,
      "grad_norm": 7.110662460327148,
      "learning_rate": 4.846153846153846e-05,
      "loss": 4.1192,
      "step": 18000
    },
    {
      "epoch": 158.1196581196581,
      "grad_norm": 8.336471557617188,
      "learning_rate": 4.841880341880342e-05,
      "loss": 4.1298,
      "step": 18500
    },
    {
      "epoch": 162.39316239316238,
      "grad_norm": 10.689091682434082,
      "learning_rate": 4.8376068376068376e-05,
      "loss": 4.1223,
      "step": 19000
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 8.600497245788574,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 4.1124,
      "step": 19500
    },
    {
      "epoch": 170.94017094017093,
      "grad_norm": 6.878803730010986,
      "learning_rate": 4.829059829059829e-05,
      "loss": 4.1094,
      "step": 20000
    },
    {
      "epoch": 175.2136752136752,
      "grad_norm": 9.887246131896973,
      "learning_rate": 4.824786324786325e-05,
      "loss": 4.1045,
      "step": 20500
    },
    {
      "epoch": 179.48717948717947,
      "grad_norm": 7.862114906311035,
      "learning_rate": 4.8205128205128205e-05,
      "loss": 4.0999,
      "step": 21000
    },
    {
      "epoch": 183.76068376068375,
      "grad_norm": 12.425511360168457,
      "learning_rate": 4.816239316239316e-05,
      "loss": 4.1091,
      "step": 21500
    },
    {
      "epoch": 188.03418803418805,
      "grad_norm": 6.507553577423096,
      "learning_rate": 4.8119658119658126e-05,
      "loss": 4.0822,
      "step": 22000
    },
    {
      "epoch": 192.30769230769232,
      "grad_norm": 8.65228271484375,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 4.0936,
      "step": 22500
    },
    {
      "epoch": 196.5811965811966,
      "grad_norm": 6.597097873687744,
      "learning_rate": 4.803418803418804e-05,
      "loss": 4.0898,
      "step": 23000
    },
    {
      "epoch": 200.85470085470087,
      "grad_norm": 9.715621948242188,
      "learning_rate": 4.7991452991453e-05,
      "loss": 4.0749,
      "step": 23500
    },
    {
      "epoch": 205.12820512820514,
      "grad_norm": 12.793821334838867,
      "learning_rate": 4.7948717948717955e-05,
      "loss": 4.0855,
      "step": 24000
    },
    {
      "epoch": 209.4017094017094,
      "grad_norm": 6.782865047454834,
      "learning_rate": 4.790598290598291e-05,
      "loss": 4.0723,
      "step": 24500
    },
    {
      "epoch": 213.67521367521368,
      "grad_norm": 7.109498500823975,
      "learning_rate": 4.786324786324787e-05,
      "loss": 4.0779,
      "step": 25000
    },
    {
      "epoch": 217.94871794871796,
      "grad_norm": 13.188231468200684,
      "learning_rate": 4.782051282051283e-05,
      "loss": 4.0752,
      "step": 25500
    },
    {
      "epoch": 222.22222222222223,
      "grad_norm": 4.958047866821289,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 4.0685,
      "step": 26000
    },
    {
      "epoch": 226.4957264957265,
      "grad_norm": 7.153029441833496,
      "learning_rate": 4.773504273504274e-05,
      "loss": 4.0625,
      "step": 26500
    },
    {
      "epoch": 230.76923076923077,
      "grad_norm": 6.8623046875,
      "learning_rate": 4.76923076923077e-05,
      "loss": 4.0617,
      "step": 27000
    },
    {
      "epoch": 235.04273504273505,
      "grad_norm": 5.7580485343933105,
      "learning_rate": 4.764957264957265e-05,
      "loss": 4.0614,
      "step": 27500
    },
    {
      "epoch": 239.31623931623932,
      "grad_norm": 9.779964447021484,
      "learning_rate": 4.7606837606837606e-05,
      "loss": 4.0556,
      "step": 28000
    },
    {
      "epoch": 243.5897435897436,
      "grad_norm": 11.769835472106934,
      "learning_rate": 4.7564102564102563e-05,
      "loss": 4.0585,
      "step": 28500
    },
    {
      "epoch": 247.86324786324786,
      "grad_norm": 6.6153154373168945,
      "learning_rate": 4.752136752136752e-05,
      "loss": 4.0474,
      "step": 29000
    },
    {
      "epoch": 252.13675213675214,
      "grad_norm": 4.998359203338623,
      "learning_rate": 4.747863247863248e-05,
      "loss": 4.0469,
      "step": 29500
    }
  ],
  "logging_steps": 500,
  "max_steps": 585000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5000,
  "save_steps": 14800,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3786098931528192.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
