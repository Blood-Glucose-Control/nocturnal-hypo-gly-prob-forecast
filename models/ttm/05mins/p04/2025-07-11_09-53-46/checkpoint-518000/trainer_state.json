{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4427.350427350428,
  "eval_steps": 500,
  "global_step": 518000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.273504273504273,
      "grad_norm": 3.1177163124084473,
      "learning_rate": 4.995726495726496e-05,
      "loss": 5.1253,
      "step": 500
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 4.9700751304626465,
      "learning_rate": 4.991452991452992e-05,
      "loss": 4.7893,
      "step": 1000
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 5.357769966125488,
      "learning_rate": 4.9871794871794874e-05,
      "loss": 4.6697,
      "step": 1500
    },
    {
      "epoch": 17.094017094017094,
      "grad_norm": 6.760709285736084,
      "learning_rate": 4.982905982905983e-05,
      "loss": 4.5894,
      "step": 2000
    },
    {
      "epoch": 21.367521367521366,
      "grad_norm": 7.849138259887695,
      "learning_rate": 4.978632478632479e-05,
      "loss": 4.5275,
      "step": 2500
    },
    {
      "epoch": 25.641025641025642,
      "grad_norm": 13.399908065795898,
      "learning_rate": 4.9743589743589746e-05,
      "loss": 4.4858,
      "step": 3000
    },
    {
      "epoch": 29.914529914529915,
      "grad_norm": 5.951698303222656,
      "learning_rate": 4.97008547008547e-05,
      "loss": 4.4704,
      "step": 3500
    },
    {
      "epoch": 34.18803418803419,
      "grad_norm": 12.644576072692871,
      "learning_rate": 4.965811965811966e-05,
      "loss": 4.4274,
      "step": 4000
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 7.810548305511475,
      "learning_rate": 4.961538461538462e-05,
      "loss": 4.4039,
      "step": 4500
    },
    {
      "epoch": 42.73504273504273,
      "grad_norm": 11.115514755249023,
      "learning_rate": 4.9572649572649575e-05,
      "loss": 4.3992,
      "step": 5000
    },
    {
      "epoch": 47.00854700854701,
      "grad_norm": 7.523601055145264,
      "learning_rate": 4.952991452991453e-05,
      "loss": 4.3591,
      "step": 5500
    },
    {
      "epoch": 51.282051282051285,
      "grad_norm": 5.32051944732666,
      "learning_rate": 4.948717948717949e-05,
      "loss": 4.3436,
      "step": 6000
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 5.7435197830200195,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 4.3226,
      "step": 6500
    },
    {
      "epoch": 59.82905982905983,
      "grad_norm": 15.95846939086914,
      "learning_rate": 4.94017094017094e-05,
      "loss": 4.311,
      "step": 7000
    },
    {
      "epoch": 64.1025641025641,
      "grad_norm": 9.219342231750488,
      "learning_rate": 4.935897435897436e-05,
      "loss": 4.3008,
      "step": 7500
    },
    {
      "epoch": 68.37606837606837,
      "grad_norm": 6.521927833557129,
      "learning_rate": 4.931623931623932e-05,
      "loss": 4.2793,
      "step": 8000
    },
    {
      "epoch": 72.64957264957265,
      "grad_norm": 10.859046936035156,
      "learning_rate": 4.927350427350428e-05,
      "loss": 4.2699,
      "step": 8500
    },
    {
      "epoch": 76.92307692307692,
      "grad_norm": 6.567059516906738,
      "learning_rate": 4.923076923076924e-05,
      "loss": 4.2601,
      "step": 9000
    },
    {
      "epoch": 81.19658119658119,
      "grad_norm": 10.567413330078125,
      "learning_rate": 4.918803418803419e-05,
      "loss": 4.248,
      "step": 9500
    },
    {
      "epoch": 85.47008547008546,
      "grad_norm": 7.6750688552856445,
      "learning_rate": 4.9145299145299147e-05,
      "loss": 4.2358,
      "step": 10000
    },
    {
      "epoch": 89.74358974358974,
      "grad_norm": 8.19306755065918,
      "learning_rate": 4.9102564102564104e-05,
      "loss": 4.2315,
      "step": 10500
    },
    {
      "epoch": 94.01709401709402,
      "grad_norm": 6.8381524085998535,
      "learning_rate": 4.905982905982906e-05,
      "loss": 4.2126,
      "step": 11000
    },
    {
      "epoch": 98.2905982905983,
      "grad_norm": 6.028937339782715,
      "learning_rate": 4.901709401709402e-05,
      "loss": 4.2104,
      "step": 11500
    },
    {
      "epoch": 102.56410256410257,
      "grad_norm": 6.542298316955566,
      "learning_rate": 4.8974358974358975e-05,
      "loss": 4.1973,
      "step": 12000
    },
    {
      "epoch": 106.83760683760684,
      "grad_norm": 7.324919700622559,
      "learning_rate": 4.893162393162393e-05,
      "loss": 4.2057,
      "step": 12500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 10.700521469116211,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.1824,
      "step": 13000
    },
    {
      "epoch": 115.38461538461539,
      "grad_norm": 10.119913101196289,
      "learning_rate": 4.884615384615385e-05,
      "loss": 4.1825,
      "step": 13500
    },
    {
      "epoch": 119.65811965811966,
      "grad_norm": 6.354224681854248,
      "learning_rate": 4.8803418803418804e-05,
      "loss": 4.181,
      "step": 14000
    },
    {
      "epoch": 123.93162393162393,
      "grad_norm": 10.041450500488281,
      "learning_rate": 4.876068376068376e-05,
      "loss": 4.1606,
      "step": 14500
    },
    {
      "epoch": 128.2051282051282,
      "grad_norm": 8.247559547424316,
      "learning_rate": 4.871794871794872e-05,
      "loss": 4.1579,
      "step": 15000
    },
    {
      "epoch": 132.47863247863248,
      "grad_norm": 7.573277950286865,
      "learning_rate": 4.8675213675213676e-05,
      "loss": 4.1568,
      "step": 15500
    },
    {
      "epoch": 136.75213675213675,
      "grad_norm": 8.592423439025879,
      "learning_rate": 4.863247863247863e-05,
      "loss": 4.1464,
      "step": 16000
    },
    {
      "epoch": 141.02564102564102,
      "grad_norm": 7.11668062210083,
      "learning_rate": 4.858974358974359e-05,
      "loss": 4.139,
      "step": 16500
    },
    {
      "epoch": 145.2991452991453,
      "grad_norm": 6.893136978149414,
      "learning_rate": 4.854700854700855e-05,
      "loss": 4.1334,
      "step": 17000
    },
    {
      "epoch": 149.57264957264957,
      "grad_norm": 8.224817276000977,
      "learning_rate": 4.8504273504273505e-05,
      "loss": 4.1382,
      "step": 17500
    },
    {
      "epoch": 153.84615384615384,
      "grad_norm": 7.110662460327148,
      "learning_rate": 4.846153846153846e-05,
      "loss": 4.1192,
      "step": 18000
    },
    {
      "epoch": 158.1196581196581,
      "grad_norm": 8.336471557617188,
      "learning_rate": 4.841880341880342e-05,
      "loss": 4.1298,
      "step": 18500
    },
    {
      "epoch": 162.39316239316238,
      "grad_norm": 10.689091682434082,
      "learning_rate": 4.8376068376068376e-05,
      "loss": 4.1223,
      "step": 19000
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 8.600497245788574,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 4.1124,
      "step": 19500
    },
    {
      "epoch": 170.94017094017093,
      "grad_norm": 6.878803730010986,
      "learning_rate": 4.829059829059829e-05,
      "loss": 4.1094,
      "step": 20000
    },
    {
      "epoch": 175.2136752136752,
      "grad_norm": 9.887246131896973,
      "learning_rate": 4.824786324786325e-05,
      "loss": 4.1045,
      "step": 20500
    },
    {
      "epoch": 179.48717948717947,
      "grad_norm": 7.862114906311035,
      "learning_rate": 4.8205128205128205e-05,
      "loss": 4.0999,
      "step": 21000
    },
    {
      "epoch": 183.76068376068375,
      "grad_norm": 12.425511360168457,
      "learning_rate": 4.816239316239316e-05,
      "loss": 4.1091,
      "step": 21500
    },
    {
      "epoch": 188.03418803418805,
      "grad_norm": 6.507553577423096,
      "learning_rate": 4.8119658119658126e-05,
      "loss": 4.0822,
      "step": 22000
    },
    {
      "epoch": 192.30769230769232,
      "grad_norm": 8.65228271484375,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 4.0936,
      "step": 22500
    },
    {
      "epoch": 196.5811965811966,
      "grad_norm": 6.597097873687744,
      "learning_rate": 4.803418803418804e-05,
      "loss": 4.0898,
      "step": 23000
    },
    {
      "epoch": 200.85470085470087,
      "grad_norm": 9.715621948242188,
      "learning_rate": 4.7991452991453e-05,
      "loss": 4.0749,
      "step": 23500
    },
    {
      "epoch": 205.12820512820514,
      "grad_norm": 12.793821334838867,
      "learning_rate": 4.7948717948717955e-05,
      "loss": 4.0855,
      "step": 24000
    },
    {
      "epoch": 209.4017094017094,
      "grad_norm": 6.782865047454834,
      "learning_rate": 4.790598290598291e-05,
      "loss": 4.0723,
      "step": 24500
    },
    {
      "epoch": 213.67521367521368,
      "grad_norm": 7.109498500823975,
      "learning_rate": 4.786324786324787e-05,
      "loss": 4.0779,
      "step": 25000
    },
    {
      "epoch": 217.94871794871796,
      "grad_norm": 13.188231468200684,
      "learning_rate": 4.782051282051283e-05,
      "loss": 4.0752,
      "step": 25500
    },
    {
      "epoch": 222.22222222222223,
      "grad_norm": 4.958047866821289,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 4.0685,
      "step": 26000
    },
    {
      "epoch": 226.4957264957265,
      "grad_norm": 7.153029441833496,
      "learning_rate": 4.773504273504274e-05,
      "loss": 4.0625,
      "step": 26500
    },
    {
      "epoch": 230.76923076923077,
      "grad_norm": 6.8623046875,
      "learning_rate": 4.76923076923077e-05,
      "loss": 4.0617,
      "step": 27000
    },
    {
      "epoch": 235.04273504273505,
      "grad_norm": 5.7580485343933105,
      "learning_rate": 4.764957264957265e-05,
      "loss": 4.0614,
      "step": 27500
    },
    {
      "epoch": 239.31623931623932,
      "grad_norm": 9.779964447021484,
      "learning_rate": 4.7606837606837606e-05,
      "loss": 4.0556,
      "step": 28000
    },
    {
      "epoch": 243.5897435897436,
      "grad_norm": 11.769835472106934,
      "learning_rate": 4.7564102564102563e-05,
      "loss": 4.0585,
      "step": 28500
    },
    {
      "epoch": 247.86324786324786,
      "grad_norm": 6.6153154373168945,
      "learning_rate": 4.752136752136752e-05,
      "loss": 4.0474,
      "step": 29000
    },
    {
      "epoch": 252.13675213675214,
      "grad_norm": 4.998359203338623,
      "learning_rate": 4.747863247863248e-05,
      "loss": 4.0469,
      "step": 29500
    },
    {
      "epoch": 256.4102564102564,
      "grad_norm": 11.855521202087402,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 4.0443,
      "step": 30000
    },
    {
      "epoch": 260.6837606837607,
      "grad_norm": 5.53583288192749,
      "learning_rate": 4.739316239316239e-05,
      "loss": 4.0498,
      "step": 30500
    },
    {
      "epoch": 264.95726495726495,
      "grad_norm": 13.338637351989746,
      "learning_rate": 4.735042735042735e-05,
      "loss": 4.0511,
      "step": 31000
    },
    {
      "epoch": 269.2307692307692,
      "grad_norm": 5.888394832611084,
      "learning_rate": 4.730769230769231e-05,
      "loss": 4.0344,
      "step": 31500
    },
    {
      "epoch": 273.5042735042735,
      "grad_norm": 8.230168342590332,
      "learning_rate": 4.7264957264957264e-05,
      "loss": 4.0365,
      "step": 32000
    },
    {
      "epoch": 277.77777777777777,
      "grad_norm": 8.78708267211914,
      "learning_rate": 4.722222222222222e-05,
      "loss": 4.0326,
      "step": 32500
    },
    {
      "epoch": 282.05128205128204,
      "grad_norm": 9.223427772521973,
      "learning_rate": 4.717948717948718e-05,
      "loss": 4.0342,
      "step": 33000
    },
    {
      "epoch": 286.3247863247863,
      "grad_norm": 8.614336013793945,
      "learning_rate": 4.7136752136752136e-05,
      "loss": 4.0241,
      "step": 33500
    },
    {
      "epoch": 290.5982905982906,
      "grad_norm": 6.094795227050781,
      "learning_rate": 4.709401709401709e-05,
      "loss": 4.0278,
      "step": 34000
    },
    {
      "epoch": 294.87179487179486,
      "grad_norm": 7.887334823608398,
      "learning_rate": 4.705128205128205e-05,
      "loss": 4.0384,
      "step": 34500
    },
    {
      "epoch": 299.14529914529913,
      "grad_norm": 6.632718086242676,
      "learning_rate": 4.700854700854701e-05,
      "loss": 4.0288,
      "step": 35000
    },
    {
      "epoch": 303.4188034188034,
      "grad_norm": 7.052670001983643,
      "learning_rate": 4.6965811965811964e-05,
      "loss": 4.026,
      "step": 35500
    },
    {
      "epoch": 307.6923076923077,
      "grad_norm": 12.567428588867188,
      "learning_rate": 4.692307692307693e-05,
      "loss": 4.0186,
      "step": 36000
    },
    {
      "epoch": 311.96581196581195,
      "grad_norm": 8.308412551879883,
      "learning_rate": 4.6880341880341886e-05,
      "loss": 4.0384,
      "step": 36500
    },
    {
      "epoch": 316.2393162393162,
      "grad_norm": 6.915613651275635,
      "learning_rate": 4.683760683760684e-05,
      "loss": 4.0256,
      "step": 37000
    },
    {
      "epoch": 320.5128205128205,
      "grad_norm": 11.106752395629883,
      "learning_rate": 4.67948717948718e-05,
      "loss": 4.0234,
      "step": 37500
    },
    {
      "epoch": 324.78632478632477,
      "grad_norm": 9.530385971069336,
      "learning_rate": 4.675213675213676e-05,
      "loss": 4.0134,
      "step": 38000
    },
    {
      "epoch": 329.05982905982904,
      "grad_norm": 7.745054721832275,
      "learning_rate": 4.6709401709401714e-05,
      "loss": 4.0107,
      "step": 38500
    },
    {
      "epoch": 333.3333333333333,
      "grad_norm": 7.7903923988342285,
      "learning_rate": 4.666666666666667e-05,
      "loss": 4.0206,
      "step": 39000
    },
    {
      "epoch": 337.6068376068376,
      "grad_norm": 12.71082592010498,
      "learning_rate": 4.662393162393163e-05,
      "loss": 4.009,
      "step": 39500
    },
    {
      "epoch": 341.88034188034186,
      "grad_norm": 9.180649757385254,
      "learning_rate": 4.6581196581196586e-05,
      "loss": 4.0012,
      "step": 40000
    },
    {
      "epoch": 346.15384615384613,
      "grad_norm": 7.996723175048828,
      "learning_rate": 4.653846153846154e-05,
      "loss": 4.0107,
      "step": 40500
    },
    {
      "epoch": 350.4273504273504,
      "grad_norm": 6.678990840911865,
      "learning_rate": 4.64957264957265e-05,
      "loss": 4.0046,
      "step": 41000
    },
    {
      "epoch": 354.7008547008547,
      "grad_norm": 6.712183475494385,
      "learning_rate": 4.645299145299146e-05,
      "loss": 4.0028,
      "step": 41500
    },
    {
      "epoch": 358.97435897435895,
      "grad_norm": 6.708301067352295,
      "learning_rate": 4.6410256410256415e-05,
      "loss": 3.9945,
      "step": 42000
    },
    {
      "epoch": 363.2478632478632,
      "grad_norm": 7.1230268478393555,
      "learning_rate": 4.636752136752137e-05,
      "loss": 4.0007,
      "step": 42500
    },
    {
      "epoch": 367.5213675213675,
      "grad_norm": 10.119011878967285,
      "learning_rate": 4.632478632478633e-05,
      "loss": 4.007,
      "step": 43000
    },
    {
      "epoch": 371.79487179487177,
      "grad_norm": 7.079206466674805,
      "learning_rate": 4.6282051282051287e-05,
      "loss": 4.0029,
      "step": 43500
    },
    {
      "epoch": 376.0683760683761,
      "grad_norm": 12.11133861541748,
      "learning_rate": 4.6239316239316244e-05,
      "loss": 3.9995,
      "step": 44000
    },
    {
      "epoch": 380.34188034188037,
      "grad_norm": 11.6453857421875,
      "learning_rate": 4.61965811965812e-05,
      "loss": 3.9973,
      "step": 44500
    },
    {
      "epoch": 384.61538461538464,
      "grad_norm": 8.945377349853516,
      "learning_rate": 4.615384615384616e-05,
      "loss": 3.9993,
      "step": 45000
    },
    {
      "epoch": 388.8888888888889,
      "grad_norm": 7.565587997436523,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 4.002,
      "step": 45500
    },
    {
      "epoch": 393.1623931623932,
      "grad_norm": 9.409317016601562,
      "learning_rate": 4.6068376068376066e-05,
      "loss": 3.9949,
      "step": 46000
    },
    {
      "epoch": 397.43589743589746,
      "grad_norm": 9.163902282714844,
      "learning_rate": 4.602564102564102e-05,
      "loss": 3.9962,
      "step": 46500
    },
    {
      "epoch": 401.70940170940173,
      "grad_norm": 7.880346775054932,
      "learning_rate": 4.598290598290598e-05,
      "loss": 3.9968,
      "step": 47000
    },
    {
      "epoch": 405.982905982906,
      "grad_norm": 5.583776473999023,
      "learning_rate": 4.594017094017094e-05,
      "loss": 3.9935,
      "step": 47500
    },
    {
      "epoch": 410.2564102564103,
      "grad_norm": 6.784665584564209,
      "learning_rate": 4.5897435897435895e-05,
      "loss": 3.9866,
      "step": 48000
    },
    {
      "epoch": 414.52991452991455,
      "grad_norm": 6.990201473236084,
      "learning_rate": 4.585470085470085e-05,
      "loss": 3.9912,
      "step": 48500
    },
    {
      "epoch": 418.8034188034188,
      "grad_norm": 10.827975273132324,
      "learning_rate": 4.581196581196581e-05,
      "loss": 3.9956,
      "step": 49000
    },
    {
      "epoch": 423.0769230769231,
      "grad_norm": 6.200128078460693,
      "learning_rate": 4.576923076923077e-05,
      "loss": 4.0058,
      "step": 49500
    },
    {
      "epoch": 427.35042735042737,
      "grad_norm": 4.725303649902344,
      "learning_rate": 4.572649572649573e-05,
      "loss": 3.9786,
      "step": 50000
    },
    {
      "epoch": 431.62393162393164,
      "grad_norm": 8.688067436218262,
      "learning_rate": 4.568376068376069e-05,
      "loss": 3.9886,
      "step": 50500
    },
    {
      "epoch": 435.8974358974359,
      "grad_norm": 7.352275848388672,
      "learning_rate": 4.5641025641025645e-05,
      "loss": 3.9889,
      "step": 51000
    },
    {
      "epoch": 440.1709401709402,
      "grad_norm": 6.991694927215576,
      "learning_rate": 4.55982905982906e-05,
      "loss": 3.9875,
      "step": 51500
    },
    {
      "epoch": 444.44444444444446,
      "grad_norm": 8.359098434448242,
      "learning_rate": 4.555555555555556e-05,
      "loss": 3.978,
      "step": 52000
    },
    {
      "epoch": 448.71794871794873,
      "grad_norm": 8.430614471435547,
      "learning_rate": 4.5512820512820516e-05,
      "loss": 3.9929,
      "step": 52500
    },
    {
      "epoch": 452.991452991453,
      "grad_norm": 6.726223945617676,
      "learning_rate": 4.5470085470085474e-05,
      "loss": 3.9868,
      "step": 53000
    },
    {
      "epoch": 457.2649572649573,
      "grad_norm": 7.383115291595459,
      "learning_rate": 4.542735042735043e-05,
      "loss": 3.9775,
      "step": 53500
    },
    {
      "epoch": 461.53846153846155,
      "grad_norm": 18.195093154907227,
      "learning_rate": 4.538461538461539e-05,
      "loss": 3.9925,
      "step": 54000
    },
    {
      "epoch": 465.8119658119658,
      "grad_norm": 6.556818962097168,
      "learning_rate": 4.5341880341880345e-05,
      "loss": 3.9789,
      "step": 54500
    },
    {
      "epoch": 470.0854700854701,
      "grad_norm": 9.531962394714355,
      "learning_rate": 4.52991452991453e-05,
      "loss": 3.9807,
      "step": 55000
    },
    {
      "epoch": 474.35897435897436,
      "grad_norm": 8.319652557373047,
      "learning_rate": 4.525641025641026e-05,
      "loss": 3.9866,
      "step": 55500
    },
    {
      "epoch": 478.63247863247864,
      "grad_norm": 8.463211059570312,
      "learning_rate": 4.521367521367522e-05,
      "loss": 3.984,
      "step": 56000
    },
    {
      "epoch": 482.9059829059829,
      "grad_norm": 11.724905014038086,
      "learning_rate": 4.5170940170940174e-05,
      "loss": 3.9803,
      "step": 56500
    },
    {
      "epoch": 487.1794871794872,
      "grad_norm": 7.868284702301025,
      "learning_rate": 4.512820512820513e-05,
      "loss": 3.9747,
      "step": 57000
    },
    {
      "epoch": 491.45299145299145,
      "grad_norm": 9.71316909790039,
      "learning_rate": 4.508547008547009e-05,
      "loss": 3.9719,
      "step": 57500
    },
    {
      "epoch": 495.7264957264957,
      "grad_norm": 6.478739261627197,
      "learning_rate": 4.5042735042735046e-05,
      "loss": 3.9667,
      "step": 58000
    },
    {
      "epoch": 500.0,
      "grad_norm": 9.252631187438965,
      "learning_rate": 4.5e-05,
      "loss": 3.9815,
      "step": 58500
    },
    {
      "epoch": 504.2735042735043,
      "grad_norm": 8.657444953918457,
      "learning_rate": 4.495726495726496e-05,
      "loss": 3.9662,
      "step": 59000
    },
    {
      "epoch": 508.54700854700855,
      "grad_norm": 11.178494453430176,
      "learning_rate": 4.491452991452992e-05,
      "loss": 3.9685,
      "step": 59500
    },
    {
      "epoch": 512.8205128205128,
      "grad_norm": 6.415782928466797,
      "learning_rate": 4.4871794871794874e-05,
      "loss": 3.9812,
      "step": 60000
    },
    {
      "epoch": 517.0940170940171,
      "grad_norm": 6.879825592041016,
      "learning_rate": 4.482905982905983e-05,
      "loss": 3.9631,
      "step": 60500
    },
    {
      "epoch": 521.3675213675214,
      "grad_norm": 9.492097854614258,
      "learning_rate": 4.478632478632479e-05,
      "loss": 3.9694,
      "step": 61000
    },
    {
      "epoch": 525.6410256410256,
      "grad_norm": 8.633626937866211,
      "learning_rate": 4.4743589743589746e-05,
      "loss": 3.9754,
      "step": 61500
    },
    {
      "epoch": 529.9145299145299,
      "grad_norm": 7.7194719314575195,
      "learning_rate": 4.47008547008547e-05,
      "loss": 3.9779,
      "step": 62000
    },
    {
      "epoch": 534.1880341880342,
      "grad_norm": 7.292169570922852,
      "learning_rate": 4.465811965811966e-05,
      "loss": 3.9675,
      "step": 62500
    },
    {
      "epoch": 538.4615384615385,
      "grad_norm": 7.294814586639404,
      "learning_rate": 4.461538461538462e-05,
      "loss": 3.9811,
      "step": 63000
    },
    {
      "epoch": 542.7350427350427,
      "grad_norm": 12.58330249786377,
      "learning_rate": 4.4572649572649575e-05,
      "loss": 3.968,
      "step": 63500
    },
    {
      "epoch": 547.008547008547,
      "grad_norm": 7.1367926597595215,
      "learning_rate": 4.452991452991453e-05,
      "loss": 3.9733,
      "step": 64000
    },
    {
      "epoch": 551.2820512820513,
      "grad_norm": 8.501925468444824,
      "learning_rate": 4.448717948717949e-05,
      "loss": 3.9738,
      "step": 64500
    },
    {
      "epoch": 555.5555555555555,
      "grad_norm": 9.262921333312988,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 3.9545,
      "step": 65000
    },
    {
      "epoch": 559.8290598290598,
      "grad_norm": 8.666219711303711,
      "learning_rate": 4.4401709401709404e-05,
      "loss": 3.9604,
      "step": 65500
    },
    {
      "epoch": 564.1025641025641,
      "grad_norm": 9.485916137695312,
      "learning_rate": 4.435897435897436e-05,
      "loss": 3.9651,
      "step": 66000
    },
    {
      "epoch": 568.3760683760684,
      "grad_norm": 8.93874740600586,
      "learning_rate": 4.431623931623932e-05,
      "loss": 3.9588,
      "step": 66500
    },
    {
      "epoch": 572.6495726495726,
      "grad_norm": 7.005195140838623,
      "learning_rate": 4.4273504273504275e-05,
      "loss": 3.963,
      "step": 67000
    },
    {
      "epoch": 576.9230769230769,
      "grad_norm": 6.633715629577637,
      "learning_rate": 4.423076923076923e-05,
      "loss": 3.954,
      "step": 67500
    },
    {
      "epoch": 581.1965811965812,
      "grad_norm": 8.847023963928223,
      "learning_rate": 4.418803418803419e-05,
      "loss": 3.9619,
      "step": 68000
    },
    {
      "epoch": 585.4700854700855,
      "grad_norm": 7.92300271987915,
      "learning_rate": 4.414529914529915e-05,
      "loss": 3.966,
      "step": 68500
    },
    {
      "epoch": 589.7435897435897,
      "grad_norm": 6.521296501159668,
      "learning_rate": 4.4102564102564104e-05,
      "loss": 3.9593,
      "step": 69000
    },
    {
      "epoch": 594.017094017094,
      "grad_norm": 5.531820774078369,
      "learning_rate": 4.405982905982906e-05,
      "loss": 3.9644,
      "step": 69500
    },
    {
      "epoch": 598.2905982905983,
      "grad_norm": 5.972663402557373,
      "learning_rate": 4.401709401709402e-05,
      "loss": 3.9542,
      "step": 70000
    },
    {
      "epoch": 602.5641025641025,
      "grad_norm": 7.20762825012207,
      "learning_rate": 4.3974358974358976e-05,
      "loss": 3.9625,
      "step": 70500
    },
    {
      "epoch": 606.8376068376068,
      "grad_norm": 11.428756713867188,
      "learning_rate": 4.393162393162393e-05,
      "loss": 3.9613,
      "step": 71000
    },
    {
      "epoch": 611.1111111111111,
      "grad_norm": 8.51745319366455,
      "learning_rate": 4.388888888888889e-05,
      "loss": 3.9488,
      "step": 71500
    },
    {
      "epoch": 615.3846153846154,
      "grad_norm": 11.08763599395752,
      "learning_rate": 4.384615384615385e-05,
      "loss": 3.9551,
      "step": 72000
    },
    {
      "epoch": 619.6581196581196,
      "grad_norm": 6.370917797088623,
      "learning_rate": 4.3803418803418805e-05,
      "loss": 3.9657,
      "step": 72500
    },
    {
      "epoch": 623.9316239316239,
      "grad_norm": 10.158863067626953,
      "learning_rate": 4.376068376068376e-05,
      "loss": 3.9511,
      "step": 73000
    },
    {
      "epoch": 628.2051282051282,
      "grad_norm": 7.9173173904418945,
      "learning_rate": 4.371794871794872e-05,
      "loss": 3.9619,
      "step": 73500
    },
    {
      "epoch": 632.4786324786324,
      "grad_norm": 5.766732215881348,
      "learning_rate": 4.3675213675213676e-05,
      "loss": 3.9547,
      "step": 74000
    },
    {
      "epoch": 636.7521367521367,
      "grad_norm": 7.563880920410156,
      "learning_rate": 4.3632478632478634e-05,
      "loss": 3.9538,
      "step": 74500
    },
    {
      "epoch": 641.025641025641,
      "grad_norm": 10.54238224029541,
      "learning_rate": 4.358974358974359e-05,
      "loss": 3.9658,
      "step": 75000
    },
    {
      "epoch": 645.2991452991453,
      "grad_norm": 6.045254707336426,
      "learning_rate": 4.354700854700855e-05,
      "loss": 3.9482,
      "step": 75500
    },
    {
      "epoch": 649.5726495726495,
      "grad_norm": 6.508040904998779,
      "learning_rate": 4.3504273504273505e-05,
      "loss": 3.9636,
      "step": 76000
    },
    {
      "epoch": 653.8461538461538,
      "grad_norm": 8.1851167678833,
      "learning_rate": 4.346153846153846e-05,
      "loss": 3.952,
      "step": 76500
    },
    {
      "epoch": 658.1196581196581,
      "grad_norm": 8.655557632446289,
      "learning_rate": 4.341880341880342e-05,
      "loss": 3.954,
      "step": 77000
    },
    {
      "epoch": 662.3931623931624,
      "grad_norm": 5.89277458190918,
      "learning_rate": 4.337606837606838e-05,
      "loss": 3.9469,
      "step": 77500
    },
    {
      "epoch": 666.6666666666666,
      "grad_norm": 7.219918251037598,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 3.948,
      "step": 78000
    },
    {
      "epoch": 670.9401709401709,
      "grad_norm": 8.60533332824707,
      "learning_rate": 4.329059829059829e-05,
      "loss": 3.9442,
      "step": 78500
    },
    {
      "epoch": 675.2136752136752,
      "grad_norm": 6.112922191619873,
      "learning_rate": 4.324786324786325e-05,
      "loss": 3.9525,
      "step": 79000
    },
    {
      "epoch": 679.4871794871794,
      "grad_norm": 5.988474369049072,
      "learning_rate": 4.320512820512821e-05,
      "loss": 3.9515,
      "step": 79500
    },
    {
      "epoch": 683.7606837606837,
      "grad_norm": 10.35961627960205,
      "learning_rate": 4.316239316239317e-05,
      "loss": 3.945,
      "step": 80000
    },
    {
      "epoch": 688.034188034188,
      "grad_norm": 11.48615550994873,
      "learning_rate": 4.311965811965813e-05,
      "loss": 3.9461,
      "step": 80500
    },
    {
      "epoch": 692.3076923076923,
      "grad_norm": 6.490581512451172,
      "learning_rate": 4.3076923076923084e-05,
      "loss": 3.9405,
      "step": 81000
    },
    {
      "epoch": 696.5811965811965,
      "grad_norm": 8.728429794311523,
      "learning_rate": 4.303418803418804e-05,
      "loss": 3.9497,
      "step": 81500
    },
    {
      "epoch": 700.8547008547008,
      "grad_norm": 10.080997467041016,
      "learning_rate": 4.2991452991453e-05,
      "loss": 3.9505,
      "step": 82000
    },
    {
      "epoch": 705.1282051282051,
      "grad_norm": 6.6495890617370605,
      "learning_rate": 4.294871794871795e-05,
      "loss": 3.9414,
      "step": 82500
    },
    {
      "epoch": 709.4017094017094,
      "grad_norm": 7.373632431030273,
      "learning_rate": 4.2905982905982906e-05,
      "loss": 3.9425,
      "step": 83000
    },
    {
      "epoch": 713.6752136752136,
      "grad_norm": 6.070995330810547,
      "learning_rate": 4.286324786324786e-05,
      "loss": 3.9551,
      "step": 83500
    },
    {
      "epoch": 717.9487179487179,
      "grad_norm": 7.597427845001221,
      "learning_rate": 4.282051282051282e-05,
      "loss": 3.9492,
      "step": 84000
    },
    {
      "epoch": 722.2222222222222,
      "grad_norm": 9.22433090209961,
      "learning_rate": 4.277777777777778e-05,
      "loss": 3.9357,
      "step": 84500
    },
    {
      "epoch": 726.4957264957264,
      "grad_norm": 7.106647968292236,
      "learning_rate": 4.2735042735042735e-05,
      "loss": 3.9463,
      "step": 85000
    },
    {
      "epoch": 730.7692307692307,
      "grad_norm": 7.7301836013793945,
      "learning_rate": 4.269230769230769e-05,
      "loss": 3.9398,
      "step": 85500
    },
    {
      "epoch": 735.042735042735,
      "grad_norm": 9.358436584472656,
      "learning_rate": 4.264957264957265e-05,
      "loss": 3.9362,
      "step": 86000
    },
    {
      "epoch": 739.3162393162393,
      "grad_norm": 8.860991477966309,
      "learning_rate": 4.260683760683761e-05,
      "loss": 3.9381,
      "step": 86500
    },
    {
      "epoch": 743.5897435897435,
      "grad_norm": 7.590705394744873,
      "learning_rate": 4.2564102564102564e-05,
      "loss": 3.9449,
      "step": 87000
    },
    {
      "epoch": 747.8632478632478,
      "grad_norm": 15.584912300109863,
      "learning_rate": 4.252136752136752e-05,
      "loss": 3.9394,
      "step": 87500
    },
    {
      "epoch": 752.1367521367522,
      "grad_norm": 7.095953941345215,
      "learning_rate": 4.247863247863248e-05,
      "loss": 3.9406,
      "step": 88000
    },
    {
      "epoch": 756.4102564102565,
      "grad_norm": 10.528404235839844,
      "learning_rate": 4.2435897435897435e-05,
      "loss": 3.9372,
      "step": 88500
    },
    {
      "epoch": 760.6837606837607,
      "grad_norm": 7.935851573944092,
      "learning_rate": 4.239316239316239e-05,
      "loss": 3.9456,
      "step": 89000
    },
    {
      "epoch": 764.957264957265,
      "grad_norm": 8.081001281738281,
      "learning_rate": 4.235042735042735e-05,
      "loss": 3.937,
      "step": 89500
    },
    {
      "epoch": 769.2307692307693,
      "grad_norm": 5.476089000701904,
      "learning_rate": 4.230769230769231e-05,
      "loss": 3.9425,
      "step": 90000
    },
    {
      "epoch": 773.5042735042736,
      "grad_norm": 6.143497943878174,
      "learning_rate": 4.2264957264957264e-05,
      "loss": 3.9487,
      "step": 90500
    },
    {
      "epoch": 777.7777777777778,
      "grad_norm": 9.346363067626953,
      "learning_rate": 4.222222222222222e-05,
      "loss": 3.9459,
      "step": 91000
    },
    {
      "epoch": 782.0512820512821,
      "grad_norm": 9.733896255493164,
      "learning_rate": 4.217948717948718e-05,
      "loss": 3.9378,
      "step": 91500
    },
    {
      "epoch": 786.3247863247864,
      "grad_norm": 5.446666240692139,
      "learning_rate": 4.2136752136752136e-05,
      "loss": 3.9457,
      "step": 92000
    },
    {
      "epoch": 790.5982905982906,
      "grad_norm": 5.807028293609619,
      "learning_rate": 4.209401709401709e-05,
      "loss": 3.9275,
      "step": 92500
    },
    {
      "epoch": 794.8717948717949,
      "grad_norm": 7.36832857131958,
      "learning_rate": 4.205128205128206e-05,
      "loss": 3.9303,
      "step": 93000
    },
    {
      "epoch": 799.1452991452992,
      "grad_norm": 17.937969207763672,
      "learning_rate": 4.2008547008547014e-05,
      "loss": 3.9299,
      "step": 93500
    },
    {
      "epoch": 803.4188034188035,
      "grad_norm": 9.982585906982422,
      "learning_rate": 4.196581196581197e-05,
      "loss": 3.9394,
      "step": 94000
    },
    {
      "epoch": 807.6923076923077,
      "grad_norm": 8.930482864379883,
      "learning_rate": 4.192307692307693e-05,
      "loss": 3.9449,
      "step": 94500
    },
    {
      "epoch": 811.965811965812,
      "grad_norm": 9.467873573303223,
      "learning_rate": 4.1880341880341886e-05,
      "loss": 3.9318,
      "step": 95000
    },
    {
      "epoch": 816.2393162393163,
      "grad_norm": 6.778914451599121,
      "learning_rate": 4.183760683760684e-05,
      "loss": 3.9374,
      "step": 95500
    },
    {
      "epoch": 820.5128205128206,
      "grad_norm": 14.1344575881958,
      "learning_rate": 4.17948717948718e-05,
      "loss": 3.94,
      "step": 96000
    },
    {
      "epoch": 824.7863247863248,
      "grad_norm": 6.6074748039245605,
      "learning_rate": 4.175213675213676e-05,
      "loss": 3.9337,
      "step": 96500
    },
    {
      "epoch": 829.0598290598291,
      "grad_norm": 9.78016471862793,
      "learning_rate": 4.1709401709401715e-05,
      "loss": 3.942,
      "step": 97000
    },
    {
      "epoch": 833.3333333333334,
      "grad_norm": 9.650123596191406,
      "learning_rate": 4.166666666666667e-05,
      "loss": 3.9254,
      "step": 97500
    },
    {
      "epoch": 837.6068376068376,
      "grad_norm": 10.543852806091309,
      "learning_rate": 4.162393162393163e-05,
      "loss": 3.9298,
      "step": 98000
    },
    {
      "epoch": 841.8803418803419,
      "grad_norm": 5.64805269241333,
      "learning_rate": 4.1581196581196586e-05,
      "loss": 3.9352,
      "step": 98500
    },
    {
      "epoch": 846.1538461538462,
      "grad_norm": 6.631591796875,
      "learning_rate": 4.1538461538461544e-05,
      "loss": 3.9362,
      "step": 99000
    },
    {
      "epoch": 850.4273504273505,
      "grad_norm": 8.008435249328613,
      "learning_rate": 4.14957264957265e-05,
      "loss": 3.9416,
      "step": 99500
    },
    {
      "epoch": 854.7008547008547,
      "grad_norm": 6.436114311218262,
      "learning_rate": 4.145299145299146e-05,
      "loss": 3.9275,
      "step": 100000
    },
    {
      "epoch": 858.974358974359,
      "grad_norm": 8.495221138000488,
      "learning_rate": 4.1410256410256415e-05,
      "loss": 3.9391,
      "step": 100500
    },
    {
      "epoch": 863.2478632478633,
      "grad_norm": 6.968729019165039,
      "learning_rate": 4.1367521367521366e-05,
      "loss": 3.9393,
      "step": 101000
    },
    {
      "epoch": 867.5213675213676,
      "grad_norm": 6.70360803604126,
      "learning_rate": 4.132478632478632e-05,
      "loss": 3.9314,
      "step": 101500
    },
    {
      "epoch": 871.7948717948718,
      "grad_norm": 9.991844177246094,
      "learning_rate": 4.128205128205128e-05,
      "loss": 3.9144,
      "step": 102000
    },
    {
      "epoch": 876.0683760683761,
      "grad_norm": 9.202394485473633,
      "learning_rate": 4.123931623931624e-05,
      "loss": 3.9348,
      "step": 102500
    },
    {
      "epoch": 880.3418803418804,
      "grad_norm": 9.7063570022583,
      "learning_rate": 4.1196581196581195e-05,
      "loss": 3.9393,
      "step": 103000
    },
    {
      "epoch": 884.6153846153846,
      "grad_norm": 20.691463470458984,
      "learning_rate": 4.115384615384615e-05,
      "loss": 3.9284,
      "step": 103500
    },
    {
      "epoch": 888.8888888888889,
      "grad_norm": 7.045592784881592,
      "learning_rate": 4.111111111111111e-05,
      "loss": 3.9315,
      "step": 104000
    },
    {
      "epoch": 893.1623931623932,
      "grad_norm": 9.13902759552002,
      "learning_rate": 4.1068376068376066e-05,
      "loss": 3.9378,
      "step": 104500
    },
    {
      "epoch": 897.4358974358975,
      "grad_norm": 10.567039489746094,
      "learning_rate": 4.1025641025641023e-05,
      "loss": 3.933,
      "step": 105000
    },
    {
      "epoch": 901.7094017094017,
      "grad_norm": 8.040574073791504,
      "learning_rate": 4.098290598290598e-05,
      "loss": 3.9329,
      "step": 105500
    },
    {
      "epoch": 905.982905982906,
      "grad_norm": 9.906173706054688,
      "learning_rate": 4.094017094017094e-05,
      "loss": 3.9327,
      "step": 106000
    },
    {
      "epoch": 910.2564102564103,
      "grad_norm": 10.4525146484375,
      "learning_rate": 4.0897435897435895e-05,
      "loss": 3.9379,
      "step": 106500
    },
    {
      "epoch": 914.5299145299145,
      "grad_norm": 7.194275856018066,
      "learning_rate": 4.085470085470086e-05,
      "loss": 3.926,
      "step": 107000
    },
    {
      "epoch": 918.8034188034188,
      "grad_norm": 6.45619535446167,
      "learning_rate": 4.0811965811965816e-05,
      "loss": 3.9253,
      "step": 107500
    },
    {
      "epoch": 923.0769230769231,
      "grad_norm": 10.631048202514648,
      "learning_rate": 4.0769230769230773e-05,
      "loss": 3.9379,
      "step": 108000
    },
    {
      "epoch": 927.3504273504274,
      "grad_norm": 6.718042373657227,
      "learning_rate": 4.072649572649573e-05,
      "loss": 3.925,
      "step": 108500
    },
    {
      "epoch": 931.6239316239316,
      "grad_norm": 8.417428016662598,
      "learning_rate": 4.068376068376069e-05,
      "loss": 3.9305,
      "step": 109000
    },
    {
      "epoch": 935.8974358974359,
      "grad_norm": 5.9878411293029785,
      "learning_rate": 4.0641025641025645e-05,
      "loss": 3.9342,
      "step": 109500
    },
    {
      "epoch": 940.1709401709402,
      "grad_norm": 7.957812786102295,
      "learning_rate": 4.05982905982906e-05,
      "loss": 3.9112,
      "step": 110000
    },
    {
      "epoch": 944.4444444444445,
      "grad_norm": 8.152853965759277,
      "learning_rate": 4.055555555555556e-05,
      "loss": 3.9275,
      "step": 110500
    },
    {
      "epoch": 948.7179487179487,
      "grad_norm": 6.989166736602783,
      "learning_rate": 4.051282051282052e-05,
      "loss": 3.928,
      "step": 111000
    },
    {
      "epoch": 952.991452991453,
      "grad_norm": 7.953254222869873,
      "learning_rate": 4.0470085470085474e-05,
      "loss": 3.9237,
      "step": 111500
    },
    {
      "epoch": 957.2649572649573,
      "grad_norm": 8.593807220458984,
      "learning_rate": 4.042735042735043e-05,
      "loss": 3.9194,
      "step": 112000
    },
    {
      "epoch": 961.5384615384615,
      "grad_norm": 7.274503707885742,
      "learning_rate": 4.038461538461539e-05,
      "loss": 3.9266,
      "step": 112500
    },
    {
      "epoch": 965.8119658119658,
      "grad_norm": 8.488576889038086,
      "learning_rate": 4.0341880341880346e-05,
      "loss": 3.9132,
      "step": 113000
    },
    {
      "epoch": 970.0854700854701,
      "grad_norm": 7.243028163909912,
      "learning_rate": 4.02991452991453e-05,
      "loss": 3.9279,
      "step": 113500
    },
    {
      "epoch": 974.3589743589744,
      "grad_norm": 6.3631391525268555,
      "learning_rate": 4.025641025641026e-05,
      "loss": 3.9279,
      "step": 114000
    },
    {
      "epoch": 978.6324786324786,
      "grad_norm": 7.6876397132873535,
      "learning_rate": 4.021367521367522e-05,
      "loss": 3.9233,
      "step": 114500
    },
    {
      "epoch": 982.9059829059829,
      "grad_norm": 27.16080093383789,
      "learning_rate": 4.0170940170940174e-05,
      "loss": 3.9221,
      "step": 115000
    },
    {
      "epoch": 987.1794871794872,
      "grad_norm": 7.697185516357422,
      "learning_rate": 4.012820512820513e-05,
      "loss": 3.9254,
      "step": 115500
    },
    {
      "epoch": 991.4529914529915,
      "grad_norm": 13.04598331451416,
      "learning_rate": 4.008547008547009e-05,
      "loss": 3.9139,
      "step": 116000
    },
    {
      "epoch": 995.7264957264957,
      "grad_norm": 7.719341278076172,
      "learning_rate": 4.0042735042735046e-05,
      "loss": 3.9205,
      "step": 116500
    },
    {
      "epoch": 1000.0,
      "grad_norm": 15.483312606811523,
      "learning_rate": 4e-05,
      "loss": 3.9254,
      "step": 117000
    },
    {
      "epoch": 1004.2735042735043,
      "grad_norm": 7.559781074523926,
      "learning_rate": 3.995726495726496e-05,
      "loss": 3.9132,
      "step": 117500
    },
    {
      "epoch": 1008.5470085470085,
      "grad_norm": 6.561147689819336,
      "learning_rate": 3.991452991452992e-05,
      "loss": 3.9305,
      "step": 118000
    },
    {
      "epoch": 1012.8205128205128,
      "grad_norm": 7.042187213897705,
      "learning_rate": 3.9871794871794875e-05,
      "loss": 3.9192,
      "step": 118500
    },
    {
      "epoch": 1017.0940170940171,
      "grad_norm": 8.575319290161133,
      "learning_rate": 3.9829059829059825e-05,
      "loss": 3.9231,
      "step": 119000
    },
    {
      "epoch": 1021.3675213675214,
      "grad_norm": 6.687872886657715,
      "learning_rate": 3.978632478632478e-05,
      "loss": 3.9213,
      "step": 119500
    },
    {
      "epoch": 1025.6410256410256,
      "grad_norm": 9.51988410949707,
      "learning_rate": 3.974358974358974e-05,
      "loss": 3.9216,
      "step": 120000
    },
    {
      "epoch": 1029.91452991453,
      "grad_norm": 9.094080924987793,
      "learning_rate": 3.9700854700854704e-05,
      "loss": 3.9167,
      "step": 120500
    },
    {
      "epoch": 1034.1880341880342,
      "grad_norm": 8.88957691192627,
      "learning_rate": 3.965811965811966e-05,
      "loss": 3.9167,
      "step": 121000
    },
    {
      "epoch": 1038.4615384615386,
      "grad_norm": 7.253787040710449,
      "learning_rate": 3.961538461538462e-05,
      "loss": 3.9208,
      "step": 121500
    },
    {
      "epoch": 1042.7350427350427,
      "grad_norm": 7.489483833312988,
      "learning_rate": 3.9572649572649575e-05,
      "loss": 3.9321,
      "step": 122000
    },
    {
      "epoch": 1047.008547008547,
      "grad_norm": 13.179643630981445,
      "learning_rate": 3.952991452991453e-05,
      "loss": 3.9312,
      "step": 122500
    },
    {
      "epoch": 1051.2820512820513,
      "grad_norm": 6.812854290008545,
      "learning_rate": 3.948717948717949e-05,
      "loss": 3.919,
      "step": 123000
    },
    {
      "epoch": 1055.5555555555557,
      "grad_norm": 8.000834465026855,
      "learning_rate": 3.944444444444445e-05,
      "loss": 3.926,
      "step": 123500
    },
    {
      "epoch": 1059.8290598290598,
      "grad_norm": 7.443535327911377,
      "learning_rate": 3.9401709401709404e-05,
      "loss": 3.9247,
      "step": 124000
    },
    {
      "epoch": 1064.1025641025642,
      "grad_norm": 7.319825649261475,
      "learning_rate": 3.935897435897436e-05,
      "loss": 3.9132,
      "step": 124500
    },
    {
      "epoch": 1068.3760683760684,
      "grad_norm": 6.919106483459473,
      "learning_rate": 3.931623931623932e-05,
      "loss": 3.9193,
      "step": 125000
    },
    {
      "epoch": 1072.6495726495727,
      "grad_norm": 10.498445510864258,
      "learning_rate": 3.9273504273504276e-05,
      "loss": 3.9238,
      "step": 125500
    },
    {
      "epoch": 1076.923076923077,
      "grad_norm": 7.051595687866211,
      "learning_rate": 3.923076923076923e-05,
      "loss": 3.9085,
      "step": 126000
    },
    {
      "epoch": 1081.1965811965813,
      "grad_norm": 7.6384758949279785,
      "learning_rate": 3.918803418803419e-05,
      "loss": 3.9347,
      "step": 126500
    },
    {
      "epoch": 1085.4700854700855,
      "grad_norm": 9.515829086303711,
      "learning_rate": 3.914529914529915e-05,
      "loss": 3.9183,
      "step": 127000
    },
    {
      "epoch": 1089.7435897435898,
      "grad_norm": 8.540751457214355,
      "learning_rate": 3.9102564102564105e-05,
      "loss": 3.916,
      "step": 127500
    },
    {
      "epoch": 1094.017094017094,
      "grad_norm": 5.476522922515869,
      "learning_rate": 3.905982905982906e-05,
      "loss": 3.9185,
      "step": 128000
    },
    {
      "epoch": 1098.2905982905984,
      "grad_norm": 11.473152160644531,
      "learning_rate": 3.901709401709402e-05,
      "loss": 3.9191,
      "step": 128500
    },
    {
      "epoch": 1102.5641025641025,
      "grad_norm": 7.747425556182861,
      "learning_rate": 3.8974358974358976e-05,
      "loss": 3.9163,
      "step": 129000
    },
    {
      "epoch": 1106.837606837607,
      "grad_norm": 9.037108421325684,
      "learning_rate": 3.8931623931623934e-05,
      "loss": 3.9159,
      "step": 129500
    },
    {
      "epoch": 1111.111111111111,
      "grad_norm": 6.139928340911865,
      "learning_rate": 3.888888888888889e-05,
      "loss": 3.91,
      "step": 130000
    },
    {
      "epoch": 1115.3846153846155,
      "grad_norm": 9.660848617553711,
      "learning_rate": 3.884615384615385e-05,
      "loss": 3.9169,
      "step": 130500
    },
    {
      "epoch": 1119.6581196581196,
      "grad_norm": 7.124261856079102,
      "learning_rate": 3.8803418803418805e-05,
      "loss": 3.9186,
      "step": 131000
    },
    {
      "epoch": 1123.931623931624,
      "grad_norm": 6.5632004737854,
      "learning_rate": 3.876068376068376e-05,
      "loss": 3.9131,
      "step": 131500
    },
    {
      "epoch": 1128.2051282051282,
      "grad_norm": 6.789401531219482,
      "learning_rate": 3.871794871794872e-05,
      "loss": 3.9166,
      "step": 132000
    },
    {
      "epoch": 1132.4786324786326,
      "grad_norm": 9.036699295043945,
      "learning_rate": 3.867521367521368e-05,
      "loss": 3.9076,
      "step": 132500
    },
    {
      "epoch": 1136.7521367521367,
      "grad_norm": 7.055581569671631,
      "learning_rate": 3.8632478632478634e-05,
      "loss": 3.9164,
      "step": 133000
    },
    {
      "epoch": 1141.025641025641,
      "grad_norm": 7.046407699584961,
      "learning_rate": 3.858974358974359e-05,
      "loss": 3.9207,
      "step": 133500
    },
    {
      "epoch": 1145.2991452991453,
      "grad_norm": 8.03333568572998,
      "learning_rate": 3.854700854700855e-05,
      "loss": 3.9097,
      "step": 134000
    },
    {
      "epoch": 1149.5726495726497,
      "grad_norm": 7.235924243927002,
      "learning_rate": 3.8504273504273506e-05,
      "loss": 3.9111,
      "step": 134500
    },
    {
      "epoch": 1153.8461538461538,
      "grad_norm": 8.40984058380127,
      "learning_rate": 3.846153846153846e-05,
      "loss": 3.9196,
      "step": 135000
    },
    {
      "epoch": 1158.1196581196582,
      "grad_norm": 11.362048149108887,
      "learning_rate": 3.841880341880342e-05,
      "loss": 3.9105,
      "step": 135500
    },
    {
      "epoch": 1162.3931623931624,
      "grad_norm": 10.719963073730469,
      "learning_rate": 3.837606837606838e-05,
      "loss": 3.9236,
      "step": 136000
    },
    {
      "epoch": 1166.6666666666667,
      "grad_norm": 10.375935554504395,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 3.9065,
      "step": 136500
    },
    {
      "epoch": 1170.940170940171,
      "grad_norm": 8.315825462341309,
      "learning_rate": 3.82905982905983e-05,
      "loss": 3.9105,
      "step": 137000
    },
    {
      "epoch": 1175.2136752136753,
      "grad_norm": 9.758587837219238,
      "learning_rate": 3.824786324786325e-05,
      "loss": 3.9024,
      "step": 137500
    },
    {
      "epoch": 1179.4871794871794,
      "grad_norm": 8.63891887664795,
      "learning_rate": 3.8205128205128206e-05,
      "loss": 3.9181,
      "step": 138000
    },
    {
      "epoch": 1183.7606837606838,
      "grad_norm": 10.706990242004395,
      "learning_rate": 3.816239316239316e-05,
      "loss": 3.9126,
      "step": 138500
    },
    {
      "epoch": 1188.034188034188,
      "grad_norm": 19.51291847229004,
      "learning_rate": 3.811965811965812e-05,
      "loss": 3.9208,
      "step": 139000
    },
    {
      "epoch": 1192.3076923076924,
      "grad_norm": 5.89124870300293,
      "learning_rate": 3.807692307692308e-05,
      "loss": 3.9075,
      "step": 139500
    },
    {
      "epoch": 1196.5811965811965,
      "grad_norm": 11.911284446716309,
      "learning_rate": 3.8034188034188035e-05,
      "loss": 3.9042,
      "step": 140000
    },
    {
      "epoch": 1200.854700854701,
      "grad_norm": 6.906155109405518,
      "learning_rate": 3.799145299145299e-05,
      "loss": 3.9244,
      "step": 140500
    },
    {
      "epoch": 1205.128205128205,
      "grad_norm": 8.183622360229492,
      "learning_rate": 3.794871794871795e-05,
      "loss": 3.9147,
      "step": 141000
    },
    {
      "epoch": 1209.4017094017095,
      "grad_norm": 6.507047653198242,
      "learning_rate": 3.7905982905982907e-05,
      "loss": 3.9109,
      "step": 141500
    },
    {
      "epoch": 1213.6752136752136,
      "grad_norm": 7.804844379425049,
      "learning_rate": 3.7863247863247864e-05,
      "loss": 3.9191,
      "step": 142000
    },
    {
      "epoch": 1217.948717948718,
      "grad_norm": 7.885542869567871,
      "learning_rate": 3.782051282051282e-05,
      "loss": 3.9223,
      "step": 142500
    },
    {
      "epoch": 1222.2222222222222,
      "grad_norm": 6.978323459625244,
      "learning_rate": 3.777777777777778e-05,
      "loss": 3.9068,
      "step": 143000
    },
    {
      "epoch": 1226.4957264957266,
      "grad_norm": 7.9006829261779785,
      "learning_rate": 3.7735042735042735e-05,
      "loss": 3.9146,
      "step": 143500
    },
    {
      "epoch": 1230.7692307692307,
      "grad_norm": 11.931824684143066,
      "learning_rate": 3.769230769230769e-05,
      "loss": 3.9121,
      "step": 144000
    },
    {
      "epoch": 1235.042735042735,
      "grad_norm": 6.041910171508789,
      "learning_rate": 3.764957264957265e-05,
      "loss": 3.9082,
      "step": 144500
    },
    {
      "epoch": 1239.3162393162393,
      "grad_norm": 8.737008094787598,
      "learning_rate": 3.760683760683761e-05,
      "loss": 3.9207,
      "step": 145000
    },
    {
      "epoch": 1243.5897435897436,
      "grad_norm": 5.823883533477783,
      "learning_rate": 3.7564102564102564e-05,
      "loss": 3.9053,
      "step": 145500
    },
    {
      "epoch": 1247.8632478632478,
      "grad_norm": 8.402179718017578,
      "learning_rate": 3.752136752136752e-05,
      "loss": 3.9085,
      "step": 146000
    },
    {
      "epoch": 1252.1367521367522,
      "grad_norm": 8.751445770263672,
      "learning_rate": 3.747863247863248e-05,
      "loss": 3.9145,
      "step": 146500
    },
    {
      "epoch": 1256.4102564102564,
      "grad_norm": 7.170765399932861,
      "learning_rate": 3.7435897435897436e-05,
      "loss": 3.912,
      "step": 147000
    },
    {
      "epoch": 1260.6837606837607,
      "grad_norm": 9.68430233001709,
      "learning_rate": 3.739316239316239e-05,
      "loss": 3.9139,
      "step": 147500
    },
    {
      "epoch": 1264.957264957265,
      "grad_norm": 12.393383979797363,
      "learning_rate": 3.735042735042735e-05,
      "loss": 3.9174,
      "step": 148000
    },
    {
      "epoch": 1269.2307692307693,
      "grad_norm": 6.346941947937012,
      "learning_rate": 3.730769230769231e-05,
      "loss": 3.9176,
      "step": 148500
    },
    {
      "epoch": 1273.5042735042734,
      "grad_norm": 10.106179237365723,
      "learning_rate": 3.7264957264957265e-05,
      "loss": 3.9086,
      "step": 149000
    },
    {
      "epoch": 1277.7777777777778,
      "grad_norm": 6.7984442710876465,
      "learning_rate": 3.722222222222222e-05,
      "loss": 3.9029,
      "step": 149500
    },
    {
      "epoch": 1282.051282051282,
      "grad_norm": 5.831560134887695,
      "learning_rate": 3.717948717948718e-05,
      "loss": 3.9063,
      "step": 150000
    },
    {
      "epoch": 1286.3247863247864,
      "grad_norm": 9.330318450927734,
      "learning_rate": 3.713675213675214e-05,
      "loss": 3.9139,
      "step": 150500
    },
    {
      "epoch": 1290.5982905982905,
      "grad_norm": 7.327159404754639,
      "learning_rate": 3.70940170940171e-05,
      "loss": 3.9136,
      "step": 151000
    },
    {
      "epoch": 1294.871794871795,
      "grad_norm": 7.202413558959961,
      "learning_rate": 3.705128205128206e-05,
      "loss": 3.9088,
      "step": 151500
    },
    {
      "epoch": 1299.145299145299,
      "grad_norm": 7.544702053070068,
      "learning_rate": 3.7008547008547015e-05,
      "loss": 3.9032,
      "step": 152000
    },
    {
      "epoch": 1303.4188034188035,
      "grad_norm": 8.222434997558594,
      "learning_rate": 3.696581196581197e-05,
      "loss": 3.9185,
      "step": 152500
    },
    {
      "epoch": 1307.6923076923076,
      "grad_norm": 10.827597618103027,
      "learning_rate": 3.692307692307693e-05,
      "loss": 3.9117,
      "step": 153000
    },
    {
      "epoch": 1311.965811965812,
      "grad_norm": 8.729974746704102,
      "learning_rate": 3.6880341880341886e-05,
      "loss": 3.9073,
      "step": 153500
    },
    {
      "epoch": 1316.2393162393162,
      "grad_norm": 8.256524085998535,
      "learning_rate": 3.6837606837606844e-05,
      "loss": 3.9114,
      "step": 154000
    },
    {
      "epoch": 1320.5128205128206,
      "grad_norm": 7.10039758682251,
      "learning_rate": 3.67948717948718e-05,
      "loss": 3.8955,
      "step": 154500
    },
    {
      "epoch": 1324.7863247863247,
      "grad_norm": 6.080343246459961,
      "learning_rate": 3.675213675213676e-05,
      "loss": 3.9156,
      "step": 155000
    },
    {
      "epoch": 1329.059829059829,
      "grad_norm": 8.386335372924805,
      "learning_rate": 3.670940170940171e-05,
      "loss": 3.902,
      "step": 155500
    },
    {
      "epoch": 1333.3333333333333,
      "grad_norm": 5.651959419250488,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 3.9104,
      "step": 156000
    },
    {
      "epoch": 1337.6068376068376,
      "grad_norm": 7.1226725578308105,
      "learning_rate": 3.662393162393162e-05,
      "loss": 3.9111,
      "step": 156500
    },
    {
      "epoch": 1341.8803418803418,
      "grad_norm": 8.558842658996582,
      "learning_rate": 3.658119658119658e-05,
      "loss": 3.9106,
      "step": 157000
    },
    {
      "epoch": 1346.1538461538462,
      "grad_norm": 8.416111946105957,
      "learning_rate": 3.653846153846154e-05,
      "loss": 3.9099,
      "step": 157500
    },
    {
      "epoch": 1350.4273504273503,
      "grad_norm": 9.80384349822998,
      "learning_rate": 3.6495726495726495e-05,
      "loss": 3.9004,
      "step": 158000
    },
    {
      "epoch": 1354.7008547008547,
      "grad_norm": 7.295526504516602,
      "learning_rate": 3.645299145299145e-05,
      "loss": 3.9144,
      "step": 158500
    },
    {
      "epoch": 1358.974358974359,
      "grad_norm": 6.454660892486572,
      "learning_rate": 3.641025641025641e-05,
      "loss": 3.9098,
      "step": 159000
    },
    {
      "epoch": 1363.2478632478633,
      "grad_norm": 7.367055892944336,
      "learning_rate": 3.6367521367521366e-05,
      "loss": 3.9014,
      "step": 159500
    },
    {
      "epoch": 1367.5213675213674,
      "grad_norm": 10.8394193649292,
      "learning_rate": 3.6324786324786323e-05,
      "loss": 3.9081,
      "step": 160000
    },
    {
      "epoch": 1371.7948717948718,
      "grad_norm": 6.266265392303467,
      "learning_rate": 3.628205128205128e-05,
      "loss": 3.9208,
      "step": 160500
    },
    {
      "epoch": 1376.068376068376,
      "grad_norm": 6.574036121368408,
      "learning_rate": 3.623931623931624e-05,
      "loss": 3.8997,
      "step": 161000
    },
    {
      "epoch": 1380.3418803418804,
      "grad_norm": 9.245071411132812,
      "learning_rate": 3.6196581196581195e-05,
      "loss": 3.8996,
      "step": 161500
    },
    {
      "epoch": 1384.6153846153845,
      "grad_norm": 7.436936378479004,
      "learning_rate": 3.615384615384615e-05,
      "loss": 3.907,
      "step": 162000
    },
    {
      "epoch": 1388.888888888889,
      "grad_norm": 10.257660865783691,
      "learning_rate": 3.611111111111111e-05,
      "loss": 3.9095,
      "step": 162500
    },
    {
      "epoch": 1393.162393162393,
      "grad_norm": 7.077186584472656,
      "learning_rate": 3.606837606837607e-05,
      "loss": 3.9111,
      "step": 163000
    },
    {
      "epoch": 1397.4358974358975,
      "grad_norm": 8.365791320800781,
      "learning_rate": 3.6025641025641024e-05,
      "loss": 3.9033,
      "step": 163500
    },
    {
      "epoch": 1401.7094017094016,
      "grad_norm": 9.699987411499023,
      "learning_rate": 3.598290598290598e-05,
      "loss": 3.8961,
      "step": 164000
    },
    {
      "epoch": 1405.982905982906,
      "grad_norm": 11.776172637939453,
      "learning_rate": 3.5940170940170945e-05,
      "loss": 3.9103,
      "step": 164500
    },
    {
      "epoch": 1410.2564102564102,
      "grad_norm": 9.463261604309082,
      "learning_rate": 3.58974358974359e-05,
      "loss": 3.909,
      "step": 165000
    },
    {
      "epoch": 1414.5299145299145,
      "grad_norm": 8.78907299041748,
      "learning_rate": 3.585470085470086e-05,
      "loss": 3.9006,
      "step": 165500
    },
    {
      "epoch": 1418.8034188034187,
      "grad_norm": 10.89334487915039,
      "learning_rate": 3.581196581196582e-05,
      "loss": 3.9048,
      "step": 166000
    },
    {
      "epoch": 1423.076923076923,
      "grad_norm": 9.227560043334961,
      "learning_rate": 3.5769230769230774e-05,
      "loss": 3.907,
      "step": 166500
    },
    {
      "epoch": 1427.3504273504273,
      "grad_norm": 8.330955505371094,
      "learning_rate": 3.572649572649573e-05,
      "loss": 3.9095,
      "step": 167000
    },
    {
      "epoch": 1431.6239316239316,
      "grad_norm": 6.694507598876953,
      "learning_rate": 3.568376068376069e-05,
      "loss": 3.9002,
      "step": 167500
    },
    {
      "epoch": 1435.8974358974358,
      "grad_norm": 7.533235549926758,
      "learning_rate": 3.5641025641025646e-05,
      "loss": 3.9121,
      "step": 168000
    },
    {
      "epoch": 1440.1709401709402,
      "grad_norm": 11.065003395080566,
      "learning_rate": 3.55982905982906e-05,
      "loss": 3.8997,
      "step": 168500
    },
    {
      "epoch": 1444.4444444444443,
      "grad_norm": 7.755876541137695,
      "learning_rate": 3.555555555555556e-05,
      "loss": 3.9027,
      "step": 169000
    },
    {
      "epoch": 1448.7179487179487,
      "grad_norm": 6.740322589874268,
      "learning_rate": 3.551282051282052e-05,
      "loss": 3.8932,
      "step": 169500
    },
    {
      "epoch": 1452.991452991453,
      "grad_norm": 11.312577247619629,
      "learning_rate": 3.5470085470085474e-05,
      "loss": 3.9104,
      "step": 170000
    },
    {
      "epoch": 1457.2649572649573,
      "grad_norm": 10.411213874816895,
      "learning_rate": 3.542735042735043e-05,
      "loss": 3.9085,
      "step": 170500
    },
    {
      "epoch": 1461.5384615384614,
      "grad_norm": 8.371713638305664,
      "learning_rate": 3.538461538461539e-05,
      "loss": 3.9078,
      "step": 171000
    },
    {
      "epoch": 1465.8119658119658,
      "grad_norm": 8.334156036376953,
      "learning_rate": 3.5341880341880346e-05,
      "loss": 3.8952,
      "step": 171500
    },
    {
      "epoch": 1470.08547008547,
      "grad_norm": 7.2238359451293945,
      "learning_rate": 3.52991452991453e-05,
      "loss": 3.9138,
      "step": 172000
    },
    {
      "epoch": 1474.3589743589744,
      "grad_norm": 9.76271915435791,
      "learning_rate": 3.525641025641026e-05,
      "loss": 3.8978,
      "step": 172500
    },
    {
      "epoch": 1478.6324786324785,
      "grad_norm": 7.651259422302246,
      "learning_rate": 3.521367521367522e-05,
      "loss": 3.9132,
      "step": 173000
    },
    {
      "epoch": 1482.905982905983,
      "grad_norm": 8.073348045349121,
      "learning_rate": 3.5170940170940175e-05,
      "loss": 3.9075,
      "step": 173500
    },
    {
      "epoch": 1487.179487179487,
      "grad_norm": 8.07939624786377,
      "learning_rate": 3.5128205128205125e-05,
      "loss": 3.9102,
      "step": 174000
    },
    {
      "epoch": 1491.4529914529915,
      "grad_norm": 7.623297691345215,
      "learning_rate": 3.508547008547008e-05,
      "loss": 3.896,
      "step": 174500
    },
    {
      "epoch": 1495.7264957264956,
      "grad_norm": 7.855970859527588,
      "learning_rate": 3.504273504273504e-05,
      "loss": 3.8995,
      "step": 175000
    },
    {
      "epoch": 1500.0,
      "grad_norm": 6.742058753967285,
      "learning_rate": 3.5e-05,
      "loss": 3.8959,
      "step": 175500
    },
    {
      "epoch": 1504.2735042735044,
      "grad_norm": 8.204747200012207,
      "learning_rate": 3.4957264957264954e-05,
      "loss": 3.9072,
      "step": 176000
    },
    {
      "epoch": 1508.5470085470085,
      "grad_norm": 7.896560192108154,
      "learning_rate": 3.491452991452991e-05,
      "loss": 3.8969,
      "step": 176500
    },
    {
      "epoch": 1512.820512820513,
      "grad_norm": 11.011876106262207,
      "learning_rate": 3.487179487179487e-05,
      "loss": 3.8969,
      "step": 177000
    },
    {
      "epoch": 1517.094017094017,
      "grad_norm": 7.757447719573975,
      "learning_rate": 3.4829059829059826e-05,
      "loss": 3.8969,
      "step": 177500
    },
    {
      "epoch": 1521.3675213675215,
      "grad_norm": 6.052780628204346,
      "learning_rate": 3.478632478632479e-05,
      "loss": 3.9145,
      "step": 178000
    },
    {
      "epoch": 1525.6410256410256,
      "grad_norm": 13.569592475891113,
      "learning_rate": 3.474358974358975e-05,
      "loss": 3.8917,
      "step": 178500
    },
    {
      "epoch": 1529.91452991453,
      "grad_norm": 5.183642864227295,
      "learning_rate": 3.4700854700854704e-05,
      "loss": 3.9008,
      "step": 179000
    },
    {
      "epoch": 1534.1880341880342,
      "grad_norm": 8.907872200012207,
      "learning_rate": 3.465811965811966e-05,
      "loss": 3.9005,
      "step": 179500
    },
    {
      "epoch": 1538.4615384615386,
      "grad_norm": 9.121519088745117,
      "learning_rate": 3.461538461538462e-05,
      "loss": 3.9008,
      "step": 180000
    },
    {
      "epoch": 1542.7350427350427,
      "grad_norm": 6.866839408874512,
      "learning_rate": 3.4572649572649576e-05,
      "loss": 3.898,
      "step": 180500
    },
    {
      "epoch": 1547.008547008547,
      "grad_norm": 10.349055290222168,
      "learning_rate": 3.452991452991453e-05,
      "loss": 3.9052,
      "step": 181000
    },
    {
      "epoch": 1551.2820512820513,
      "grad_norm": 11.634439468383789,
      "learning_rate": 3.448717948717949e-05,
      "loss": 3.8967,
      "step": 181500
    },
    {
      "epoch": 1555.5555555555557,
      "grad_norm": 6.226003170013428,
      "learning_rate": 3.444444444444445e-05,
      "loss": 3.9017,
      "step": 182000
    },
    {
      "epoch": 1559.8290598290598,
      "grad_norm": 8.66417121887207,
      "learning_rate": 3.4401709401709405e-05,
      "loss": 3.8982,
      "step": 182500
    },
    {
      "epoch": 1564.1025641025642,
      "grad_norm": 6.099926948547363,
      "learning_rate": 3.435897435897436e-05,
      "loss": 3.9001,
      "step": 183000
    },
    {
      "epoch": 1568.3760683760684,
      "grad_norm": 8.28065299987793,
      "learning_rate": 3.431623931623932e-05,
      "loss": 3.9076,
      "step": 183500
    },
    {
      "epoch": 1572.6495726495727,
      "grad_norm": 8.837797164916992,
      "learning_rate": 3.4273504273504276e-05,
      "loss": 3.9016,
      "step": 184000
    },
    {
      "epoch": 1576.923076923077,
      "grad_norm": 6.202044486999512,
      "learning_rate": 3.4230769230769234e-05,
      "loss": 3.8907,
      "step": 184500
    },
    {
      "epoch": 1581.1965811965813,
      "grad_norm": 7.418674945831299,
      "learning_rate": 3.418803418803419e-05,
      "loss": 3.8964,
      "step": 185000
    },
    {
      "epoch": 1585.4700854700855,
      "grad_norm": 7.049860000610352,
      "learning_rate": 3.414529914529915e-05,
      "loss": 3.8881,
      "step": 185500
    },
    {
      "epoch": 1589.7435897435898,
      "grad_norm": 7.665564060211182,
      "learning_rate": 3.4102564102564105e-05,
      "loss": 3.8948,
      "step": 186000
    },
    {
      "epoch": 1594.017094017094,
      "grad_norm": 8.678642272949219,
      "learning_rate": 3.405982905982906e-05,
      "loss": 3.8944,
      "step": 186500
    },
    {
      "epoch": 1598.2905982905984,
      "grad_norm": 8.484149932861328,
      "learning_rate": 3.401709401709402e-05,
      "loss": 3.8939,
      "step": 187000
    },
    {
      "epoch": 1602.5641025641025,
      "grad_norm": 9.234060287475586,
      "learning_rate": 3.397435897435898e-05,
      "loss": 3.901,
      "step": 187500
    },
    {
      "epoch": 1606.837606837607,
      "grad_norm": 5.727376937866211,
      "learning_rate": 3.3931623931623934e-05,
      "loss": 3.8925,
      "step": 188000
    },
    {
      "epoch": 1611.111111111111,
      "grad_norm": 9.838391304016113,
      "learning_rate": 3.388888888888889e-05,
      "loss": 3.8945,
      "step": 188500
    },
    {
      "epoch": 1615.3846153846155,
      "grad_norm": 10.821279525756836,
      "learning_rate": 3.384615384615385e-05,
      "loss": 3.8953,
      "step": 189000
    },
    {
      "epoch": 1619.6581196581196,
      "grad_norm": 5.7223219871521,
      "learning_rate": 3.3803418803418806e-05,
      "loss": 3.8997,
      "step": 189500
    },
    {
      "epoch": 1623.931623931624,
      "grad_norm": 8.224842071533203,
      "learning_rate": 3.376068376068376e-05,
      "loss": 3.8842,
      "step": 190000
    },
    {
      "epoch": 1628.2051282051282,
      "grad_norm": 6.076670169830322,
      "learning_rate": 3.371794871794872e-05,
      "loss": 3.8995,
      "step": 190500
    },
    {
      "epoch": 1632.4786324786326,
      "grad_norm": 7.775661468505859,
      "learning_rate": 3.367521367521368e-05,
      "loss": 3.9035,
      "step": 191000
    },
    {
      "epoch": 1636.7521367521367,
      "grad_norm": 7.644288063049316,
      "learning_rate": 3.3632478632478634e-05,
      "loss": 3.8969,
      "step": 191500
    },
    {
      "epoch": 1641.025641025641,
      "grad_norm": 9.672250747680664,
      "learning_rate": 3.358974358974359e-05,
      "loss": 3.8968,
      "step": 192000
    },
    {
      "epoch": 1645.2991452991453,
      "grad_norm": 8.150096893310547,
      "learning_rate": 3.354700854700855e-05,
      "loss": 3.8912,
      "step": 192500
    },
    {
      "epoch": 1649.5726495726497,
      "grad_norm": 6.204437255859375,
      "learning_rate": 3.3504273504273506e-05,
      "loss": 3.8991,
      "step": 193000
    },
    {
      "epoch": 1653.8461538461538,
      "grad_norm": 12.867668151855469,
      "learning_rate": 3.346153846153846e-05,
      "loss": 3.9024,
      "step": 193500
    },
    {
      "epoch": 1658.1196581196582,
      "grad_norm": 6.426950454711914,
      "learning_rate": 3.341880341880342e-05,
      "loss": 3.8977,
      "step": 194000
    },
    {
      "epoch": 1662.3931623931624,
      "grad_norm": 10.532017707824707,
      "learning_rate": 3.337606837606838e-05,
      "loss": 3.8999,
      "step": 194500
    },
    {
      "epoch": 1666.6666666666667,
      "grad_norm": 9.5266752243042,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 3.8985,
      "step": 195000
    },
    {
      "epoch": 1670.940170940171,
      "grad_norm": 7.417616844177246,
      "learning_rate": 3.329059829059829e-05,
      "loss": 3.8973,
      "step": 195500
    },
    {
      "epoch": 1675.2136752136753,
      "grad_norm": 6.921426773071289,
      "learning_rate": 3.324786324786325e-05,
      "loss": 3.8865,
      "step": 196000
    },
    {
      "epoch": 1679.4871794871794,
      "grad_norm": 6.570374488830566,
      "learning_rate": 3.3205128205128207e-05,
      "loss": 3.8943,
      "step": 196500
    },
    {
      "epoch": 1683.7606837606838,
      "grad_norm": 10.017274856567383,
      "learning_rate": 3.3162393162393164e-05,
      "loss": 3.9022,
      "step": 197000
    },
    {
      "epoch": 1688.034188034188,
      "grad_norm": 6.989814758300781,
      "learning_rate": 3.311965811965812e-05,
      "loss": 3.8898,
      "step": 197500
    },
    {
      "epoch": 1692.3076923076924,
      "grad_norm": 7.459713935852051,
      "learning_rate": 3.307692307692308e-05,
      "loss": 3.8962,
      "step": 198000
    },
    {
      "epoch": 1696.5811965811965,
      "grad_norm": 7.5013556480407715,
      "learning_rate": 3.3034188034188035e-05,
      "loss": 3.8998,
      "step": 198500
    },
    {
      "epoch": 1700.854700854701,
      "grad_norm": 7.486828804016113,
      "learning_rate": 3.299145299145299e-05,
      "loss": 3.8979,
      "step": 199000
    },
    {
      "epoch": 1705.128205128205,
      "grad_norm": 8.461849212646484,
      "learning_rate": 3.294871794871795e-05,
      "loss": 3.8911,
      "step": 199500
    },
    {
      "epoch": 1709.4017094017095,
      "grad_norm": 7.27902889251709,
      "learning_rate": 3.290598290598291e-05,
      "loss": 3.8856,
      "step": 200000
    },
    {
      "epoch": 1713.6752136752136,
      "grad_norm": 7.538240909576416,
      "learning_rate": 3.2863247863247864e-05,
      "loss": 3.8972,
      "step": 200500
    },
    {
      "epoch": 1717.948717948718,
      "grad_norm": 8.056373596191406,
      "learning_rate": 3.282051282051282e-05,
      "loss": 3.8962,
      "step": 201000
    },
    {
      "epoch": 1722.2222222222222,
      "grad_norm": 6.037605285644531,
      "learning_rate": 3.277777777777778e-05,
      "loss": 3.8957,
      "step": 201500
    },
    {
      "epoch": 1726.4957264957266,
      "grad_norm": 6.400764465332031,
      "learning_rate": 3.2735042735042736e-05,
      "loss": 3.8846,
      "step": 202000
    },
    {
      "epoch": 1730.7692307692307,
      "grad_norm": 7.129708766937256,
      "learning_rate": 3.269230769230769e-05,
      "loss": 3.9053,
      "step": 202500
    },
    {
      "epoch": 1735.042735042735,
      "grad_norm": 5.67130184173584,
      "learning_rate": 3.264957264957265e-05,
      "loss": 3.8931,
      "step": 203000
    },
    {
      "epoch": 1739.3162393162393,
      "grad_norm": 8.538399696350098,
      "learning_rate": 3.260683760683761e-05,
      "loss": 3.8978,
      "step": 203500
    },
    {
      "epoch": 1743.5897435897436,
      "grad_norm": 8.595690727233887,
      "learning_rate": 3.2564102564102565e-05,
      "loss": 3.8904,
      "step": 204000
    },
    {
      "epoch": 1747.8632478632478,
      "grad_norm": 9.321552276611328,
      "learning_rate": 3.252136752136752e-05,
      "loss": 3.8901,
      "step": 204500
    },
    {
      "epoch": 1752.1367521367522,
      "grad_norm": 8.754563331604004,
      "learning_rate": 3.247863247863248e-05,
      "loss": 3.8949,
      "step": 205000
    },
    {
      "epoch": 1756.4102564102564,
      "grad_norm": 9.220211029052734,
      "learning_rate": 3.2435897435897436e-05,
      "loss": 3.8954,
      "step": 205500
    },
    {
      "epoch": 1760.6837606837607,
      "grad_norm": 9.443500518798828,
      "learning_rate": 3.2393162393162394e-05,
      "loss": 3.8839,
      "step": 206000
    },
    {
      "epoch": 1764.957264957265,
      "grad_norm": 11.564794540405273,
      "learning_rate": 3.235042735042735e-05,
      "loss": 3.8821,
      "step": 206500
    },
    {
      "epoch": 1769.2307692307693,
      "grad_norm": 6.6147661209106445,
      "learning_rate": 3.230769230769231e-05,
      "loss": 3.8969,
      "step": 207000
    },
    {
      "epoch": 1773.5042735042734,
      "grad_norm": 7.131074905395508,
      "learning_rate": 3.2264957264957265e-05,
      "loss": 3.8926,
      "step": 207500
    },
    {
      "epoch": 1777.7777777777778,
      "grad_norm": 11.808355331420898,
      "learning_rate": 3.222222222222223e-05,
      "loss": 3.8869,
      "step": 208000
    },
    {
      "epoch": 1782.051282051282,
      "grad_norm": 7.039468765258789,
      "learning_rate": 3.2179487179487186e-05,
      "loss": 3.8986,
      "step": 208500
    },
    {
      "epoch": 1786.3247863247864,
      "grad_norm": 5.749073028564453,
      "learning_rate": 3.2136752136752144e-05,
      "loss": 3.8964,
      "step": 209000
    },
    {
      "epoch": 1790.5982905982905,
      "grad_norm": 6.292267799377441,
      "learning_rate": 3.20940170940171e-05,
      "loss": 3.8927,
      "step": 209500
    },
    {
      "epoch": 1794.871794871795,
      "grad_norm": 9.239045143127441,
      "learning_rate": 3.205128205128206e-05,
      "loss": 3.8902,
      "step": 210000
    },
    {
      "epoch": 1799.145299145299,
      "grad_norm": 6.502125263214111,
      "learning_rate": 3.200854700854701e-05,
      "loss": 3.8867,
      "step": 210500
    },
    {
      "epoch": 1803.4188034188035,
      "grad_norm": 7.084902286529541,
      "learning_rate": 3.1965811965811966e-05,
      "loss": 3.9061,
      "step": 211000
    },
    {
      "epoch": 1807.6923076923076,
      "grad_norm": 7.6456732749938965,
      "learning_rate": 3.192307692307692e-05,
      "loss": 3.8864,
      "step": 211500
    },
    {
      "epoch": 1811.965811965812,
      "grad_norm": 5.763060092926025,
      "learning_rate": 3.188034188034188e-05,
      "loss": 3.8909,
      "step": 212000
    },
    {
      "epoch": 1816.2393162393162,
      "grad_norm": 7.946770191192627,
      "learning_rate": 3.183760683760684e-05,
      "loss": 3.8958,
      "step": 212500
    },
    {
      "epoch": 1820.5128205128206,
      "grad_norm": 9.404902458190918,
      "learning_rate": 3.1794871794871795e-05,
      "loss": 3.9,
      "step": 213000
    },
    {
      "epoch": 1824.7863247863247,
      "grad_norm": 7.371310710906982,
      "learning_rate": 3.175213675213675e-05,
      "loss": 3.889,
      "step": 213500
    },
    {
      "epoch": 1829.059829059829,
      "grad_norm": 9.426101684570312,
      "learning_rate": 3.170940170940171e-05,
      "loss": 3.8978,
      "step": 214000
    },
    {
      "epoch": 1833.3333333333333,
      "grad_norm": 7.439797878265381,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 3.8947,
      "step": 214500
    },
    {
      "epoch": 1837.6068376068376,
      "grad_norm": 7.112018585205078,
      "learning_rate": 3.162393162393162e-05,
      "loss": 3.8864,
      "step": 215000
    },
    {
      "epoch": 1841.8803418803418,
      "grad_norm": 7.506447792053223,
      "learning_rate": 3.158119658119658e-05,
      "loss": 3.8878,
      "step": 215500
    },
    {
      "epoch": 1846.1538461538462,
      "grad_norm": 6.3538360595703125,
      "learning_rate": 3.153846153846154e-05,
      "loss": 3.8925,
      "step": 216000
    },
    {
      "epoch": 1850.4273504273503,
      "grad_norm": 5.01438570022583,
      "learning_rate": 3.1495726495726495e-05,
      "loss": 3.8857,
      "step": 216500
    },
    {
      "epoch": 1854.7008547008547,
      "grad_norm": 7.195269584655762,
      "learning_rate": 3.145299145299145e-05,
      "loss": 3.8913,
      "step": 217000
    },
    {
      "epoch": 1858.974358974359,
      "grad_norm": 7.31259822845459,
      "learning_rate": 3.141025641025641e-05,
      "loss": 3.8832,
      "step": 217500
    },
    {
      "epoch": 1863.2478632478633,
      "grad_norm": 6.881649971008301,
      "learning_rate": 3.136752136752137e-05,
      "loss": 3.8949,
      "step": 218000
    },
    {
      "epoch": 1867.5213675213674,
      "grad_norm": 16.550437927246094,
      "learning_rate": 3.1324786324786324e-05,
      "loss": 3.8704,
      "step": 218500
    },
    {
      "epoch": 1871.7948717948718,
      "grad_norm": 6.673086166381836,
      "learning_rate": 3.128205128205128e-05,
      "loss": 3.9022,
      "step": 219000
    },
    {
      "epoch": 1876.068376068376,
      "grad_norm": 6.484838485717773,
      "learning_rate": 3.123931623931624e-05,
      "loss": 3.8867,
      "step": 219500
    },
    {
      "epoch": 1880.3418803418804,
      "grad_norm": 6.951236248016357,
      "learning_rate": 3.1196581196581195e-05,
      "loss": 3.8917,
      "step": 220000
    },
    {
      "epoch": 1884.6153846153845,
      "grad_norm": 7.478229522705078,
      "learning_rate": 3.115384615384615e-05,
      "loss": 3.8967,
      "step": 220500
    },
    {
      "epoch": 1888.888888888889,
      "grad_norm": 6.623932838439941,
      "learning_rate": 3.111111111111111e-05,
      "loss": 3.8853,
      "step": 221000
    },
    {
      "epoch": 1893.162393162393,
      "grad_norm": 9.610457420349121,
      "learning_rate": 3.1068376068376074e-05,
      "loss": 3.8901,
      "step": 221500
    },
    {
      "epoch": 1897.4358974358975,
      "grad_norm": 10.090814590454102,
      "learning_rate": 3.102564102564103e-05,
      "loss": 3.8921,
      "step": 222000
    },
    {
      "epoch": 1901.7094017094016,
      "grad_norm": 7.8843255043029785,
      "learning_rate": 3.098290598290599e-05,
      "loss": 3.8962,
      "step": 222500
    },
    {
      "epoch": 1905.982905982906,
      "grad_norm": 6.479804515838623,
      "learning_rate": 3.0940170940170946e-05,
      "loss": 3.8916,
      "step": 223000
    },
    {
      "epoch": 1910.2564102564102,
      "grad_norm": 8.30970287322998,
      "learning_rate": 3.08974358974359e-05,
      "loss": 3.887,
      "step": 223500
    },
    {
      "epoch": 1914.5299145299145,
      "grad_norm": 5.701936721801758,
      "learning_rate": 3.085470085470086e-05,
      "loss": 3.8818,
      "step": 224000
    },
    {
      "epoch": 1918.8034188034187,
      "grad_norm": 6.481179714202881,
      "learning_rate": 3.081196581196582e-05,
      "loss": 3.8991,
      "step": 224500
    },
    {
      "epoch": 1923.076923076923,
      "grad_norm": 11.29606819152832,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 3.8801,
      "step": 225000
    },
    {
      "epoch": 1927.3504273504273,
      "grad_norm": 7.848442077636719,
      "learning_rate": 3.072649572649573e-05,
      "loss": 3.8901,
      "step": 225500
    },
    {
      "epoch": 1931.6239316239316,
      "grad_norm": 13.637633323669434,
      "learning_rate": 3.068376068376069e-05,
      "loss": 3.8931,
      "step": 226000
    },
    {
      "epoch": 1935.8974358974358,
      "grad_norm": 7.810997486114502,
      "learning_rate": 3.0641025641025646e-05,
      "loss": 3.891,
      "step": 226500
    },
    {
      "epoch": 1940.1709401709402,
      "grad_norm": 7.40074348449707,
      "learning_rate": 3.05982905982906e-05,
      "loss": 3.8881,
      "step": 227000
    },
    {
      "epoch": 1944.4444444444443,
      "grad_norm": 8.273170471191406,
      "learning_rate": 3.055555555555556e-05,
      "loss": 3.8889,
      "step": 227500
    },
    {
      "epoch": 1948.7179487179487,
      "grad_norm": 9.3974609375,
      "learning_rate": 3.0512820512820518e-05,
      "loss": 3.8984,
      "step": 228000
    },
    {
      "epoch": 1952.991452991453,
      "grad_norm": 8.421468734741211,
      "learning_rate": 3.0470085470085475e-05,
      "loss": 3.8999,
      "step": 228500
    },
    {
      "epoch": 1957.2649572649573,
      "grad_norm": 8.131104469299316,
      "learning_rate": 3.0427350427350425e-05,
      "loss": 3.8751,
      "step": 229000
    },
    {
      "epoch": 1961.5384615384614,
      "grad_norm": 7.065290451049805,
      "learning_rate": 3.0384615384615382e-05,
      "loss": 3.8955,
      "step": 229500
    },
    {
      "epoch": 1965.8119658119658,
      "grad_norm": 6.732264518737793,
      "learning_rate": 3.034188034188034e-05,
      "loss": 3.8946,
      "step": 230000
    },
    {
      "epoch": 1970.08547008547,
      "grad_norm": 8.119376182556152,
      "learning_rate": 3.0299145299145297e-05,
      "loss": 3.887,
      "step": 230500
    },
    {
      "epoch": 1974.3589743589744,
      "grad_norm": 7.5766191482543945,
      "learning_rate": 3.0256410256410257e-05,
      "loss": 3.8821,
      "step": 231000
    },
    {
      "epoch": 1978.6324786324785,
      "grad_norm": 8.661247253417969,
      "learning_rate": 3.0213675213675215e-05,
      "loss": 3.8891,
      "step": 231500
    },
    {
      "epoch": 1982.905982905983,
      "grad_norm": 8.754638671875,
      "learning_rate": 3.0170940170940172e-05,
      "loss": 3.8866,
      "step": 232000
    },
    {
      "epoch": 1987.179487179487,
      "grad_norm": 8.595465660095215,
      "learning_rate": 3.012820512820513e-05,
      "loss": 3.8903,
      "step": 232500
    },
    {
      "epoch": 1991.4529914529915,
      "grad_norm": 8.209895133972168,
      "learning_rate": 3.0085470085470086e-05,
      "loss": 3.8912,
      "step": 233000
    },
    {
      "epoch": 1995.7264957264956,
      "grad_norm": 6.755066871643066,
      "learning_rate": 3.0042735042735044e-05,
      "loss": 3.8801,
      "step": 233500
    },
    {
      "epoch": 2000.0,
      "grad_norm": 13.488792419433594,
      "learning_rate": 3e-05,
      "loss": 3.8819,
      "step": 234000
    },
    {
      "epoch": 2004.2735042735044,
      "grad_norm": 6.161379814147949,
      "learning_rate": 2.9957264957264958e-05,
      "loss": 3.8805,
      "step": 234500
    },
    {
      "epoch": 2008.5470085470085,
      "grad_norm": 8.607243537902832,
      "learning_rate": 2.9914529914529915e-05,
      "loss": 3.8857,
      "step": 235000
    },
    {
      "epoch": 2012.820512820513,
      "grad_norm": 7.7503533363342285,
      "learning_rate": 2.9871794871794872e-05,
      "loss": 3.882,
      "step": 235500
    },
    {
      "epoch": 2017.094017094017,
      "grad_norm": 7.442008972167969,
      "learning_rate": 2.982905982905983e-05,
      "loss": 3.8908,
      "step": 236000
    },
    {
      "epoch": 2021.3675213675215,
      "grad_norm": 7.847203731536865,
      "learning_rate": 2.9786324786324787e-05,
      "loss": 3.8855,
      "step": 236500
    },
    {
      "epoch": 2025.6410256410256,
      "grad_norm": 7.374721527099609,
      "learning_rate": 2.9743589743589744e-05,
      "loss": 3.8806,
      "step": 237000
    },
    {
      "epoch": 2029.91452991453,
      "grad_norm": 8.79566764831543,
      "learning_rate": 2.97008547008547e-05,
      "loss": 3.8865,
      "step": 237500
    },
    {
      "epoch": 2034.1880341880342,
      "grad_norm": 14.51423168182373,
      "learning_rate": 2.965811965811966e-05,
      "loss": 3.8886,
      "step": 238000
    },
    {
      "epoch": 2038.4615384615386,
      "grad_norm": 7.160704612731934,
      "learning_rate": 2.9615384615384616e-05,
      "loss": 3.8704,
      "step": 238500
    },
    {
      "epoch": 2042.7350427350427,
      "grad_norm": 12.519752502441406,
      "learning_rate": 2.9572649572649573e-05,
      "loss": 3.8879,
      "step": 239000
    },
    {
      "epoch": 2047.008547008547,
      "grad_norm": 6.556916236877441,
      "learning_rate": 2.9529914529914533e-05,
      "loss": 3.8914,
      "step": 239500
    },
    {
      "epoch": 2051.2820512820513,
      "grad_norm": 8.98116397857666,
      "learning_rate": 2.948717948717949e-05,
      "loss": 3.8909,
      "step": 240000
    },
    {
      "epoch": 2055.5555555555557,
      "grad_norm": 8.508842468261719,
      "learning_rate": 2.9444444444444448e-05,
      "loss": 3.8778,
      "step": 240500
    },
    {
      "epoch": 2059.82905982906,
      "grad_norm": 8.408182144165039,
      "learning_rate": 2.9401709401709405e-05,
      "loss": 3.8833,
      "step": 241000
    },
    {
      "epoch": 2064.102564102564,
      "grad_norm": 9.404754638671875,
      "learning_rate": 2.9358974358974362e-05,
      "loss": 3.8826,
      "step": 241500
    },
    {
      "epoch": 2068.3760683760684,
      "grad_norm": 5.8262786865234375,
      "learning_rate": 2.931623931623932e-05,
      "loss": 3.8728,
      "step": 242000
    },
    {
      "epoch": 2072.6495726495727,
      "grad_norm": 7.214557647705078,
      "learning_rate": 2.9273504273504277e-05,
      "loss": 3.8834,
      "step": 242500
    },
    {
      "epoch": 2076.923076923077,
      "grad_norm": 7.555171966552734,
      "learning_rate": 2.9230769230769234e-05,
      "loss": 3.8825,
      "step": 243000
    },
    {
      "epoch": 2081.196581196581,
      "grad_norm": 8.890862464904785,
      "learning_rate": 2.918803418803419e-05,
      "loss": 3.8844,
      "step": 243500
    },
    {
      "epoch": 2085.4700854700855,
      "grad_norm": 10.090734481811523,
      "learning_rate": 2.914529914529915e-05,
      "loss": 3.8762,
      "step": 244000
    },
    {
      "epoch": 2089.74358974359,
      "grad_norm": 9.340322494506836,
      "learning_rate": 2.9102564102564106e-05,
      "loss": 3.887,
      "step": 244500
    },
    {
      "epoch": 2094.017094017094,
      "grad_norm": 8.470709800720215,
      "learning_rate": 2.9059829059829063e-05,
      "loss": 3.8901,
      "step": 245000
    },
    {
      "epoch": 2098.290598290598,
      "grad_norm": 6.290541648864746,
      "learning_rate": 2.901709401709402e-05,
      "loss": 3.8753,
      "step": 245500
    },
    {
      "epoch": 2102.5641025641025,
      "grad_norm": 7.43135404586792,
      "learning_rate": 2.8974358974358977e-05,
      "loss": 3.8901,
      "step": 246000
    },
    {
      "epoch": 2106.837606837607,
      "grad_norm": 9.646260261535645,
      "learning_rate": 2.8931623931623934e-05,
      "loss": 3.8907,
      "step": 246500
    },
    {
      "epoch": 2111.1111111111113,
      "grad_norm": 8.557271957397461,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 3.883,
      "step": 247000
    },
    {
      "epoch": 2115.3846153846152,
      "grad_norm": 6.014499664306641,
      "learning_rate": 2.8846153846153845e-05,
      "loss": 3.8818,
      "step": 247500
    },
    {
      "epoch": 2119.6581196581196,
      "grad_norm": 16.18816566467285,
      "learning_rate": 2.8803418803418803e-05,
      "loss": 3.8855,
      "step": 248000
    },
    {
      "epoch": 2123.931623931624,
      "grad_norm": 9.866352081298828,
      "learning_rate": 2.876068376068376e-05,
      "loss": 3.8837,
      "step": 248500
    },
    {
      "epoch": 2128.2051282051284,
      "grad_norm": 5.937504768371582,
      "learning_rate": 2.8717948717948717e-05,
      "loss": 3.8865,
      "step": 249000
    },
    {
      "epoch": 2132.4786324786323,
      "grad_norm": 7.571413040161133,
      "learning_rate": 2.8675213675213674e-05,
      "loss": 3.8825,
      "step": 249500
    },
    {
      "epoch": 2136.7521367521367,
      "grad_norm": 6.662204265594482,
      "learning_rate": 2.863247863247863e-05,
      "loss": 3.8715,
      "step": 250000
    },
    {
      "epoch": 2141.025641025641,
      "grad_norm": 7.4684977531433105,
      "learning_rate": 2.858974358974359e-05,
      "loss": 3.8866,
      "step": 250500
    },
    {
      "epoch": 2145.2991452991455,
      "grad_norm": 17.159059524536133,
      "learning_rate": 2.8547008547008546e-05,
      "loss": 3.8745,
      "step": 251000
    },
    {
      "epoch": 2149.5726495726494,
      "grad_norm": 7.583874225616455,
      "learning_rate": 2.8504273504273503e-05,
      "loss": 3.8782,
      "step": 251500
    },
    {
      "epoch": 2153.846153846154,
      "grad_norm": 6.983214855194092,
      "learning_rate": 2.846153846153846e-05,
      "loss": 3.882,
      "step": 252000
    },
    {
      "epoch": 2158.119658119658,
      "grad_norm": 9.176597595214844,
      "learning_rate": 2.8418803418803418e-05,
      "loss": 3.8809,
      "step": 252500
    },
    {
      "epoch": 2162.3931623931626,
      "grad_norm": 9.366825103759766,
      "learning_rate": 2.8376068376068378e-05,
      "loss": 3.8821,
      "step": 253000
    },
    {
      "epoch": 2166.6666666666665,
      "grad_norm": 8.346723556518555,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 3.8741,
      "step": 253500
    },
    {
      "epoch": 2170.940170940171,
      "grad_norm": 8.104876518249512,
      "learning_rate": 2.8290598290598293e-05,
      "loss": 3.8855,
      "step": 254000
    },
    {
      "epoch": 2175.2136752136753,
      "grad_norm": 6.089850425720215,
      "learning_rate": 2.824786324786325e-05,
      "loss": 3.8747,
      "step": 254500
    },
    {
      "epoch": 2179.4871794871797,
      "grad_norm": 6.674896240234375,
      "learning_rate": 2.8205128205128207e-05,
      "loss": 3.8791,
      "step": 255000
    },
    {
      "epoch": 2183.7606837606836,
      "grad_norm": 8.166886329650879,
      "learning_rate": 2.8162393162393164e-05,
      "loss": 3.8807,
      "step": 255500
    },
    {
      "epoch": 2188.034188034188,
      "grad_norm": 8.226521492004395,
      "learning_rate": 2.811965811965812e-05,
      "loss": 3.871,
      "step": 256000
    },
    {
      "epoch": 2192.3076923076924,
      "grad_norm": 6.502252101898193,
      "learning_rate": 2.807692307692308e-05,
      "loss": 3.8863,
      "step": 256500
    },
    {
      "epoch": 2196.5811965811968,
      "grad_norm": 7.288045883178711,
      "learning_rate": 2.8034188034188036e-05,
      "loss": 3.8805,
      "step": 257000
    },
    {
      "epoch": 2200.8547008547007,
      "grad_norm": 5.802639484405518,
      "learning_rate": 2.7991452991452993e-05,
      "loss": 3.8887,
      "step": 257500
    },
    {
      "epoch": 2205.128205128205,
      "grad_norm": 8.17229175567627,
      "learning_rate": 2.794871794871795e-05,
      "loss": 3.8812,
      "step": 258000
    },
    {
      "epoch": 2209.4017094017095,
      "grad_norm": 8.413043022155762,
      "learning_rate": 2.7905982905982907e-05,
      "loss": 3.8833,
      "step": 258500
    },
    {
      "epoch": 2213.675213675214,
      "grad_norm": 7.775301456451416,
      "learning_rate": 2.7863247863247865e-05,
      "loss": 3.8904,
      "step": 259000
    },
    {
      "epoch": 2217.948717948718,
      "grad_norm": 7.641665458679199,
      "learning_rate": 2.7820512820512822e-05,
      "loss": 3.8786,
      "step": 259500
    },
    {
      "epoch": 2222.222222222222,
      "grad_norm": 7.188630104064941,
      "learning_rate": 2.777777777777778e-05,
      "loss": 3.8828,
      "step": 260000
    },
    {
      "epoch": 2226.4957264957266,
      "grad_norm": 5.554131031036377,
      "learning_rate": 2.7735042735042736e-05,
      "loss": 3.8816,
      "step": 260500
    },
    {
      "epoch": 2230.769230769231,
      "grad_norm": 10.621453285217285,
      "learning_rate": 2.7692307692307694e-05,
      "loss": 3.8838,
      "step": 261000
    },
    {
      "epoch": 2235.042735042735,
      "grad_norm": 8.104023933410645,
      "learning_rate": 2.7649572649572654e-05,
      "loss": 3.8905,
      "step": 261500
    },
    {
      "epoch": 2239.3162393162393,
      "grad_norm": 9.494000434875488,
      "learning_rate": 2.760683760683761e-05,
      "loss": 3.883,
      "step": 262000
    },
    {
      "epoch": 2243.5897435897436,
      "grad_norm": 7.668914794921875,
      "learning_rate": 2.756410256410257e-05,
      "loss": 3.881,
      "step": 262500
    },
    {
      "epoch": 2247.863247863248,
      "grad_norm": 9.970454216003418,
      "learning_rate": 2.7521367521367526e-05,
      "loss": 3.8873,
      "step": 263000
    },
    {
      "epoch": 2252.136752136752,
      "grad_norm": 6.333165645599365,
      "learning_rate": 2.7478632478632483e-05,
      "loss": 3.9005,
      "step": 263500
    },
    {
      "epoch": 2256.4102564102564,
      "grad_norm": 6.482918739318848,
      "learning_rate": 2.743589743589744e-05,
      "loss": 3.8759,
      "step": 264000
    },
    {
      "epoch": 2260.6837606837607,
      "grad_norm": 7.01277494430542,
      "learning_rate": 2.7393162393162397e-05,
      "loss": 3.8877,
      "step": 264500
    },
    {
      "epoch": 2264.957264957265,
      "grad_norm": 7.115384578704834,
      "learning_rate": 2.7350427350427355e-05,
      "loss": 3.8803,
      "step": 265000
    },
    {
      "epoch": 2269.230769230769,
      "grad_norm": 8.14963150024414,
      "learning_rate": 2.7307692307692305e-05,
      "loss": 3.8916,
      "step": 265500
    },
    {
      "epoch": 2273.5042735042734,
      "grad_norm": 6.9931960105896,
      "learning_rate": 2.7264957264957262e-05,
      "loss": 3.8831,
      "step": 266000
    },
    {
      "epoch": 2277.777777777778,
      "grad_norm": 7.193343162536621,
      "learning_rate": 2.7222222222222223e-05,
      "loss": 3.8752,
      "step": 266500
    },
    {
      "epoch": 2282.051282051282,
      "grad_norm": 7.377634525299072,
      "learning_rate": 2.717948717948718e-05,
      "loss": 3.8911,
      "step": 267000
    },
    {
      "epoch": 2286.324786324786,
      "grad_norm": 8.467179298400879,
      "learning_rate": 2.7136752136752137e-05,
      "loss": 3.8781,
      "step": 267500
    },
    {
      "epoch": 2290.5982905982905,
      "grad_norm": 6.214844703674316,
      "learning_rate": 2.7094017094017094e-05,
      "loss": 3.8864,
      "step": 268000
    },
    {
      "epoch": 2294.871794871795,
      "grad_norm": 5.960350513458252,
      "learning_rate": 2.705128205128205e-05,
      "loss": 3.8802,
      "step": 268500
    },
    {
      "epoch": 2299.1452991452993,
      "grad_norm": 10.185103416442871,
      "learning_rate": 2.700854700854701e-05,
      "loss": 3.878,
      "step": 269000
    },
    {
      "epoch": 2303.4188034188032,
      "grad_norm": 12.02734661102295,
      "learning_rate": 2.6965811965811966e-05,
      "loss": 3.8848,
      "step": 269500
    },
    {
      "epoch": 2307.6923076923076,
      "grad_norm": 10.017751693725586,
      "learning_rate": 2.6923076923076923e-05,
      "loss": 3.8808,
      "step": 270000
    },
    {
      "epoch": 2311.965811965812,
      "grad_norm": 5.48105525970459,
      "learning_rate": 2.688034188034188e-05,
      "loss": 3.8847,
      "step": 270500
    },
    {
      "epoch": 2316.2393162393164,
      "grad_norm": 8.660683631896973,
      "learning_rate": 2.6837606837606838e-05,
      "loss": 3.876,
      "step": 271000
    },
    {
      "epoch": 2320.5128205128203,
      "grad_norm": 10.734576225280762,
      "learning_rate": 2.6794871794871795e-05,
      "loss": 3.88,
      "step": 271500
    },
    {
      "epoch": 2324.7863247863247,
      "grad_norm": 6.31517219543457,
      "learning_rate": 2.6752136752136752e-05,
      "loss": 3.8756,
      "step": 272000
    },
    {
      "epoch": 2329.059829059829,
      "grad_norm": 10.40366268157959,
      "learning_rate": 2.670940170940171e-05,
      "loss": 3.8828,
      "step": 272500
    },
    {
      "epoch": 2333.3333333333335,
      "grad_norm": 12.710843086242676,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 3.8736,
      "step": 273000
    },
    {
      "epoch": 2337.6068376068374,
      "grad_norm": 8.993544578552246,
      "learning_rate": 2.6623931623931624e-05,
      "loss": 3.8755,
      "step": 273500
    },
    {
      "epoch": 2341.880341880342,
      "grad_norm": 6.1174092292785645,
      "learning_rate": 2.658119658119658e-05,
      "loss": 3.8789,
      "step": 274000
    },
    {
      "epoch": 2346.153846153846,
      "grad_norm": 6.645010471343994,
      "learning_rate": 2.6538461538461538e-05,
      "loss": 3.8785,
      "step": 274500
    },
    {
      "epoch": 2350.4273504273506,
      "grad_norm": 9.59327507019043,
      "learning_rate": 2.64957264957265e-05,
      "loss": 3.8714,
      "step": 275000
    },
    {
      "epoch": 2354.7008547008545,
      "grad_norm": 6.838067531585693,
      "learning_rate": 2.6452991452991456e-05,
      "loss": 3.8719,
      "step": 275500
    },
    {
      "epoch": 2358.974358974359,
      "grad_norm": 9.372518539428711,
      "learning_rate": 2.6410256410256413e-05,
      "loss": 3.8826,
      "step": 276000
    },
    {
      "epoch": 2363.2478632478633,
      "grad_norm": 8.992594718933105,
      "learning_rate": 2.636752136752137e-05,
      "loss": 3.8797,
      "step": 276500
    },
    {
      "epoch": 2367.5213675213677,
      "grad_norm": 7.3308000564575195,
      "learning_rate": 2.6324786324786328e-05,
      "loss": 3.8821,
      "step": 277000
    },
    {
      "epoch": 2371.7948717948716,
      "grad_norm": 7.271777153015137,
      "learning_rate": 2.6282051282051285e-05,
      "loss": 3.8748,
      "step": 277500
    },
    {
      "epoch": 2376.068376068376,
      "grad_norm": 9.10300350189209,
      "learning_rate": 2.6239316239316242e-05,
      "loss": 3.8773,
      "step": 278000
    },
    {
      "epoch": 2380.3418803418804,
      "grad_norm": 8.44090461730957,
      "learning_rate": 2.61965811965812e-05,
      "loss": 3.8791,
      "step": 278500
    },
    {
      "epoch": 2384.6153846153848,
      "grad_norm": 7.383447170257568,
      "learning_rate": 2.6153846153846157e-05,
      "loss": 3.8729,
      "step": 279000
    },
    {
      "epoch": 2388.8888888888887,
      "grad_norm": 11.52219295501709,
      "learning_rate": 2.6111111111111114e-05,
      "loss": 3.8788,
      "step": 279500
    },
    {
      "epoch": 2393.162393162393,
      "grad_norm": 6.951722621917725,
      "learning_rate": 2.606837606837607e-05,
      "loss": 3.8875,
      "step": 280000
    },
    {
      "epoch": 2397.4358974358975,
      "grad_norm": 10.634651184082031,
      "learning_rate": 2.6025641025641028e-05,
      "loss": 3.8658,
      "step": 280500
    },
    {
      "epoch": 2401.709401709402,
      "grad_norm": 11.418190956115723,
      "learning_rate": 2.5982905982905985e-05,
      "loss": 3.872,
      "step": 281000
    },
    {
      "epoch": 2405.982905982906,
      "grad_norm": 10.080662727355957,
      "learning_rate": 2.5940170940170943e-05,
      "loss": 3.8767,
      "step": 281500
    },
    {
      "epoch": 2410.25641025641,
      "grad_norm": 9.443284034729004,
      "learning_rate": 2.58974358974359e-05,
      "loss": 3.8776,
      "step": 282000
    },
    {
      "epoch": 2414.5299145299145,
      "grad_norm": 11.592992782592773,
      "learning_rate": 2.5854700854700857e-05,
      "loss": 3.8803,
      "step": 282500
    },
    {
      "epoch": 2418.803418803419,
      "grad_norm": 9.650859832763672,
      "learning_rate": 2.5811965811965814e-05,
      "loss": 3.8834,
      "step": 283000
    },
    {
      "epoch": 2423.076923076923,
      "grad_norm": 6.94976282119751,
      "learning_rate": 2.5769230769230768e-05,
      "loss": 3.8753,
      "step": 283500
    },
    {
      "epoch": 2427.3504273504273,
      "grad_norm": 6.173253536224365,
      "learning_rate": 2.5726495726495725e-05,
      "loss": 3.8831,
      "step": 284000
    },
    {
      "epoch": 2431.6239316239316,
      "grad_norm": 7.611146926879883,
      "learning_rate": 2.5683760683760682e-05,
      "loss": 3.8877,
      "step": 284500
    },
    {
      "epoch": 2435.897435897436,
      "grad_norm": 6.920261859893799,
      "learning_rate": 2.564102564102564e-05,
      "loss": 3.8732,
      "step": 285000
    },
    {
      "epoch": 2440.17094017094,
      "grad_norm": 7.3132643699646,
      "learning_rate": 2.5598290598290597e-05,
      "loss": 3.8735,
      "step": 285500
    },
    {
      "epoch": 2444.4444444444443,
      "grad_norm": 8.192848205566406,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 3.8741,
      "step": 286000
    },
    {
      "epoch": 2448.7179487179487,
      "grad_norm": 5.364180088043213,
      "learning_rate": 2.551282051282051e-05,
      "loss": 3.8805,
      "step": 286500
    },
    {
      "epoch": 2452.991452991453,
      "grad_norm": 7.3438401222229,
      "learning_rate": 2.547008547008547e-05,
      "loss": 3.8736,
      "step": 287000
    },
    {
      "epoch": 2457.264957264957,
      "grad_norm": 5.497261047363281,
      "learning_rate": 2.5427350427350426e-05,
      "loss": 3.8821,
      "step": 287500
    },
    {
      "epoch": 2461.5384615384614,
      "grad_norm": 8.915410041809082,
      "learning_rate": 2.5384615384615383e-05,
      "loss": 3.8806,
      "step": 288000
    },
    {
      "epoch": 2465.811965811966,
      "grad_norm": 7.263113498687744,
      "learning_rate": 2.5341880341880344e-05,
      "loss": 3.8794,
      "step": 288500
    },
    {
      "epoch": 2470.08547008547,
      "grad_norm": 7.692009449005127,
      "learning_rate": 2.52991452991453e-05,
      "loss": 3.8677,
      "step": 289000
    },
    {
      "epoch": 2474.358974358974,
      "grad_norm": 8.90407943725586,
      "learning_rate": 2.5256410256410258e-05,
      "loss": 3.8791,
      "step": 289500
    },
    {
      "epoch": 2478.6324786324785,
      "grad_norm": 6.362864017486572,
      "learning_rate": 2.5213675213675215e-05,
      "loss": 3.8827,
      "step": 290000
    },
    {
      "epoch": 2482.905982905983,
      "grad_norm": 6.796254634857178,
      "learning_rate": 2.5170940170940172e-05,
      "loss": 3.8855,
      "step": 290500
    },
    {
      "epoch": 2487.1794871794873,
      "grad_norm": 6.806291103363037,
      "learning_rate": 2.512820512820513e-05,
      "loss": 3.8709,
      "step": 291000
    },
    {
      "epoch": 2491.4529914529912,
      "grad_norm": 8.026284217834473,
      "learning_rate": 2.5085470085470087e-05,
      "loss": 3.8788,
      "step": 291500
    },
    {
      "epoch": 2495.7264957264956,
      "grad_norm": 6.414483547210693,
      "learning_rate": 2.5042735042735044e-05,
      "loss": 3.8712,
      "step": 292000
    },
    {
      "epoch": 2500.0,
      "grad_norm": 7.5873918533325195,
      "learning_rate": 2.5e-05,
      "loss": 3.8769,
      "step": 292500
    },
    {
      "epoch": 2504.2735042735044,
      "grad_norm": 10.509665489196777,
      "learning_rate": 2.495726495726496e-05,
      "loss": 3.871,
      "step": 293000
    },
    {
      "epoch": 2508.5470085470088,
      "grad_norm": 9.324054718017578,
      "learning_rate": 2.4914529914529916e-05,
      "loss": 3.8746,
      "step": 293500
    },
    {
      "epoch": 2512.8205128205127,
      "grad_norm": 7.061398506164551,
      "learning_rate": 2.4871794871794873e-05,
      "loss": 3.8769,
      "step": 294000
    },
    {
      "epoch": 2517.094017094017,
      "grad_norm": 7.219516754150391,
      "learning_rate": 2.482905982905983e-05,
      "loss": 3.875,
      "step": 294500
    },
    {
      "epoch": 2521.3675213675215,
      "grad_norm": 5.224717140197754,
      "learning_rate": 2.4786324786324787e-05,
      "loss": 3.8815,
      "step": 295000
    },
    {
      "epoch": 2525.641025641026,
      "grad_norm": 7.176068305969238,
      "learning_rate": 2.4743589743589744e-05,
      "loss": 3.8804,
      "step": 295500
    },
    {
      "epoch": 2529.91452991453,
      "grad_norm": 6.234370708465576,
      "learning_rate": 2.47008547008547e-05,
      "loss": 3.8688,
      "step": 296000
    },
    {
      "epoch": 2534.188034188034,
      "grad_norm": 6.433103561401367,
      "learning_rate": 2.465811965811966e-05,
      "loss": 3.878,
      "step": 296500
    },
    {
      "epoch": 2538.4615384615386,
      "grad_norm": 7.126989841461182,
      "learning_rate": 2.461538461538462e-05,
      "loss": 3.8813,
      "step": 297000
    },
    {
      "epoch": 2542.735042735043,
      "grad_norm": 6.756336688995361,
      "learning_rate": 2.4572649572649573e-05,
      "loss": 3.8712,
      "step": 297500
    },
    {
      "epoch": 2547.008547008547,
      "grad_norm": 8.59756088256836,
      "learning_rate": 2.452991452991453e-05,
      "loss": 3.8675,
      "step": 298000
    },
    {
      "epoch": 2551.2820512820513,
      "grad_norm": 10.563742637634277,
      "learning_rate": 2.4487179487179488e-05,
      "loss": 3.8749,
      "step": 298500
    },
    {
      "epoch": 2555.5555555555557,
      "grad_norm": 6.353139877319336,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 3.8758,
      "step": 299000
    },
    {
      "epoch": 2559.82905982906,
      "grad_norm": 7.328649520874023,
      "learning_rate": 2.4401709401709402e-05,
      "loss": 3.8843,
      "step": 299500
    },
    {
      "epoch": 2564.102564102564,
      "grad_norm": 8.953536987304688,
      "learning_rate": 2.435897435897436e-05,
      "loss": 3.8693,
      "step": 300000
    },
    {
      "epoch": 2568.3760683760684,
      "grad_norm": 8.307686805725098,
      "learning_rate": 2.4316239316239317e-05,
      "loss": 3.8733,
      "step": 300500
    },
    {
      "epoch": 2572.6495726495727,
      "grad_norm": 8.505088806152344,
      "learning_rate": 2.4273504273504274e-05,
      "loss": 3.8623,
      "step": 301000
    },
    {
      "epoch": 2576.923076923077,
      "grad_norm": 7.363946914672852,
      "learning_rate": 2.423076923076923e-05,
      "loss": 3.8902,
      "step": 301500
    },
    {
      "epoch": 2581.196581196581,
      "grad_norm": 8.548291206359863,
      "learning_rate": 2.4188034188034188e-05,
      "loss": 3.8766,
      "step": 302000
    },
    {
      "epoch": 2585.4700854700855,
      "grad_norm": 7.895135879516602,
      "learning_rate": 2.4145299145299145e-05,
      "loss": 3.8821,
      "step": 302500
    },
    {
      "epoch": 2589.74358974359,
      "grad_norm": 7.089999675750732,
      "learning_rate": 2.4102564102564103e-05,
      "loss": 3.8714,
      "step": 303000
    },
    {
      "epoch": 2594.017094017094,
      "grad_norm": 7.944955825805664,
      "learning_rate": 2.4059829059829063e-05,
      "loss": 3.8758,
      "step": 303500
    },
    {
      "epoch": 2598.290598290598,
      "grad_norm": 8.913530349731445,
      "learning_rate": 2.401709401709402e-05,
      "loss": 3.8649,
      "step": 304000
    },
    {
      "epoch": 2602.5641025641025,
      "grad_norm": 16.198673248291016,
      "learning_rate": 2.3974358974358978e-05,
      "loss": 3.8769,
      "step": 304500
    },
    {
      "epoch": 2606.837606837607,
      "grad_norm": 7.882073879241943,
      "learning_rate": 2.3931623931623935e-05,
      "loss": 3.8818,
      "step": 305000
    },
    {
      "epoch": 2611.1111111111113,
      "grad_norm": 6.958526134490967,
      "learning_rate": 2.3888888888888892e-05,
      "loss": 3.8716,
      "step": 305500
    },
    {
      "epoch": 2615.3846153846152,
      "grad_norm": 18.518587112426758,
      "learning_rate": 2.384615384615385e-05,
      "loss": 3.8811,
      "step": 306000
    },
    {
      "epoch": 2619.6581196581196,
      "grad_norm": 6.914005756378174,
      "learning_rate": 2.3803418803418803e-05,
      "loss": 3.8818,
      "step": 306500
    },
    {
      "epoch": 2623.931623931624,
      "grad_norm": 7.0570478439331055,
      "learning_rate": 2.376068376068376e-05,
      "loss": 3.8718,
      "step": 307000
    },
    {
      "epoch": 2628.2051282051284,
      "grad_norm": 6.809906482696533,
      "learning_rate": 2.3717948717948718e-05,
      "loss": 3.8714,
      "step": 307500
    },
    {
      "epoch": 2632.4786324786323,
      "grad_norm": 7.897292613983154,
      "learning_rate": 2.3675213675213675e-05,
      "loss": 3.8673,
      "step": 308000
    },
    {
      "epoch": 2636.7521367521367,
      "grad_norm": 9.310554504394531,
      "learning_rate": 2.3632478632478632e-05,
      "loss": 3.8841,
      "step": 308500
    },
    {
      "epoch": 2641.025641025641,
      "grad_norm": 6.555904865264893,
      "learning_rate": 2.358974358974359e-05,
      "loss": 3.8652,
      "step": 309000
    },
    {
      "epoch": 2645.2991452991455,
      "grad_norm": 9.235993385314941,
      "learning_rate": 2.3547008547008546e-05,
      "loss": 3.8778,
      "step": 309500
    },
    {
      "epoch": 2649.5726495726494,
      "grad_norm": 8.557612419128418,
      "learning_rate": 2.3504273504273504e-05,
      "loss": 3.8907,
      "step": 310000
    },
    {
      "epoch": 2653.846153846154,
      "grad_norm": 10.734793663024902,
      "learning_rate": 2.3461538461538464e-05,
      "loss": 3.8764,
      "step": 310500
    },
    {
      "epoch": 2658.119658119658,
      "grad_norm": 5.924179553985596,
      "learning_rate": 2.341880341880342e-05,
      "loss": 3.8705,
      "step": 311000
    },
    {
      "epoch": 2662.3931623931626,
      "grad_norm": 9.000740051269531,
      "learning_rate": 2.337606837606838e-05,
      "loss": 3.8709,
      "step": 311500
    },
    {
      "epoch": 2666.6666666666665,
      "grad_norm": 6.646178722381592,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 3.8825,
      "step": 312000
    },
    {
      "epoch": 2670.940170940171,
      "grad_norm": 8.096332550048828,
      "learning_rate": 2.3290598290598293e-05,
      "loss": 3.875,
      "step": 312500
    },
    {
      "epoch": 2675.2136752136753,
      "grad_norm": 8.37933349609375,
      "learning_rate": 2.324786324786325e-05,
      "loss": 3.8742,
      "step": 313000
    },
    {
      "epoch": 2679.4871794871797,
      "grad_norm": 6.344832420349121,
      "learning_rate": 2.3205128205128207e-05,
      "loss": 3.8778,
      "step": 313500
    },
    {
      "epoch": 2683.7606837606836,
      "grad_norm": 10.371866226196289,
      "learning_rate": 2.3162393162393165e-05,
      "loss": 3.8644,
      "step": 314000
    },
    {
      "epoch": 2688.034188034188,
      "grad_norm": 10.931112289428711,
      "learning_rate": 2.3119658119658122e-05,
      "loss": 3.8811,
      "step": 314500
    },
    {
      "epoch": 2692.3076923076924,
      "grad_norm": 8.818549156188965,
      "learning_rate": 2.307692307692308e-05,
      "loss": 3.8683,
      "step": 315000
    },
    {
      "epoch": 2696.5811965811968,
      "grad_norm": 7.341259956359863,
      "learning_rate": 2.3034188034188033e-05,
      "loss": 3.8746,
      "step": 315500
    },
    {
      "epoch": 2700.8547008547007,
      "grad_norm": 5.200289726257324,
      "learning_rate": 2.299145299145299e-05,
      "loss": 3.8721,
      "step": 316000
    },
    {
      "epoch": 2705.128205128205,
      "grad_norm": 6.715023994445801,
      "learning_rate": 2.2948717948717947e-05,
      "loss": 3.8736,
      "step": 316500
    },
    {
      "epoch": 2709.4017094017095,
      "grad_norm": 5.644590377807617,
      "learning_rate": 2.2905982905982905e-05,
      "loss": 3.871,
      "step": 317000
    },
    {
      "epoch": 2713.675213675214,
      "grad_norm": 6.774980545043945,
      "learning_rate": 2.2863247863247865e-05,
      "loss": 3.8822,
      "step": 317500
    },
    {
      "epoch": 2717.948717948718,
      "grad_norm": 12.29592227935791,
      "learning_rate": 2.2820512820512822e-05,
      "loss": 3.8654,
      "step": 318000
    },
    {
      "epoch": 2722.222222222222,
      "grad_norm": 10.068364143371582,
      "learning_rate": 2.277777777777778e-05,
      "loss": 3.8733,
      "step": 318500
    },
    {
      "epoch": 2726.4957264957266,
      "grad_norm": 5.378899097442627,
      "learning_rate": 2.2735042735042737e-05,
      "loss": 3.8743,
      "step": 319000
    },
    {
      "epoch": 2730.769230769231,
      "grad_norm": 7.6216349601745605,
      "learning_rate": 2.2692307692307694e-05,
      "loss": 3.87,
      "step": 319500
    },
    {
      "epoch": 2735.042735042735,
      "grad_norm": 8.469145774841309,
      "learning_rate": 2.264957264957265e-05,
      "loss": 3.8687,
      "step": 320000
    },
    {
      "epoch": 2739.3162393162393,
      "grad_norm": 8.474568367004395,
      "learning_rate": 2.260683760683761e-05,
      "loss": 3.8734,
      "step": 320500
    },
    {
      "epoch": 2743.5897435897436,
      "grad_norm": 7.157370567321777,
      "learning_rate": 2.2564102564102566e-05,
      "loss": 3.8731,
      "step": 321000
    },
    {
      "epoch": 2747.863247863248,
      "grad_norm": 11.739134788513184,
      "learning_rate": 2.2521367521367523e-05,
      "loss": 3.8726,
      "step": 321500
    },
    {
      "epoch": 2752.136752136752,
      "grad_norm": 8.606283187866211,
      "learning_rate": 2.247863247863248e-05,
      "loss": 3.8815,
      "step": 322000
    },
    {
      "epoch": 2756.4102564102564,
      "grad_norm": 11.061966896057129,
      "learning_rate": 2.2435897435897437e-05,
      "loss": 3.8682,
      "step": 322500
    },
    {
      "epoch": 2760.6837606837607,
      "grad_norm": 8.50113582611084,
      "learning_rate": 2.2393162393162394e-05,
      "loss": 3.8644,
      "step": 323000
    },
    {
      "epoch": 2764.957264957265,
      "grad_norm": 8.727392196655273,
      "learning_rate": 2.235042735042735e-05,
      "loss": 3.8754,
      "step": 323500
    },
    {
      "epoch": 2769.230769230769,
      "grad_norm": 7.150263786315918,
      "learning_rate": 2.230769230769231e-05,
      "loss": 3.8776,
      "step": 324000
    },
    {
      "epoch": 2773.5042735042734,
      "grad_norm": 6.997973442077637,
      "learning_rate": 2.2264957264957266e-05,
      "loss": 3.8679,
      "step": 324500
    },
    {
      "epoch": 2777.777777777778,
      "grad_norm": 8.844876289367676,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 3.877,
      "step": 325000
    },
    {
      "epoch": 2782.051282051282,
      "grad_norm": 7.267214775085449,
      "learning_rate": 2.217948717948718e-05,
      "loss": 3.8728,
      "step": 325500
    },
    {
      "epoch": 2786.324786324786,
      "grad_norm": 7.612776756286621,
      "learning_rate": 2.2136752136752138e-05,
      "loss": 3.8671,
      "step": 326000
    },
    {
      "epoch": 2790.5982905982905,
      "grad_norm": 7.000133037567139,
      "learning_rate": 2.2094017094017095e-05,
      "loss": 3.8751,
      "step": 326500
    },
    {
      "epoch": 2794.871794871795,
      "grad_norm": 10.89911937713623,
      "learning_rate": 2.2051282051282052e-05,
      "loss": 3.871,
      "step": 327000
    },
    {
      "epoch": 2799.1452991452993,
      "grad_norm": 5.838742256164551,
      "learning_rate": 2.200854700854701e-05,
      "loss": 3.8824,
      "step": 327500
    },
    {
      "epoch": 2803.4188034188032,
      "grad_norm": 11.624866485595703,
      "learning_rate": 2.1965811965811967e-05,
      "loss": 3.8718,
      "step": 328000
    },
    {
      "epoch": 2807.6923076923076,
      "grad_norm": 7.591381072998047,
      "learning_rate": 2.1923076923076924e-05,
      "loss": 3.8765,
      "step": 328500
    },
    {
      "epoch": 2811.965811965812,
      "grad_norm": 6.954352855682373,
      "learning_rate": 2.188034188034188e-05,
      "loss": 3.8661,
      "step": 329000
    },
    {
      "epoch": 2816.2393162393164,
      "grad_norm": 7.548831462860107,
      "learning_rate": 2.1837606837606838e-05,
      "loss": 3.8775,
      "step": 329500
    },
    {
      "epoch": 2820.5128205128203,
      "grad_norm": 5.16595983505249,
      "learning_rate": 2.1794871794871795e-05,
      "loss": 3.8626,
      "step": 330000
    },
    {
      "epoch": 2824.7863247863247,
      "grad_norm": 6.845653533935547,
      "learning_rate": 2.1752136752136753e-05,
      "loss": 3.8657,
      "step": 330500
    },
    {
      "epoch": 2829.059829059829,
      "grad_norm": 10.018425941467285,
      "learning_rate": 2.170940170940171e-05,
      "loss": 3.8679,
      "step": 331000
    },
    {
      "epoch": 2833.3333333333335,
      "grad_norm": 7.0381340980529785,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 3.867,
      "step": 331500
    },
    {
      "epoch": 2837.6068376068374,
      "grad_norm": 12.048552513122559,
      "learning_rate": 2.1623931623931624e-05,
      "loss": 3.8659,
      "step": 332000
    },
    {
      "epoch": 2841.880341880342,
      "grad_norm": 10.038005828857422,
      "learning_rate": 2.1581196581196585e-05,
      "loss": 3.8801,
      "step": 332500
    },
    {
      "epoch": 2846.153846153846,
      "grad_norm": 7.189593315124512,
      "learning_rate": 2.1538461538461542e-05,
      "loss": 3.8783,
      "step": 333000
    },
    {
      "epoch": 2850.4273504273506,
      "grad_norm": 8.330848693847656,
      "learning_rate": 2.14957264957265e-05,
      "loss": 3.8634,
      "step": 333500
    },
    {
      "epoch": 2854.7008547008545,
      "grad_norm": 7.519842624664307,
      "learning_rate": 2.1452991452991453e-05,
      "loss": 3.8798,
      "step": 334000
    },
    {
      "epoch": 2858.974358974359,
      "grad_norm": 11.766231536865234,
      "learning_rate": 2.141025641025641e-05,
      "loss": 3.8685,
      "step": 334500
    },
    {
      "epoch": 2863.2478632478633,
      "grad_norm": 8.836122512817383,
      "learning_rate": 2.1367521367521368e-05,
      "loss": 3.8647,
      "step": 335000
    },
    {
      "epoch": 2867.5213675213677,
      "grad_norm": 12.424052238464355,
      "learning_rate": 2.1324786324786325e-05,
      "loss": 3.8727,
      "step": 335500
    },
    {
      "epoch": 2871.7948717948716,
      "grad_norm": 7.0750274658203125,
      "learning_rate": 2.1282051282051282e-05,
      "loss": 3.8655,
      "step": 336000
    },
    {
      "epoch": 2876.068376068376,
      "grad_norm": 8.215617179870605,
      "learning_rate": 2.123931623931624e-05,
      "loss": 3.876,
      "step": 336500
    },
    {
      "epoch": 2880.3418803418804,
      "grad_norm": 7.95670747756958,
      "learning_rate": 2.1196581196581196e-05,
      "loss": 3.8679,
      "step": 337000
    },
    {
      "epoch": 2884.6153846153848,
      "grad_norm": 10.670045852661133,
      "learning_rate": 2.1153846153846154e-05,
      "loss": 3.8687,
      "step": 337500
    },
    {
      "epoch": 2888.8888888888887,
      "grad_norm": 7.039992332458496,
      "learning_rate": 2.111111111111111e-05,
      "loss": 3.8653,
      "step": 338000
    },
    {
      "epoch": 2893.162393162393,
      "grad_norm": 6.132679462432861,
      "learning_rate": 2.1068376068376068e-05,
      "loss": 3.8701,
      "step": 338500
    },
    {
      "epoch": 2897.4358974358975,
      "grad_norm": 8.23397445678711,
      "learning_rate": 2.102564102564103e-05,
      "loss": 3.8649,
      "step": 339000
    },
    {
      "epoch": 2901.709401709402,
      "grad_norm": 7.1687846183776855,
      "learning_rate": 2.0982905982905986e-05,
      "loss": 3.87,
      "step": 339500
    },
    {
      "epoch": 2905.982905982906,
      "grad_norm": 7.490036487579346,
      "learning_rate": 2.0940170940170943e-05,
      "loss": 3.873,
      "step": 340000
    },
    {
      "epoch": 2910.25641025641,
      "grad_norm": 6.064828395843506,
      "learning_rate": 2.08974358974359e-05,
      "loss": 3.8797,
      "step": 340500
    },
    {
      "epoch": 2914.5299145299145,
      "grad_norm": 7.633486270904541,
      "learning_rate": 2.0854700854700857e-05,
      "loss": 3.8696,
      "step": 341000
    },
    {
      "epoch": 2918.803418803419,
      "grad_norm": 6.9950408935546875,
      "learning_rate": 2.0811965811965815e-05,
      "loss": 3.8775,
      "step": 341500
    },
    {
      "epoch": 2923.076923076923,
      "grad_norm": 8.021245002746582,
      "learning_rate": 2.0769230769230772e-05,
      "loss": 3.8692,
      "step": 342000
    },
    {
      "epoch": 2927.3504273504273,
      "grad_norm": 6.9977498054504395,
      "learning_rate": 2.072649572649573e-05,
      "loss": 3.8667,
      "step": 342500
    },
    {
      "epoch": 2931.6239316239316,
      "grad_norm": 6.724332332611084,
      "learning_rate": 2.0683760683760683e-05,
      "loss": 3.8598,
      "step": 343000
    },
    {
      "epoch": 2935.897435897436,
      "grad_norm": 9.136563301086426,
      "learning_rate": 2.064102564102564e-05,
      "loss": 3.8704,
      "step": 343500
    },
    {
      "epoch": 2940.17094017094,
      "grad_norm": 11.375521659851074,
      "learning_rate": 2.0598290598290597e-05,
      "loss": 3.8763,
      "step": 344000
    },
    {
      "epoch": 2944.4444444444443,
      "grad_norm": 6.338460445404053,
      "learning_rate": 2.0555555555555555e-05,
      "loss": 3.8806,
      "step": 344500
    },
    {
      "epoch": 2948.7179487179487,
      "grad_norm": 6.038307189941406,
      "learning_rate": 2.0512820512820512e-05,
      "loss": 3.863,
      "step": 345000
    },
    {
      "epoch": 2952.991452991453,
      "grad_norm": 8.466547966003418,
      "learning_rate": 2.047008547008547e-05,
      "loss": 3.8726,
      "step": 345500
    },
    {
      "epoch": 2957.264957264957,
      "grad_norm": 5.985409736633301,
      "learning_rate": 2.042735042735043e-05,
      "loss": 3.8728,
      "step": 346000
    },
    {
      "epoch": 2961.5384615384614,
      "grad_norm": 7.66369104385376,
      "learning_rate": 2.0384615384615387e-05,
      "loss": 3.8802,
      "step": 346500
    },
    {
      "epoch": 2965.811965811966,
      "grad_norm": 6.739178657531738,
      "learning_rate": 2.0341880341880344e-05,
      "loss": 3.8692,
      "step": 347000
    },
    {
      "epoch": 2970.08547008547,
      "grad_norm": 9.901533126831055,
      "learning_rate": 2.02991452991453e-05,
      "loss": 3.8789,
      "step": 347500
    },
    {
      "epoch": 2974.358974358974,
      "grad_norm": 6.247977256774902,
      "learning_rate": 2.025641025641026e-05,
      "loss": 3.8683,
      "step": 348000
    },
    {
      "epoch": 2978.6324786324785,
      "grad_norm": 6.75100564956665,
      "learning_rate": 2.0213675213675216e-05,
      "loss": 3.8798,
      "step": 348500
    },
    {
      "epoch": 2982.905982905983,
      "grad_norm": 5.387789726257324,
      "learning_rate": 2.0170940170940173e-05,
      "loss": 3.8696,
      "step": 349000
    },
    {
      "epoch": 2987.1794871794873,
      "grad_norm": 8.474579811096191,
      "learning_rate": 2.012820512820513e-05,
      "loss": 3.8826,
      "step": 349500
    },
    {
      "epoch": 2991.4529914529912,
      "grad_norm": 5.975306987762451,
      "learning_rate": 2.0085470085470087e-05,
      "loss": 3.8458,
      "step": 350000
    },
    {
      "epoch": 2995.7264957264956,
      "grad_norm": 7.026066780090332,
      "learning_rate": 2.0042735042735044e-05,
      "loss": 3.8805,
      "step": 350500
    },
    {
      "epoch": 3000.0,
      "grad_norm": 8.943251609802246,
      "learning_rate": 2e-05,
      "loss": 3.8736,
      "step": 351000
    },
    {
      "epoch": 3004.2735042735044,
      "grad_norm": 11.132708549499512,
      "learning_rate": 1.995726495726496e-05,
      "loss": 3.8729,
      "step": 351500
    },
    {
      "epoch": 3008.5470085470088,
      "grad_norm": 7.874745845794678,
      "learning_rate": 1.9914529914529913e-05,
      "loss": 3.8781,
      "step": 352000
    },
    {
      "epoch": 3012.8205128205127,
      "grad_norm": 8.360215187072754,
      "learning_rate": 1.987179487179487e-05,
      "loss": 3.8814,
      "step": 352500
    },
    {
      "epoch": 3017.094017094017,
      "grad_norm": 10.518081665039062,
      "learning_rate": 1.982905982905983e-05,
      "loss": 3.867,
      "step": 353000
    },
    {
      "epoch": 3021.3675213675215,
      "grad_norm": 11.079313278198242,
      "learning_rate": 1.9786324786324788e-05,
      "loss": 3.8745,
      "step": 353500
    },
    {
      "epoch": 3025.641025641026,
      "grad_norm": 5.832015514373779,
      "learning_rate": 1.9743589743589745e-05,
      "loss": 3.8632,
      "step": 354000
    },
    {
      "epoch": 3029.91452991453,
      "grad_norm": 10.328245162963867,
      "learning_rate": 1.9700854700854702e-05,
      "loss": 3.8668,
      "step": 354500
    },
    {
      "epoch": 3034.188034188034,
      "grad_norm": 6.3832292556762695,
      "learning_rate": 1.965811965811966e-05,
      "loss": 3.8602,
      "step": 355000
    },
    {
      "epoch": 3038.4615384615386,
      "grad_norm": 7.8023529052734375,
      "learning_rate": 1.9615384615384617e-05,
      "loss": 3.8661,
      "step": 355500
    },
    {
      "epoch": 3042.735042735043,
      "grad_norm": 11.295807838439941,
      "learning_rate": 1.9572649572649574e-05,
      "loss": 3.8643,
      "step": 356000
    },
    {
      "epoch": 3047.008547008547,
      "grad_norm": 7.515908241271973,
      "learning_rate": 1.952991452991453e-05,
      "loss": 3.8612,
      "step": 356500
    },
    {
      "epoch": 3051.2820512820513,
      "grad_norm": 6.857945919036865,
      "learning_rate": 1.9487179487179488e-05,
      "loss": 3.8617,
      "step": 357000
    },
    {
      "epoch": 3055.5555555555557,
      "grad_norm": 6.442270278930664,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 3.8722,
      "step": 357500
    },
    {
      "epoch": 3059.82905982906,
      "grad_norm": 6.90511417388916,
      "learning_rate": 1.9401709401709403e-05,
      "loss": 3.8705,
      "step": 358000
    },
    {
      "epoch": 3064.102564102564,
      "grad_norm": 8.012727737426758,
      "learning_rate": 1.935897435897436e-05,
      "loss": 3.8713,
      "step": 358500
    },
    {
      "epoch": 3068.3760683760684,
      "grad_norm": 9.234233856201172,
      "learning_rate": 1.9316239316239317e-05,
      "loss": 3.8758,
      "step": 359000
    },
    {
      "epoch": 3072.6495726495727,
      "grad_norm": 6.2506585121154785,
      "learning_rate": 1.9273504273504274e-05,
      "loss": 3.8631,
      "step": 359500
    },
    {
      "epoch": 3076.923076923077,
      "grad_norm": 7.101428508758545,
      "learning_rate": 1.923076923076923e-05,
      "loss": 3.8708,
      "step": 360000
    },
    {
      "epoch": 3081.196581196581,
      "grad_norm": 10.209336280822754,
      "learning_rate": 1.918803418803419e-05,
      "loss": 3.8684,
      "step": 360500
    },
    {
      "epoch": 3085.4700854700855,
      "grad_norm": 6.2977190017700195,
      "learning_rate": 1.914529914529915e-05,
      "loss": 3.8775,
      "step": 361000
    },
    {
      "epoch": 3089.74358974359,
      "grad_norm": 8.540809631347656,
      "learning_rate": 1.9102564102564103e-05,
      "loss": 3.8726,
      "step": 361500
    },
    {
      "epoch": 3094.017094017094,
      "grad_norm": 9.652348518371582,
      "learning_rate": 1.905982905982906e-05,
      "loss": 3.87,
      "step": 362000
    },
    {
      "epoch": 3098.290598290598,
      "grad_norm": 7.7655863761901855,
      "learning_rate": 1.9017094017094017e-05,
      "loss": 3.8678,
      "step": 362500
    },
    {
      "epoch": 3102.5641025641025,
      "grad_norm": 7.0358686447143555,
      "learning_rate": 1.8974358974358975e-05,
      "loss": 3.8694,
      "step": 363000
    },
    {
      "epoch": 3106.837606837607,
      "grad_norm": 6.153120517730713,
      "learning_rate": 1.8931623931623932e-05,
      "loss": 3.8699,
      "step": 363500
    },
    {
      "epoch": 3111.1111111111113,
      "grad_norm": 6.72052526473999,
      "learning_rate": 1.888888888888889e-05,
      "loss": 3.8735,
      "step": 364000
    },
    {
      "epoch": 3115.3846153846152,
      "grad_norm": 7.569969177246094,
      "learning_rate": 1.8846153846153846e-05,
      "loss": 3.8778,
      "step": 364500
    },
    {
      "epoch": 3119.6581196581196,
      "grad_norm": 8.088789939880371,
      "learning_rate": 1.8803418803418804e-05,
      "loss": 3.8698,
      "step": 365000
    },
    {
      "epoch": 3123.931623931624,
      "grad_norm": 9.427404403686523,
      "learning_rate": 1.876068376068376e-05,
      "loss": 3.8668,
      "step": 365500
    },
    {
      "epoch": 3128.2051282051284,
      "grad_norm": 7.383189678192139,
      "learning_rate": 1.8717948717948718e-05,
      "loss": 3.8727,
      "step": 366000
    },
    {
      "epoch": 3132.4786324786323,
      "grad_norm": 6.968044757843018,
      "learning_rate": 1.8675213675213675e-05,
      "loss": 3.8643,
      "step": 366500
    },
    {
      "epoch": 3136.7521367521367,
      "grad_norm": 14.980299949645996,
      "learning_rate": 1.8632478632478632e-05,
      "loss": 3.8715,
      "step": 367000
    },
    {
      "epoch": 3141.025641025641,
      "grad_norm": 5.23820686340332,
      "learning_rate": 1.858974358974359e-05,
      "loss": 3.8688,
      "step": 367500
    },
    {
      "epoch": 3145.2991452991455,
      "grad_norm": 8.929092407226562,
      "learning_rate": 1.854700854700855e-05,
      "loss": 3.8724,
      "step": 368000
    },
    {
      "epoch": 3149.5726495726494,
      "grad_norm": 7.940183162689209,
      "learning_rate": 1.8504273504273507e-05,
      "loss": 3.8646,
      "step": 368500
    },
    {
      "epoch": 3153.846153846154,
      "grad_norm": 5.8079023361206055,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 3.8613,
      "step": 369000
    },
    {
      "epoch": 3158.119658119658,
      "grad_norm": 7.969864368438721,
      "learning_rate": 1.8418803418803422e-05,
      "loss": 3.8786,
      "step": 369500
    },
    {
      "epoch": 3162.3931623931626,
      "grad_norm": 10.086294174194336,
      "learning_rate": 1.837606837606838e-05,
      "loss": 3.8735,
      "step": 370000
    },
    {
      "epoch": 3166.6666666666665,
      "grad_norm": 10.156953811645508,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 3.8704,
      "step": 370500
    },
    {
      "epoch": 3170.940170940171,
      "grad_norm": 7.269861221313477,
      "learning_rate": 1.829059829059829e-05,
      "loss": 3.8612,
      "step": 371000
    },
    {
      "epoch": 3175.2136752136753,
      "grad_norm": 7.867919445037842,
      "learning_rate": 1.8247863247863247e-05,
      "loss": 3.8761,
      "step": 371500
    },
    {
      "epoch": 3179.4871794871797,
      "grad_norm": 9.49611759185791,
      "learning_rate": 1.8205128205128204e-05,
      "loss": 3.8631,
      "step": 372000
    },
    {
      "epoch": 3183.7606837606836,
      "grad_norm": 9.587148666381836,
      "learning_rate": 1.8162393162393162e-05,
      "loss": 3.8745,
      "step": 372500
    },
    {
      "epoch": 3188.034188034188,
      "grad_norm": 7.04217004776001,
      "learning_rate": 1.811965811965812e-05,
      "loss": 3.8736,
      "step": 373000
    },
    {
      "epoch": 3192.3076923076924,
      "grad_norm": 7.78208065032959,
      "learning_rate": 1.8076923076923076e-05,
      "loss": 3.8694,
      "step": 373500
    },
    {
      "epoch": 3196.5811965811968,
      "grad_norm": 7.143069267272949,
      "learning_rate": 1.8034188034188033e-05,
      "loss": 3.8607,
      "step": 374000
    },
    {
      "epoch": 3200.8547008547007,
      "grad_norm": 6.514270305633545,
      "learning_rate": 1.799145299145299e-05,
      "loss": 3.8626,
      "step": 374500
    },
    {
      "epoch": 3205.128205128205,
      "grad_norm": 7.632586479187012,
      "learning_rate": 1.794871794871795e-05,
      "loss": 3.8562,
      "step": 375000
    },
    {
      "epoch": 3209.4017094017095,
      "grad_norm": 5.775819778442383,
      "learning_rate": 1.790598290598291e-05,
      "loss": 3.8791,
      "step": 375500
    },
    {
      "epoch": 3213.675213675214,
      "grad_norm": 9.185027122497559,
      "learning_rate": 1.7863247863247866e-05,
      "loss": 3.8615,
      "step": 376000
    },
    {
      "epoch": 3217.948717948718,
      "grad_norm": 6.3039984703063965,
      "learning_rate": 1.7820512820512823e-05,
      "loss": 3.8631,
      "step": 376500
    },
    {
      "epoch": 3222.222222222222,
      "grad_norm": 7.982367038726807,
      "learning_rate": 1.777777777777778e-05,
      "loss": 3.8752,
      "step": 377000
    },
    {
      "epoch": 3226.4957264957266,
      "grad_norm": 7.696084499359131,
      "learning_rate": 1.7735042735042737e-05,
      "loss": 3.8634,
      "step": 377500
    },
    {
      "epoch": 3230.769230769231,
      "grad_norm": 6.863245010375977,
      "learning_rate": 1.7692307692307694e-05,
      "loss": 3.8815,
      "step": 378000
    },
    {
      "epoch": 3235.042735042735,
      "grad_norm": 8.622881889343262,
      "learning_rate": 1.764957264957265e-05,
      "loss": 3.8693,
      "step": 378500
    },
    {
      "epoch": 3239.3162393162393,
      "grad_norm": 6.98700475692749,
      "learning_rate": 1.760683760683761e-05,
      "loss": 3.8726,
      "step": 379000
    },
    {
      "epoch": 3243.5897435897436,
      "grad_norm": 10.256736755371094,
      "learning_rate": 1.7564102564102563e-05,
      "loss": 3.8579,
      "step": 379500
    },
    {
      "epoch": 3247.863247863248,
      "grad_norm": 10.689510345458984,
      "learning_rate": 1.752136752136752e-05,
      "loss": 3.8648,
      "step": 380000
    },
    {
      "epoch": 3252.136752136752,
      "grad_norm": 6.53915548324585,
      "learning_rate": 1.7478632478632477e-05,
      "loss": 3.8646,
      "step": 380500
    },
    {
      "epoch": 3256.4102564102564,
      "grad_norm": 6.501630783081055,
      "learning_rate": 1.7435897435897434e-05,
      "loss": 3.8633,
      "step": 381000
    },
    {
      "epoch": 3260.6837606837607,
      "grad_norm": 5.569904804229736,
      "learning_rate": 1.7393162393162395e-05,
      "loss": 3.8642,
      "step": 381500
    },
    {
      "epoch": 3264.957264957265,
      "grad_norm": 7.418942928314209,
      "learning_rate": 1.7350427350427352e-05,
      "loss": 3.8779,
      "step": 382000
    },
    {
      "epoch": 3269.230769230769,
      "grad_norm": 6.6355061531066895,
      "learning_rate": 1.730769230769231e-05,
      "loss": 3.8765,
      "step": 382500
    },
    {
      "epoch": 3273.5042735042734,
      "grad_norm": 6.118690013885498,
      "learning_rate": 1.7264957264957267e-05,
      "loss": 3.8638,
      "step": 383000
    },
    {
      "epoch": 3277.777777777778,
      "grad_norm": 7.466014862060547,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 3.8639,
      "step": 383500
    },
    {
      "epoch": 3282.051282051282,
      "grad_norm": 7.938340663909912,
      "learning_rate": 1.717948717948718e-05,
      "loss": 3.8578,
      "step": 384000
    },
    {
      "epoch": 3286.324786324786,
      "grad_norm": 8.961493492126465,
      "learning_rate": 1.7136752136752138e-05,
      "loss": 3.8653,
      "step": 384500
    },
    {
      "epoch": 3290.5982905982905,
      "grad_norm": 17.727535247802734,
      "learning_rate": 1.7094017094017095e-05,
      "loss": 3.8626,
      "step": 385000
    },
    {
      "epoch": 3294.871794871795,
      "grad_norm": 8.064998626708984,
      "learning_rate": 1.7051282051282053e-05,
      "loss": 3.8681,
      "step": 385500
    },
    {
      "epoch": 3299.1452991452993,
      "grad_norm": 7.455202102661133,
      "learning_rate": 1.700854700854701e-05,
      "loss": 3.8657,
      "step": 386000
    },
    {
      "epoch": 3303.4188034188032,
      "grad_norm": 7.376513481140137,
      "learning_rate": 1.6965811965811967e-05,
      "loss": 3.8603,
      "step": 386500
    },
    {
      "epoch": 3307.6923076923076,
      "grad_norm": 7.200796127319336,
      "learning_rate": 1.6923076923076924e-05,
      "loss": 3.8676,
      "step": 387000
    },
    {
      "epoch": 3311.965811965812,
      "grad_norm": 13.86443042755127,
      "learning_rate": 1.688034188034188e-05,
      "loss": 3.8602,
      "step": 387500
    },
    {
      "epoch": 3316.2393162393164,
      "grad_norm": 6.73759651184082,
      "learning_rate": 1.683760683760684e-05,
      "loss": 3.8659,
      "step": 388000
    },
    {
      "epoch": 3320.5128205128203,
      "grad_norm": 7.248754501342773,
      "learning_rate": 1.6794871794871796e-05,
      "loss": 3.8615,
      "step": 388500
    },
    {
      "epoch": 3324.7863247863247,
      "grad_norm": 9.257972717285156,
      "learning_rate": 1.6752136752136753e-05,
      "loss": 3.8661,
      "step": 389000
    },
    {
      "epoch": 3329.059829059829,
      "grad_norm": 13.3080472946167,
      "learning_rate": 1.670940170940171e-05,
      "loss": 3.8651,
      "step": 389500
    },
    {
      "epoch": 3333.3333333333335,
      "grad_norm": 7.974774360656738,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 3.8721,
      "step": 390000
    },
    {
      "epoch": 3337.6068376068374,
      "grad_norm": 9.749917030334473,
      "learning_rate": 1.6623931623931625e-05,
      "loss": 3.8538,
      "step": 390500
    },
    {
      "epoch": 3341.880341880342,
      "grad_norm": 7.605290412902832,
      "learning_rate": 1.6581196581196582e-05,
      "loss": 3.8706,
      "step": 391000
    },
    {
      "epoch": 3346.153846153846,
      "grad_norm": 6.9299750328063965,
      "learning_rate": 1.653846153846154e-05,
      "loss": 3.8597,
      "step": 391500
    },
    {
      "epoch": 3350.4273504273506,
      "grad_norm": 8.09888744354248,
      "learning_rate": 1.6495726495726496e-05,
      "loss": 3.8657,
      "step": 392000
    },
    {
      "epoch": 3354.7008547008545,
      "grad_norm": 5.6683807373046875,
      "learning_rate": 1.6452991452991454e-05,
      "loss": 3.8554,
      "step": 392500
    },
    {
      "epoch": 3358.974358974359,
      "grad_norm": 9.559353828430176,
      "learning_rate": 1.641025641025641e-05,
      "loss": 3.876,
      "step": 393000
    },
    {
      "epoch": 3363.2478632478633,
      "grad_norm": 9.2791109085083,
      "learning_rate": 1.6367521367521368e-05,
      "loss": 3.8725,
      "step": 393500
    },
    {
      "epoch": 3367.5213675213677,
      "grad_norm": 11.171344757080078,
      "learning_rate": 1.6324786324786325e-05,
      "loss": 3.8659,
      "step": 394000
    },
    {
      "epoch": 3371.7948717948716,
      "grad_norm": 7.680442810058594,
      "learning_rate": 1.6282051282051282e-05,
      "loss": 3.8652,
      "step": 394500
    },
    {
      "epoch": 3376.068376068376,
      "grad_norm": 11.43649959564209,
      "learning_rate": 1.623931623931624e-05,
      "loss": 3.8613,
      "step": 395000
    },
    {
      "epoch": 3380.3418803418804,
      "grad_norm": 7.203107833862305,
      "learning_rate": 1.6196581196581197e-05,
      "loss": 3.8606,
      "step": 395500
    },
    {
      "epoch": 3384.6153846153848,
      "grad_norm": 9.680456161499023,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 3.8704,
      "step": 396000
    },
    {
      "epoch": 3388.8888888888887,
      "grad_norm": 6.6490373611450195,
      "learning_rate": 1.6111111111111115e-05,
      "loss": 3.8565,
      "step": 396500
    },
    {
      "epoch": 3393.162393162393,
      "grad_norm": 10.508036613464355,
      "learning_rate": 1.6068376068376072e-05,
      "loss": 3.8658,
      "step": 397000
    },
    {
      "epoch": 3397.4358974358975,
      "grad_norm": 9.170965194702148,
      "learning_rate": 1.602564102564103e-05,
      "loss": 3.8665,
      "step": 397500
    },
    {
      "epoch": 3401.709401709402,
      "grad_norm": 7.807075500488281,
      "learning_rate": 1.5982905982905983e-05,
      "loss": 3.8661,
      "step": 398000
    },
    {
      "epoch": 3405.982905982906,
      "grad_norm": 7.249600410461426,
      "learning_rate": 1.594017094017094e-05,
      "loss": 3.8642,
      "step": 398500
    },
    {
      "epoch": 3410.25641025641,
      "grad_norm": 6.447136878967285,
      "learning_rate": 1.5897435897435897e-05,
      "loss": 3.8684,
      "step": 399000
    },
    {
      "epoch": 3414.5299145299145,
      "grad_norm": 6.844610691070557,
      "learning_rate": 1.5854700854700854e-05,
      "loss": 3.8638,
      "step": 399500
    },
    {
      "epoch": 3418.803418803419,
      "grad_norm": 9.577085494995117,
      "learning_rate": 1.581196581196581e-05,
      "loss": 3.8614,
      "step": 400000
    },
    {
      "epoch": 3423.076923076923,
      "grad_norm": 6.915129661560059,
      "learning_rate": 1.576923076923077e-05,
      "loss": 3.8639,
      "step": 400500
    },
    {
      "epoch": 3427.3504273504273,
      "grad_norm": 8.723685264587402,
      "learning_rate": 1.5726495726495726e-05,
      "loss": 3.8606,
      "step": 401000
    },
    {
      "epoch": 3431.6239316239316,
      "grad_norm": 13.184491157531738,
      "learning_rate": 1.5683760683760683e-05,
      "loss": 3.8739,
      "step": 401500
    },
    {
      "epoch": 3435.897435897436,
      "grad_norm": 6.246737003326416,
      "learning_rate": 1.564102564102564e-05,
      "loss": 3.8659,
      "step": 402000
    },
    {
      "epoch": 3440.17094017094,
      "grad_norm": 6.390819072723389,
      "learning_rate": 1.5598290598290598e-05,
      "loss": 3.8603,
      "step": 402500
    },
    {
      "epoch": 3444.4444444444443,
      "grad_norm": 6.20907735824585,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 3.8651,
      "step": 403000
    },
    {
      "epoch": 3448.7179487179487,
      "grad_norm": 8.92228889465332,
      "learning_rate": 1.5512820512820516e-05,
      "loss": 3.8653,
      "step": 403500
    },
    {
      "epoch": 3452.991452991453,
      "grad_norm": 6.848456859588623,
      "learning_rate": 1.5470085470085473e-05,
      "loss": 3.8607,
      "step": 404000
    },
    {
      "epoch": 3457.264957264957,
      "grad_norm": 10.133363723754883,
      "learning_rate": 1.542735042735043e-05,
      "loss": 3.8612,
      "step": 404500
    },
    {
      "epoch": 3461.5384615384614,
      "grad_norm": 5.823537349700928,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 3.8629,
      "step": 405000
    },
    {
      "epoch": 3465.811965811966,
      "grad_norm": 5.375941276550293,
      "learning_rate": 1.5341880341880344e-05,
      "loss": 3.8668,
      "step": 405500
    },
    {
      "epoch": 3470.08547008547,
      "grad_norm": 6.111418724060059,
      "learning_rate": 1.52991452991453e-05,
      "loss": 3.8602,
      "step": 406000
    },
    {
      "epoch": 3474.358974358974,
      "grad_norm": 6.417657852172852,
      "learning_rate": 1.5256410256410259e-05,
      "loss": 3.87,
      "step": 406500
    },
    {
      "epoch": 3478.6324786324785,
      "grad_norm": 6.917044639587402,
      "learning_rate": 1.5213675213675213e-05,
      "loss": 3.8627,
      "step": 407000
    },
    {
      "epoch": 3482.905982905983,
      "grad_norm": 6.846547603607178,
      "learning_rate": 1.517094017094017e-05,
      "loss": 3.8723,
      "step": 407500
    },
    {
      "epoch": 3487.1794871794873,
      "grad_norm": 8.552536010742188,
      "learning_rate": 1.5128205128205129e-05,
      "loss": 3.8563,
      "step": 408000
    },
    {
      "epoch": 3491.4529914529912,
      "grad_norm": 8.614087104797363,
      "learning_rate": 1.5085470085470086e-05,
      "loss": 3.8598,
      "step": 408500
    },
    {
      "epoch": 3495.7264957264956,
      "grad_norm": 6.164653301239014,
      "learning_rate": 1.5042735042735043e-05,
      "loss": 3.8548,
      "step": 409000
    },
    {
      "epoch": 3500.0,
      "grad_norm": 11.84445571899414,
      "learning_rate": 1.5e-05,
      "loss": 3.8697,
      "step": 409500
    },
    {
      "epoch": 3504.2735042735044,
      "grad_norm": 9.877301216125488,
      "learning_rate": 1.4957264957264958e-05,
      "loss": 3.8731,
      "step": 410000
    },
    {
      "epoch": 3508.5470085470088,
      "grad_norm": 7.319911003112793,
      "learning_rate": 1.4914529914529915e-05,
      "loss": 3.8546,
      "step": 410500
    },
    {
      "epoch": 3512.8205128205127,
      "grad_norm": 8.569754600524902,
      "learning_rate": 1.4871794871794872e-05,
      "loss": 3.8628,
      "step": 411000
    },
    {
      "epoch": 3517.094017094017,
      "grad_norm": 6.571986675262451,
      "learning_rate": 1.482905982905983e-05,
      "loss": 3.868,
      "step": 411500
    },
    {
      "epoch": 3521.3675213675215,
      "grad_norm": 8.118894577026367,
      "learning_rate": 1.4786324786324786e-05,
      "loss": 3.8615,
      "step": 412000
    },
    {
      "epoch": 3525.641025641026,
      "grad_norm": 5.838194370269775,
      "learning_rate": 1.4743589743589745e-05,
      "loss": 3.8483,
      "step": 412500
    },
    {
      "epoch": 3529.91452991453,
      "grad_norm": 6.898101329803467,
      "learning_rate": 1.4700854700854703e-05,
      "loss": 3.8714,
      "step": 413000
    },
    {
      "epoch": 3534.188034188034,
      "grad_norm": 7.755508899688721,
      "learning_rate": 1.465811965811966e-05,
      "loss": 3.8562,
      "step": 413500
    },
    {
      "epoch": 3538.4615384615386,
      "grad_norm": 8.039238929748535,
      "learning_rate": 1.4615384615384617e-05,
      "loss": 3.8696,
      "step": 414000
    },
    {
      "epoch": 3542.735042735043,
      "grad_norm": 7.497688293457031,
      "learning_rate": 1.4572649572649574e-05,
      "loss": 3.8667,
      "step": 414500
    },
    {
      "epoch": 3547.008547008547,
      "grad_norm": 7.613471984863281,
      "learning_rate": 1.4529914529914531e-05,
      "loss": 3.8565,
      "step": 415000
    },
    {
      "epoch": 3551.2820512820513,
      "grad_norm": 6.818145751953125,
      "learning_rate": 1.4487179487179489e-05,
      "loss": 3.864,
      "step": 415500
    },
    {
      "epoch": 3555.5555555555557,
      "grad_norm": 7.985652446746826,
      "learning_rate": 1.4444444444444444e-05,
      "loss": 3.8574,
      "step": 416000
    },
    {
      "epoch": 3559.82905982906,
      "grad_norm": 9.932462692260742,
      "learning_rate": 1.4401709401709401e-05,
      "loss": 3.8632,
      "step": 416500
    },
    {
      "epoch": 3564.102564102564,
      "grad_norm": 4.834048271179199,
      "learning_rate": 1.4358974358974359e-05,
      "loss": 3.8607,
      "step": 417000
    },
    {
      "epoch": 3568.3760683760684,
      "grad_norm": 8.093829154968262,
      "learning_rate": 1.4316239316239316e-05,
      "loss": 3.8628,
      "step": 417500
    },
    {
      "epoch": 3572.6495726495727,
      "grad_norm": 7.931203365325928,
      "learning_rate": 1.4273504273504273e-05,
      "loss": 3.8687,
      "step": 418000
    },
    {
      "epoch": 3576.923076923077,
      "grad_norm": 6.73276424407959,
      "learning_rate": 1.423076923076923e-05,
      "loss": 3.8607,
      "step": 418500
    },
    {
      "epoch": 3581.196581196581,
      "grad_norm": 12.175539016723633,
      "learning_rate": 1.4188034188034189e-05,
      "loss": 3.8718,
      "step": 419000
    },
    {
      "epoch": 3585.4700854700855,
      "grad_norm": 6.608311653137207,
      "learning_rate": 1.4145299145299146e-05,
      "loss": 3.8643,
      "step": 419500
    },
    {
      "epoch": 3589.74358974359,
      "grad_norm": 12.038921356201172,
      "learning_rate": 1.4102564102564104e-05,
      "loss": 3.8639,
      "step": 420000
    },
    {
      "epoch": 3594.017094017094,
      "grad_norm": 6.758659362792969,
      "learning_rate": 1.405982905982906e-05,
      "loss": 3.8605,
      "step": 420500
    },
    {
      "epoch": 3598.290598290598,
      "grad_norm": 7.044413089752197,
      "learning_rate": 1.4017094017094018e-05,
      "loss": 3.8535,
      "step": 421000
    },
    {
      "epoch": 3602.5641025641025,
      "grad_norm": 9.597978591918945,
      "learning_rate": 1.3974358974358975e-05,
      "loss": 3.8674,
      "step": 421500
    },
    {
      "epoch": 3606.837606837607,
      "grad_norm": 9.930685043334961,
      "learning_rate": 1.3931623931623932e-05,
      "loss": 3.8713,
      "step": 422000
    },
    {
      "epoch": 3611.1111111111113,
      "grad_norm": 7.009864807128906,
      "learning_rate": 1.388888888888889e-05,
      "loss": 3.866,
      "step": 422500
    },
    {
      "epoch": 3615.3846153846152,
      "grad_norm": 8.33596134185791,
      "learning_rate": 1.3846153846153847e-05,
      "loss": 3.8625,
      "step": 423000
    },
    {
      "epoch": 3619.6581196581196,
      "grad_norm": 8.172148704528809,
      "learning_rate": 1.3803418803418806e-05,
      "loss": 3.8603,
      "step": 423500
    },
    {
      "epoch": 3623.931623931624,
      "grad_norm": 8.006607055664062,
      "learning_rate": 1.3760683760683763e-05,
      "loss": 3.8594,
      "step": 424000
    },
    {
      "epoch": 3628.2051282051284,
      "grad_norm": 7.810396194458008,
      "learning_rate": 1.371794871794872e-05,
      "loss": 3.8669,
      "step": 424500
    },
    {
      "epoch": 3632.4786324786323,
      "grad_norm": 9.904202461242676,
      "learning_rate": 1.3675213675213677e-05,
      "loss": 3.868,
      "step": 425000
    },
    {
      "epoch": 3636.7521367521367,
      "grad_norm": 8.599382400512695,
      "learning_rate": 1.3632478632478631e-05,
      "loss": 3.87,
      "step": 425500
    },
    {
      "epoch": 3641.025641025641,
      "grad_norm": 8.452521324157715,
      "learning_rate": 1.358974358974359e-05,
      "loss": 3.8627,
      "step": 426000
    },
    {
      "epoch": 3645.2991452991455,
      "grad_norm": 10.7506685256958,
      "learning_rate": 1.3547008547008547e-05,
      "loss": 3.8537,
      "step": 426500
    },
    {
      "epoch": 3649.5726495726494,
      "grad_norm": 6.593174457550049,
      "learning_rate": 1.3504273504273504e-05,
      "loss": 3.856,
      "step": 427000
    },
    {
      "epoch": 3653.846153846154,
      "grad_norm": 13.471181869506836,
      "learning_rate": 1.3461538461538462e-05,
      "loss": 3.8648,
      "step": 427500
    },
    {
      "epoch": 3658.119658119658,
      "grad_norm": 6.359681129455566,
      "learning_rate": 1.3418803418803419e-05,
      "loss": 3.8651,
      "step": 428000
    },
    {
      "epoch": 3662.3931623931626,
      "grad_norm": 9.067720413208008,
      "learning_rate": 1.3376068376068376e-05,
      "loss": 3.8542,
      "step": 428500
    },
    {
      "epoch": 3666.6666666666665,
      "grad_norm": 7.670444011688232,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 3.8679,
      "step": 429000
    },
    {
      "epoch": 3670.940170940171,
      "grad_norm": 9.712586402893066,
      "learning_rate": 1.329059829059829e-05,
      "loss": 3.8636,
      "step": 429500
    },
    {
      "epoch": 3675.2136752136753,
      "grad_norm": 7.387716770172119,
      "learning_rate": 1.324786324786325e-05,
      "loss": 3.8752,
      "step": 430000
    },
    {
      "epoch": 3679.4871794871797,
      "grad_norm": 6.643683910369873,
      "learning_rate": 1.3205128205128207e-05,
      "loss": 3.8608,
      "step": 430500
    },
    {
      "epoch": 3683.7606837606836,
      "grad_norm": 8.728196144104004,
      "learning_rate": 1.3162393162393164e-05,
      "loss": 3.8601,
      "step": 431000
    },
    {
      "epoch": 3688.034188034188,
      "grad_norm": 11.178653717041016,
      "learning_rate": 1.3119658119658121e-05,
      "loss": 3.8611,
      "step": 431500
    },
    {
      "epoch": 3692.3076923076924,
      "grad_norm": 9.084945678710938,
      "learning_rate": 1.3076923076923078e-05,
      "loss": 3.8665,
      "step": 432000
    },
    {
      "epoch": 3696.5811965811968,
      "grad_norm": 9.700885772705078,
      "learning_rate": 1.3034188034188035e-05,
      "loss": 3.8609,
      "step": 432500
    },
    {
      "epoch": 3700.8547008547007,
      "grad_norm": 8.8931303024292,
      "learning_rate": 1.2991452991452993e-05,
      "loss": 3.8583,
      "step": 433000
    },
    {
      "epoch": 3705.128205128205,
      "grad_norm": 6.031345367431641,
      "learning_rate": 1.294871794871795e-05,
      "loss": 3.8689,
      "step": 433500
    },
    {
      "epoch": 3709.4017094017095,
      "grad_norm": 5.9713006019592285,
      "learning_rate": 1.2905982905982907e-05,
      "loss": 3.8584,
      "step": 434000
    },
    {
      "epoch": 3713.675213675214,
      "grad_norm": 9.910594940185547,
      "learning_rate": 1.2863247863247863e-05,
      "loss": 3.8565,
      "step": 434500
    },
    {
      "epoch": 3717.948717948718,
      "grad_norm": 6.259790420532227,
      "learning_rate": 1.282051282051282e-05,
      "loss": 3.8603,
      "step": 435000
    },
    {
      "epoch": 3722.222222222222,
      "grad_norm": 8.426040649414062,
      "learning_rate": 1.2777777777777777e-05,
      "loss": 3.8557,
      "step": 435500
    },
    {
      "epoch": 3726.4957264957266,
      "grad_norm": 7.027787685394287,
      "learning_rate": 1.2735042735042734e-05,
      "loss": 3.8673,
      "step": 436000
    },
    {
      "epoch": 3730.769230769231,
      "grad_norm": 9.697381019592285,
      "learning_rate": 1.2692307692307691e-05,
      "loss": 3.8526,
      "step": 436500
    },
    {
      "epoch": 3735.042735042735,
      "grad_norm": 6.5858235359191895,
      "learning_rate": 1.264957264957265e-05,
      "loss": 3.8664,
      "step": 437000
    },
    {
      "epoch": 3739.3162393162393,
      "grad_norm": 6.71769380569458,
      "learning_rate": 1.2606837606837608e-05,
      "loss": 3.859,
      "step": 437500
    },
    {
      "epoch": 3743.5897435897436,
      "grad_norm": 11.518609046936035,
      "learning_rate": 1.2564102564102565e-05,
      "loss": 3.8609,
      "step": 438000
    },
    {
      "epoch": 3747.863247863248,
      "grad_norm": 6.430694580078125,
      "learning_rate": 1.2521367521367522e-05,
      "loss": 3.8664,
      "step": 438500
    },
    {
      "epoch": 3752.136752136752,
      "grad_norm": 10.299846649169922,
      "learning_rate": 1.247863247863248e-05,
      "loss": 3.8645,
      "step": 439000
    },
    {
      "epoch": 3756.4102564102564,
      "grad_norm": 11.013274192810059,
      "learning_rate": 1.2435897435897436e-05,
      "loss": 3.8703,
      "step": 439500
    },
    {
      "epoch": 3760.6837606837607,
      "grad_norm": 7.721687316894531,
      "learning_rate": 1.2393162393162394e-05,
      "loss": 3.8661,
      "step": 440000
    },
    {
      "epoch": 3764.957264957265,
      "grad_norm": 6.205790996551514,
      "learning_rate": 1.235042735042735e-05,
      "loss": 3.863,
      "step": 440500
    },
    {
      "epoch": 3769.230769230769,
      "grad_norm": 8.43713092803955,
      "learning_rate": 1.230769230769231e-05,
      "loss": 3.8607,
      "step": 441000
    },
    {
      "epoch": 3773.5042735042734,
      "grad_norm": 10.424081802368164,
      "learning_rate": 1.2264957264957265e-05,
      "loss": 3.8653,
      "step": 441500
    },
    {
      "epoch": 3777.777777777778,
      "grad_norm": 6.89390754699707,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 3.8644,
      "step": 442000
    },
    {
      "epoch": 3782.051282051282,
      "grad_norm": 6.028408527374268,
      "learning_rate": 1.217948717948718e-05,
      "loss": 3.8636,
      "step": 442500
    },
    {
      "epoch": 3786.324786324786,
      "grad_norm": 9.673084259033203,
      "learning_rate": 1.2136752136752137e-05,
      "loss": 3.8691,
      "step": 443000
    },
    {
      "epoch": 3790.5982905982905,
      "grad_norm": 9.217926025390625,
      "learning_rate": 1.2094017094017094e-05,
      "loss": 3.8549,
      "step": 443500
    },
    {
      "epoch": 3794.871794871795,
      "grad_norm": 6.7104105949401855,
      "learning_rate": 1.2051282051282051e-05,
      "loss": 3.8652,
      "step": 444000
    },
    {
      "epoch": 3799.1452991452993,
      "grad_norm": 7.432149887084961,
      "learning_rate": 1.200854700854701e-05,
      "loss": 3.8518,
      "step": 444500
    },
    {
      "epoch": 3803.4188034188032,
      "grad_norm": 6.652068138122559,
      "learning_rate": 1.1965811965811967e-05,
      "loss": 3.8645,
      "step": 445000
    },
    {
      "epoch": 3807.6923076923076,
      "grad_norm": 5.536707401275635,
      "learning_rate": 1.1923076923076925e-05,
      "loss": 3.8557,
      "step": 445500
    },
    {
      "epoch": 3811.965811965812,
      "grad_norm": 5.487752914428711,
      "learning_rate": 1.188034188034188e-05,
      "loss": 3.8614,
      "step": 446000
    },
    {
      "epoch": 3816.2393162393164,
      "grad_norm": 9.972230911254883,
      "learning_rate": 1.1837606837606837e-05,
      "loss": 3.8609,
      "step": 446500
    },
    {
      "epoch": 3820.5128205128203,
      "grad_norm": 6.007218360900879,
      "learning_rate": 1.1794871794871795e-05,
      "loss": 3.8528,
      "step": 447000
    },
    {
      "epoch": 3824.7863247863247,
      "grad_norm": 9.819074630737305,
      "learning_rate": 1.1752136752136752e-05,
      "loss": 3.8632,
      "step": 447500
    },
    {
      "epoch": 3829.059829059829,
      "grad_norm": 10.557877540588379,
      "learning_rate": 1.170940170940171e-05,
      "loss": 3.8533,
      "step": 448000
    },
    {
      "epoch": 3833.3333333333335,
      "grad_norm": 9.312655448913574,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 3.8559,
      "step": 448500
    },
    {
      "epoch": 3837.6068376068374,
      "grad_norm": 7.1868462562561035,
      "learning_rate": 1.1623931623931625e-05,
      "loss": 3.8574,
      "step": 449000
    },
    {
      "epoch": 3841.880341880342,
      "grad_norm": 10.072559356689453,
      "learning_rate": 1.1581196581196582e-05,
      "loss": 3.8681,
      "step": 449500
    },
    {
      "epoch": 3846.153846153846,
      "grad_norm": 6.741735458374023,
      "learning_rate": 1.153846153846154e-05,
      "loss": 3.8584,
      "step": 450000
    },
    {
      "epoch": 3850.4273504273506,
      "grad_norm": 7.32432746887207,
      "learning_rate": 1.1495726495726495e-05,
      "loss": 3.8612,
      "step": 450500
    },
    {
      "epoch": 3854.7008547008545,
      "grad_norm": 7.601332187652588,
      "learning_rate": 1.1452991452991452e-05,
      "loss": 3.8606,
      "step": 451000
    },
    {
      "epoch": 3858.974358974359,
      "grad_norm": 7.182327747344971,
      "learning_rate": 1.1410256410256411e-05,
      "loss": 3.8581,
      "step": 451500
    },
    {
      "epoch": 3863.2478632478633,
      "grad_norm": 7.646780490875244,
      "learning_rate": 1.1367521367521368e-05,
      "loss": 3.8605,
      "step": 452000
    },
    {
      "epoch": 3867.5213675213677,
      "grad_norm": 6.79727840423584,
      "learning_rate": 1.1324786324786326e-05,
      "loss": 3.8639,
      "step": 452500
    },
    {
      "epoch": 3871.7948717948716,
      "grad_norm": 8.277552604675293,
      "learning_rate": 1.1282051282051283e-05,
      "loss": 3.8705,
      "step": 453000
    },
    {
      "epoch": 3876.068376068376,
      "grad_norm": 10.057058334350586,
      "learning_rate": 1.123931623931624e-05,
      "loss": 3.8634,
      "step": 453500
    },
    {
      "epoch": 3880.3418803418804,
      "grad_norm": 7.308511734008789,
      "learning_rate": 1.1196581196581197e-05,
      "loss": 3.8596,
      "step": 454000
    },
    {
      "epoch": 3884.6153846153848,
      "grad_norm": 8.453849792480469,
      "learning_rate": 1.1153846153846154e-05,
      "loss": 3.8653,
      "step": 454500
    },
    {
      "epoch": 3888.8888888888887,
      "grad_norm": 8.317529678344727,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 3.8632,
      "step": 455000
    },
    {
      "epoch": 3893.162393162393,
      "grad_norm": 6.7815937995910645,
      "learning_rate": 1.1068376068376069e-05,
      "loss": 3.8657,
      "step": 455500
    },
    {
      "epoch": 3897.4358974358975,
      "grad_norm": 9.045042037963867,
      "learning_rate": 1.1025641025641026e-05,
      "loss": 3.8596,
      "step": 456000
    },
    {
      "epoch": 3901.709401709402,
      "grad_norm": 8.795246124267578,
      "learning_rate": 1.0982905982905983e-05,
      "loss": 3.8508,
      "step": 456500
    },
    {
      "epoch": 3905.982905982906,
      "grad_norm": 8.00238037109375,
      "learning_rate": 1.094017094017094e-05,
      "loss": 3.8649,
      "step": 457000
    },
    {
      "epoch": 3910.25641025641,
      "grad_norm": 9.453766822814941,
      "learning_rate": 1.0897435897435898e-05,
      "loss": 3.8535,
      "step": 457500
    },
    {
      "epoch": 3914.5299145299145,
      "grad_norm": 6.728309631347656,
      "learning_rate": 1.0854700854700855e-05,
      "loss": 3.8451,
      "step": 458000
    },
    {
      "epoch": 3918.803418803419,
      "grad_norm": 7.14880895614624,
      "learning_rate": 1.0811965811965812e-05,
      "loss": 3.8694,
      "step": 458500
    },
    {
      "epoch": 3923.076923076923,
      "grad_norm": 6.371626377105713,
      "learning_rate": 1.0769230769230771e-05,
      "loss": 3.8621,
      "step": 459000
    },
    {
      "epoch": 3927.3504273504273,
      "grad_norm": 7.281616687774658,
      "learning_rate": 1.0726495726495727e-05,
      "loss": 3.8662,
      "step": 459500
    },
    {
      "epoch": 3931.6239316239316,
      "grad_norm": 8.76583194732666,
      "learning_rate": 1.0683760683760684e-05,
      "loss": 3.8637,
      "step": 460000
    },
    {
      "epoch": 3935.897435897436,
      "grad_norm": 7.246132850646973,
      "learning_rate": 1.0641025641025641e-05,
      "loss": 3.8465,
      "step": 460500
    },
    {
      "epoch": 3940.17094017094,
      "grad_norm": 7.294674873352051,
      "learning_rate": 1.0598290598290598e-05,
      "loss": 3.8624,
      "step": 461000
    },
    {
      "epoch": 3944.4444444444443,
      "grad_norm": 9.854888916015625,
      "learning_rate": 1.0555555555555555e-05,
      "loss": 3.855,
      "step": 461500
    },
    {
      "epoch": 3948.7179487179487,
      "grad_norm": 7.129552841186523,
      "learning_rate": 1.0512820512820514e-05,
      "loss": 3.8617,
      "step": 462000
    },
    {
      "epoch": 3952.991452991453,
      "grad_norm": 8.441125869750977,
      "learning_rate": 1.0470085470085471e-05,
      "loss": 3.8642,
      "step": 462500
    },
    {
      "epoch": 3957.264957264957,
      "grad_norm": 7.320191383361816,
      "learning_rate": 1.0427350427350429e-05,
      "loss": 3.8519,
      "step": 463000
    },
    {
      "epoch": 3961.5384615384614,
      "grad_norm": 6.619969844818115,
      "learning_rate": 1.0384615384615386e-05,
      "loss": 3.8635,
      "step": 463500
    },
    {
      "epoch": 3965.811965811966,
      "grad_norm": 7.666583061218262,
      "learning_rate": 1.0341880341880341e-05,
      "loss": 3.8714,
      "step": 464000
    },
    {
      "epoch": 3970.08547008547,
      "grad_norm": 9.05552864074707,
      "learning_rate": 1.0299145299145299e-05,
      "loss": 3.8604,
      "step": 464500
    },
    {
      "epoch": 3974.358974358974,
      "grad_norm": 7.391560077667236,
      "learning_rate": 1.0256410256410256e-05,
      "loss": 3.8556,
      "step": 465000
    },
    {
      "epoch": 3978.6324786324785,
      "grad_norm": 8.924522399902344,
      "learning_rate": 1.0213675213675215e-05,
      "loss": 3.8591,
      "step": 465500
    },
    {
      "epoch": 3982.905982905983,
      "grad_norm": 4.435009956359863,
      "learning_rate": 1.0170940170940172e-05,
      "loss": 3.8636,
      "step": 466000
    },
    {
      "epoch": 3987.1794871794873,
      "grad_norm": 6.357913494110107,
      "learning_rate": 1.012820512820513e-05,
      "loss": 3.8583,
      "step": 466500
    },
    {
      "epoch": 3991.4529914529912,
      "grad_norm": 7.096660614013672,
      "learning_rate": 1.0085470085470086e-05,
      "loss": 3.8627,
      "step": 467000
    },
    {
      "epoch": 3995.7264957264956,
      "grad_norm": 9.210216522216797,
      "learning_rate": 1.0042735042735044e-05,
      "loss": 3.8704,
      "step": 467500
    },
    {
      "epoch": 4000.0,
      "grad_norm": 9.487162590026855,
      "learning_rate": 1e-05,
      "loss": 3.8571,
      "step": 468000
    },
    {
      "epoch": 4004.2735042735044,
      "grad_norm": 7.12018346786499,
      "learning_rate": 9.957264957264956e-06,
      "loss": 3.856,
      "step": 468500
    },
    {
      "epoch": 4008.5470085470088,
      "grad_norm": 4.741637229919434,
      "learning_rate": 9.914529914529915e-06,
      "loss": 3.8672,
      "step": 469000
    },
    {
      "epoch": 4012.8205128205127,
      "grad_norm": 6.4379143714904785,
      "learning_rate": 9.871794871794872e-06,
      "loss": 3.8582,
      "step": 469500
    },
    {
      "epoch": 4017.094017094017,
      "grad_norm": 11.81250286102295,
      "learning_rate": 9.82905982905983e-06,
      "loss": 3.8644,
      "step": 470000
    },
    {
      "epoch": 4021.3675213675215,
      "grad_norm": 9.063387870788574,
      "learning_rate": 9.786324786324787e-06,
      "loss": 3.8588,
      "step": 470500
    },
    {
      "epoch": 4025.641025641026,
      "grad_norm": 7.594265937805176,
      "learning_rate": 9.743589743589744e-06,
      "loss": 3.8589,
      "step": 471000
    },
    {
      "epoch": 4029.91452991453,
      "grad_norm": 10.156315803527832,
      "learning_rate": 9.700854700854701e-06,
      "loss": 3.8644,
      "step": 471500
    },
    {
      "epoch": 4034.188034188034,
      "grad_norm": 7.582855224609375,
      "learning_rate": 9.658119658119659e-06,
      "loss": 3.8633,
      "step": 472000
    },
    {
      "epoch": 4038.4615384615386,
      "grad_norm": 9.258124351501465,
      "learning_rate": 9.615384615384616e-06,
      "loss": 3.8576,
      "step": 472500
    },
    {
      "epoch": 4042.735042735043,
      "grad_norm": 6.887225151062012,
      "learning_rate": 9.572649572649575e-06,
      "loss": 3.8666,
      "step": 473000
    },
    {
      "epoch": 4047.008547008547,
      "grad_norm": 6.5755791664123535,
      "learning_rate": 9.52991452991453e-06,
      "loss": 3.8637,
      "step": 473500
    },
    {
      "epoch": 4051.2820512820513,
      "grad_norm": 8.407148361206055,
      "learning_rate": 9.487179487179487e-06,
      "loss": 3.8622,
      "step": 474000
    },
    {
      "epoch": 4055.5555555555557,
      "grad_norm": 9.001065254211426,
      "learning_rate": 9.444444444444445e-06,
      "loss": 3.8722,
      "step": 474500
    },
    {
      "epoch": 4059.82905982906,
      "grad_norm": 6.949166297912598,
      "learning_rate": 9.401709401709402e-06,
      "loss": 3.8585,
      "step": 475000
    },
    {
      "epoch": 4064.102564102564,
      "grad_norm": 5.738988399505615,
      "learning_rate": 9.358974358974359e-06,
      "loss": 3.8597,
      "step": 475500
    },
    {
      "epoch": 4068.3760683760684,
      "grad_norm": 7.398311138153076,
      "learning_rate": 9.316239316239316e-06,
      "loss": 3.8576,
      "step": 476000
    },
    {
      "epoch": 4072.6495726495727,
      "grad_norm": 8.394957542419434,
      "learning_rate": 9.273504273504275e-06,
      "loss": 3.8633,
      "step": 476500
    },
    {
      "epoch": 4076.923076923077,
      "grad_norm": 7.677635669708252,
      "learning_rate": 9.230769230769232e-06,
      "loss": 3.8598,
      "step": 477000
    },
    {
      "epoch": 4081.196581196581,
      "grad_norm": 8.96096134185791,
      "learning_rate": 9.18803418803419e-06,
      "loss": 3.8571,
      "step": 477500
    },
    {
      "epoch": 4085.4700854700855,
      "grad_norm": 11.941067695617676,
      "learning_rate": 9.145299145299145e-06,
      "loss": 3.8532,
      "step": 478000
    },
    {
      "epoch": 4089.74358974359,
      "grad_norm": 8.02379035949707,
      "learning_rate": 9.102564102564102e-06,
      "loss": 3.8684,
      "step": 478500
    },
    {
      "epoch": 4094.017094017094,
      "grad_norm": 6.949399471282959,
      "learning_rate": 9.05982905982906e-06,
      "loss": 3.8573,
      "step": 479000
    },
    {
      "epoch": 4098.290598290599,
      "grad_norm": 10.273016929626465,
      "learning_rate": 9.017094017094017e-06,
      "loss": 3.8545,
      "step": 479500
    },
    {
      "epoch": 4102.5641025641025,
      "grad_norm": 5.510629177093506,
      "learning_rate": 8.974358974358976e-06,
      "loss": 3.8585,
      "step": 480000
    },
    {
      "epoch": 4106.8376068376065,
      "grad_norm": 8.599813461303711,
      "learning_rate": 8.931623931623933e-06,
      "loss": 3.8679,
      "step": 480500
    },
    {
      "epoch": 4111.111111111111,
      "grad_norm": 8.23393726348877,
      "learning_rate": 8.88888888888889e-06,
      "loss": 3.8616,
      "step": 481000
    },
    {
      "epoch": 4115.384615384615,
      "grad_norm": 7.179633617401123,
      "learning_rate": 8.846153846153847e-06,
      "loss": 3.848,
      "step": 481500
    },
    {
      "epoch": 4119.65811965812,
      "grad_norm": 6.885029315948486,
      "learning_rate": 8.803418803418804e-06,
      "loss": 3.8671,
      "step": 482000
    },
    {
      "epoch": 4123.931623931624,
      "grad_norm": 7.128113746643066,
      "learning_rate": 8.76068376068376e-06,
      "loss": 3.853,
      "step": 482500
    },
    {
      "epoch": 4128.205128205128,
      "grad_norm": 8.258902549743652,
      "learning_rate": 8.717948717948717e-06,
      "loss": 3.8513,
      "step": 483000
    },
    {
      "epoch": 4132.478632478633,
      "grad_norm": 8.345333099365234,
      "learning_rate": 8.675213675213676e-06,
      "loss": 3.86,
      "step": 483500
    },
    {
      "epoch": 4136.752136752137,
      "grad_norm": 9.36492919921875,
      "learning_rate": 8.632478632478633e-06,
      "loss": 3.8608,
      "step": 484000
    },
    {
      "epoch": 4141.025641025641,
      "grad_norm": 10.303084373474121,
      "learning_rate": 8.58974358974359e-06,
      "loss": 3.8616,
      "step": 484500
    },
    {
      "epoch": 4145.2991452991455,
      "grad_norm": 7.8226704597473145,
      "learning_rate": 8.547008547008548e-06,
      "loss": 3.8563,
      "step": 485000
    },
    {
      "epoch": 4149.572649572649,
      "grad_norm": 5.773894309997559,
      "learning_rate": 8.504273504273505e-06,
      "loss": 3.8641,
      "step": 485500
    },
    {
      "epoch": 4153.846153846154,
      "grad_norm": 7.214920997619629,
      "learning_rate": 8.461538461538462e-06,
      "loss": 3.852,
      "step": 486000
    },
    {
      "epoch": 4158.119658119658,
      "grad_norm": 9.992918968200684,
      "learning_rate": 8.41880341880342e-06,
      "loss": 3.8574,
      "step": 486500
    },
    {
      "epoch": 4162.393162393162,
      "grad_norm": 8.035429000854492,
      "learning_rate": 8.376068376068377e-06,
      "loss": 3.8556,
      "step": 487000
    },
    {
      "epoch": 4166.666666666667,
      "grad_norm": 11.386371612548828,
      "learning_rate": 8.333333333333334e-06,
      "loss": 3.8593,
      "step": 487500
    },
    {
      "epoch": 4170.940170940171,
      "grad_norm": 7.01936674118042,
      "learning_rate": 8.290598290598291e-06,
      "loss": 3.8679,
      "step": 488000
    },
    {
      "epoch": 4175.213675213675,
      "grad_norm": 6.47403621673584,
      "learning_rate": 8.247863247863248e-06,
      "loss": 3.8455,
      "step": 488500
    },
    {
      "epoch": 4179.48717948718,
      "grad_norm": 5.984039783477783,
      "learning_rate": 8.205128205128205e-06,
      "loss": 3.8448,
      "step": 489000
    },
    {
      "epoch": 4183.760683760684,
      "grad_norm": 8.448040962219238,
      "learning_rate": 8.162393162393163e-06,
      "loss": 3.8724,
      "step": 489500
    },
    {
      "epoch": 4188.034188034188,
      "grad_norm": 8.085918426513672,
      "learning_rate": 8.11965811965812e-06,
      "loss": 3.8645,
      "step": 490000
    },
    {
      "epoch": 4192.307692307692,
      "grad_norm": 9.907697677612305,
      "learning_rate": 8.076923076923077e-06,
      "loss": 3.858,
      "step": 490500
    },
    {
      "epoch": 4196.581196581196,
      "grad_norm": 5.601161479949951,
      "learning_rate": 8.034188034188036e-06,
      "loss": 3.8572,
      "step": 491000
    },
    {
      "epoch": 4200.854700854701,
      "grad_norm": 6.6292524337768555,
      "learning_rate": 7.991452991452991e-06,
      "loss": 3.8602,
      "step": 491500
    },
    {
      "epoch": 4205.128205128205,
      "grad_norm": 7.343631267547607,
      "learning_rate": 7.948717948717949e-06,
      "loss": 3.8625,
      "step": 492000
    },
    {
      "epoch": 4209.401709401709,
      "grad_norm": 9.49142074584961,
      "learning_rate": 7.905982905982906e-06,
      "loss": 3.8581,
      "step": 492500
    },
    {
      "epoch": 4213.675213675214,
      "grad_norm": 8.426918983459473,
      "learning_rate": 7.863247863247863e-06,
      "loss": 3.8535,
      "step": 493000
    },
    {
      "epoch": 4217.948717948718,
      "grad_norm": 10.661005973815918,
      "learning_rate": 7.82051282051282e-06,
      "loss": 3.8603,
      "step": 493500
    },
    {
      "epoch": 4222.222222222223,
      "grad_norm": 8.083718299865723,
      "learning_rate": 7.777777777777777e-06,
      "loss": 3.8498,
      "step": 494000
    },
    {
      "epoch": 4226.495726495727,
      "grad_norm": 9.299099922180176,
      "learning_rate": 7.735042735042736e-06,
      "loss": 3.8573,
      "step": 494500
    },
    {
      "epoch": 4230.7692307692305,
      "grad_norm": 5.643470287322998,
      "learning_rate": 7.692307692307694e-06,
      "loss": 3.8593,
      "step": 495000
    },
    {
      "epoch": 4235.042735042735,
      "grad_norm": 5.606377124786377,
      "learning_rate": 7.64957264957265e-06,
      "loss": 3.8566,
      "step": 495500
    },
    {
      "epoch": 4239.316239316239,
      "grad_norm": 9.638766288757324,
      "learning_rate": 7.606837606837606e-06,
      "loss": 3.8491,
      "step": 496000
    },
    {
      "epoch": 4243.589743589743,
      "grad_norm": 7.586766719818115,
      "learning_rate": 7.564102564102564e-06,
      "loss": 3.8631,
      "step": 496500
    },
    {
      "epoch": 4247.863247863248,
      "grad_norm": 7.717062473297119,
      "learning_rate": 7.521367521367522e-06,
      "loss": 3.8545,
      "step": 497000
    },
    {
      "epoch": 4252.136752136752,
      "grad_norm": 7.668787956237793,
      "learning_rate": 7.478632478632479e-06,
      "loss": 3.8734,
      "step": 497500
    },
    {
      "epoch": 4256.410256410257,
      "grad_norm": 6.062705039978027,
      "learning_rate": 7.435897435897436e-06,
      "loss": 3.8616,
      "step": 498000
    },
    {
      "epoch": 4260.683760683761,
      "grad_norm": 9.72486686706543,
      "learning_rate": 7.393162393162393e-06,
      "loss": 3.8643,
      "step": 498500
    },
    {
      "epoch": 4264.957264957265,
      "grad_norm": 7.147214889526367,
      "learning_rate": 7.350427350427351e-06,
      "loss": 3.8591,
      "step": 499000
    },
    {
      "epoch": 4269.2307692307695,
      "grad_norm": 7.514578819274902,
      "learning_rate": 7.3076923076923085e-06,
      "loss": 3.856,
      "step": 499500
    },
    {
      "epoch": 4273.504273504273,
      "grad_norm": 6.71247673034668,
      "learning_rate": 7.264957264957266e-06,
      "loss": 3.8628,
      "step": 500000
    },
    {
      "epoch": 4277.777777777777,
      "grad_norm": 11.848636627197266,
      "learning_rate": 7.222222222222222e-06,
      "loss": 3.8609,
      "step": 500500
    },
    {
      "epoch": 4282.051282051282,
      "grad_norm": 11.045197486877441,
      "learning_rate": 7.179487179487179e-06,
      "loss": 3.853,
      "step": 501000
    },
    {
      "epoch": 4286.324786324786,
      "grad_norm": 8.483200073242188,
      "learning_rate": 7.1367521367521365e-06,
      "loss": 3.8465,
      "step": 501500
    },
    {
      "epoch": 4290.598290598291,
      "grad_norm": 6.4995927810668945,
      "learning_rate": 7.0940170940170945e-06,
      "loss": 3.8554,
      "step": 502000
    },
    {
      "epoch": 4294.871794871795,
      "grad_norm": 5.734519004821777,
      "learning_rate": 7.051282051282052e-06,
      "loss": 3.858,
      "step": 502500
    },
    {
      "epoch": 4299.145299145299,
      "grad_norm": 5.981606483459473,
      "learning_rate": 7.008547008547009e-06,
      "loss": 3.8563,
      "step": 503000
    },
    {
      "epoch": 4303.418803418804,
      "grad_norm": 6.535131931304932,
      "learning_rate": 6.965811965811966e-06,
      "loss": 3.851,
      "step": 503500
    },
    {
      "epoch": 4307.692307692308,
      "grad_norm": 7.919234275817871,
      "learning_rate": 6.923076923076923e-06,
      "loss": 3.8683,
      "step": 504000
    },
    {
      "epoch": 4311.965811965812,
      "grad_norm": 8.819173812866211,
      "learning_rate": 6.8803418803418814e-06,
      "loss": 3.854,
      "step": 504500
    },
    {
      "epoch": 4316.239316239316,
      "grad_norm": 8.5548095703125,
      "learning_rate": 6.837606837606839e-06,
      "loss": 3.8551,
      "step": 505000
    },
    {
      "epoch": 4320.51282051282,
      "grad_norm": 9.009775161743164,
      "learning_rate": 6.794871794871795e-06,
      "loss": 3.8584,
      "step": 505500
    },
    {
      "epoch": 4324.786324786325,
      "grad_norm": 8.08368968963623,
      "learning_rate": 6.752136752136752e-06,
      "loss": 3.8535,
      "step": 506000
    },
    {
      "epoch": 4329.059829059829,
      "grad_norm": 7.631331443786621,
      "learning_rate": 6.7094017094017094e-06,
      "loss": 3.8561,
      "step": 506500
    },
    {
      "epoch": 4333.333333333333,
      "grad_norm": 6.449179649353027,
      "learning_rate": 6.666666666666667e-06,
      "loss": 3.8533,
      "step": 507000
    },
    {
      "epoch": 4337.606837606838,
      "grad_norm": 5.822954177856445,
      "learning_rate": 6.623931623931625e-06,
      "loss": 3.8555,
      "step": 507500
    },
    {
      "epoch": 4341.880341880342,
      "grad_norm": 7.679749965667725,
      "learning_rate": 6.581196581196582e-06,
      "loss": 3.8699,
      "step": 508000
    },
    {
      "epoch": 4346.153846153846,
      "grad_norm": 7.643255233764648,
      "learning_rate": 6.538461538461539e-06,
      "loss": 3.8631,
      "step": 508500
    },
    {
      "epoch": 4350.427350427351,
      "grad_norm": 5.908190727233887,
      "learning_rate": 6.495726495726496e-06,
      "loss": 3.8534,
      "step": 509000
    },
    {
      "epoch": 4354.7008547008545,
      "grad_norm": 7.851797103881836,
      "learning_rate": 6.4529914529914535e-06,
      "loss": 3.8564,
      "step": 509500
    },
    {
      "epoch": 4358.974358974359,
      "grad_norm": 6.534074306488037,
      "learning_rate": 6.41025641025641e-06,
      "loss": 3.8557,
      "step": 510000
    },
    {
      "epoch": 4363.247863247863,
      "grad_norm": 8.139010429382324,
      "learning_rate": 6.367521367521367e-06,
      "loss": 3.8588,
      "step": 510500
    },
    {
      "epoch": 4367.521367521367,
      "grad_norm": 7.628418922424316,
      "learning_rate": 6.324786324786325e-06,
      "loss": 3.8604,
      "step": 511000
    },
    {
      "epoch": 4371.794871794872,
      "grad_norm": 8.385743141174316,
      "learning_rate": 6.282051282051282e-06,
      "loss": 3.8563,
      "step": 511500
    },
    {
      "epoch": 4376.068376068376,
      "grad_norm": 8.9276704788208,
      "learning_rate": 6.23931623931624e-06,
      "loss": 3.8677,
      "step": 512000
    },
    {
      "epoch": 4380.34188034188,
      "grad_norm": 14.646141052246094,
      "learning_rate": 6.196581196581197e-06,
      "loss": 3.8567,
      "step": 512500
    },
    {
      "epoch": 4384.615384615385,
      "grad_norm": 5.769300937652588,
      "learning_rate": 6.153846153846155e-06,
      "loss": 3.8508,
      "step": 513000
    },
    {
      "epoch": 4388.888888888889,
      "grad_norm": 10.39576244354248,
      "learning_rate": 6.111111111111111e-06,
      "loss": 3.8568,
      "step": 513500
    },
    {
      "epoch": 4393.1623931623935,
      "grad_norm": 10.510297775268555,
      "learning_rate": 6.0683760683760684e-06,
      "loss": 3.8485,
      "step": 514000
    },
    {
      "epoch": 4397.4358974358975,
      "grad_norm": 7.576146125793457,
      "learning_rate": 6.025641025641026e-06,
      "loss": 3.8614,
      "step": 514500
    },
    {
      "epoch": 4401.709401709401,
      "grad_norm": 6.151114463806152,
      "learning_rate": 5.982905982905984e-06,
      "loss": 3.8538,
      "step": 515000
    },
    {
      "epoch": 4405.982905982906,
      "grad_norm": 5.268264293670654,
      "learning_rate": 5.94017094017094e-06,
      "loss": 3.8596,
      "step": 515500
    },
    {
      "epoch": 4410.25641025641,
      "grad_norm": 6.75053071975708,
      "learning_rate": 5.897435897435897e-06,
      "loss": 3.8657,
      "step": 516000
    },
    {
      "epoch": 4414.529914529914,
      "grad_norm": 9.044414520263672,
      "learning_rate": 5.854700854700855e-06,
      "loss": 3.8571,
      "step": 516500
    },
    {
      "epoch": 4418.803418803419,
      "grad_norm": 7.874474048614502,
      "learning_rate": 5.8119658119658126e-06,
      "loss": 3.8585,
      "step": 517000
    },
    {
      "epoch": 4423.076923076923,
      "grad_norm": 7.545346736907959,
      "learning_rate": 5.76923076923077e-06,
      "loss": 3.8511,
      "step": 517500
    },
    {
      "epoch": 4427.350427350428,
      "grad_norm": 10.191519737243652,
      "learning_rate": 5.726495726495726e-06,
      "loss": 3.8462,
      "step": 518000
    }
  ],
  "logging_steps": 500,
  "max_steps": 585000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5000,
  "save_steps": 14800,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.625589617384819e+16,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
