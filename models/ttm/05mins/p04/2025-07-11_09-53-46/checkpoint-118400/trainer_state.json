{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1011.965811965812,
  "eval_steps": 500,
  "global_step": 118400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.273504273504273,
      "grad_norm": 3.1177163124084473,
      "learning_rate": 4.995726495726496e-05,
      "loss": 5.1253,
      "step": 500
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 4.9700751304626465,
      "learning_rate": 4.991452991452992e-05,
      "loss": 4.7893,
      "step": 1000
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 5.357769966125488,
      "learning_rate": 4.9871794871794874e-05,
      "loss": 4.6697,
      "step": 1500
    },
    {
      "epoch": 17.094017094017094,
      "grad_norm": 6.760709285736084,
      "learning_rate": 4.982905982905983e-05,
      "loss": 4.5894,
      "step": 2000
    },
    {
      "epoch": 21.367521367521366,
      "grad_norm": 7.849138259887695,
      "learning_rate": 4.978632478632479e-05,
      "loss": 4.5275,
      "step": 2500
    },
    {
      "epoch": 25.641025641025642,
      "grad_norm": 13.399908065795898,
      "learning_rate": 4.9743589743589746e-05,
      "loss": 4.4858,
      "step": 3000
    },
    {
      "epoch": 29.914529914529915,
      "grad_norm": 5.951698303222656,
      "learning_rate": 4.97008547008547e-05,
      "loss": 4.4704,
      "step": 3500
    },
    {
      "epoch": 34.18803418803419,
      "grad_norm": 12.644576072692871,
      "learning_rate": 4.965811965811966e-05,
      "loss": 4.4274,
      "step": 4000
    },
    {
      "epoch": 38.46153846153846,
      "grad_norm": 7.810548305511475,
      "learning_rate": 4.961538461538462e-05,
      "loss": 4.4039,
      "step": 4500
    },
    {
      "epoch": 42.73504273504273,
      "grad_norm": 11.115514755249023,
      "learning_rate": 4.9572649572649575e-05,
      "loss": 4.3992,
      "step": 5000
    },
    {
      "epoch": 47.00854700854701,
      "grad_norm": 7.523601055145264,
      "learning_rate": 4.952991452991453e-05,
      "loss": 4.3591,
      "step": 5500
    },
    {
      "epoch": 51.282051282051285,
      "grad_norm": 5.32051944732666,
      "learning_rate": 4.948717948717949e-05,
      "loss": 4.3436,
      "step": 6000
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 5.7435197830200195,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 4.3226,
      "step": 6500
    },
    {
      "epoch": 59.82905982905983,
      "grad_norm": 15.95846939086914,
      "learning_rate": 4.94017094017094e-05,
      "loss": 4.311,
      "step": 7000
    },
    {
      "epoch": 64.1025641025641,
      "grad_norm": 9.219342231750488,
      "learning_rate": 4.935897435897436e-05,
      "loss": 4.3008,
      "step": 7500
    },
    {
      "epoch": 68.37606837606837,
      "grad_norm": 6.521927833557129,
      "learning_rate": 4.931623931623932e-05,
      "loss": 4.2793,
      "step": 8000
    },
    {
      "epoch": 72.64957264957265,
      "grad_norm": 10.859046936035156,
      "learning_rate": 4.927350427350428e-05,
      "loss": 4.2699,
      "step": 8500
    },
    {
      "epoch": 76.92307692307692,
      "grad_norm": 6.567059516906738,
      "learning_rate": 4.923076923076924e-05,
      "loss": 4.2601,
      "step": 9000
    },
    {
      "epoch": 81.19658119658119,
      "grad_norm": 10.567413330078125,
      "learning_rate": 4.918803418803419e-05,
      "loss": 4.248,
      "step": 9500
    },
    {
      "epoch": 85.47008547008546,
      "grad_norm": 7.6750688552856445,
      "learning_rate": 4.9145299145299147e-05,
      "loss": 4.2358,
      "step": 10000
    },
    {
      "epoch": 89.74358974358974,
      "grad_norm": 8.19306755065918,
      "learning_rate": 4.9102564102564104e-05,
      "loss": 4.2315,
      "step": 10500
    },
    {
      "epoch": 94.01709401709402,
      "grad_norm": 6.8381524085998535,
      "learning_rate": 4.905982905982906e-05,
      "loss": 4.2126,
      "step": 11000
    },
    {
      "epoch": 98.2905982905983,
      "grad_norm": 6.028937339782715,
      "learning_rate": 4.901709401709402e-05,
      "loss": 4.2104,
      "step": 11500
    },
    {
      "epoch": 102.56410256410257,
      "grad_norm": 6.542298316955566,
      "learning_rate": 4.8974358974358975e-05,
      "loss": 4.1973,
      "step": 12000
    },
    {
      "epoch": 106.83760683760684,
      "grad_norm": 7.324919700622559,
      "learning_rate": 4.893162393162393e-05,
      "loss": 4.2057,
      "step": 12500
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 10.700521469116211,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.1824,
      "step": 13000
    },
    {
      "epoch": 115.38461538461539,
      "grad_norm": 10.119913101196289,
      "learning_rate": 4.884615384615385e-05,
      "loss": 4.1825,
      "step": 13500
    },
    {
      "epoch": 119.65811965811966,
      "grad_norm": 6.354224681854248,
      "learning_rate": 4.8803418803418804e-05,
      "loss": 4.181,
      "step": 14000
    },
    {
      "epoch": 123.93162393162393,
      "grad_norm": 10.041450500488281,
      "learning_rate": 4.876068376068376e-05,
      "loss": 4.1606,
      "step": 14500
    },
    {
      "epoch": 128.2051282051282,
      "grad_norm": 8.247559547424316,
      "learning_rate": 4.871794871794872e-05,
      "loss": 4.1579,
      "step": 15000
    },
    {
      "epoch": 132.47863247863248,
      "grad_norm": 7.573277950286865,
      "learning_rate": 4.8675213675213676e-05,
      "loss": 4.1568,
      "step": 15500
    },
    {
      "epoch": 136.75213675213675,
      "grad_norm": 8.592423439025879,
      "learning_rate": 4.863247863247863e-05,
      "loss": 4.1464,
      "step": 16000
    },
    {
      "epoch": 141.02564102564102,
      "grad_norm": 7.11668062210083,
      "learning_rate": 4.858974358974359e-05,
      "loss": 4.139,
      "step": 16500
    },
    {
      "epoch": 145.2991452991453,
      "grad_norm": 6.893136978149414,
      "learning_rate": 4.854700854700855e-05,
      "loss": 4.1334,
      "step": 17000
    },
    {
      "epoch": 149.57264957264957,
      "grad_norm": 8.224817276000977,
      "learning_rate": 4.8504273504273505e-05,
      "loss": 4.1382,
      "step": 17500
    },
    {
      "epoch": 153.84615384615384,
      "grad_norm": 7.110662460327148,
      "learning_rate": 4.846153846153846e-05,
      "loss": 4.1192,
      "step": 18000
    },
    {
      "epoch": 158.1196581196581,
      "grad_norm": 8.336471557617188,
      "learning_rate": 4.841880341880342e-05,
      "loss": 4.1298,
      "step": 18500
    },
    {
      "epoch": 162.39316239316238,
      "grad_norm": 10.689091682434082,
      "learning_rate": 4.8376068376068376e-05,
      "loss": 4.1223,
      "step": 19000
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 8.600497245788574,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 4.1124,
      "step": 19500
    },
    {
      "epoch": 170.94017094017093,
      "grad_norm": 6.878803730010986,
      "learning_rate": 4.829059829059829e-05,
      "loss": 4.1094,
      "step": 20000
    },
    {
      "epoch": 175.2136752136752,
      "grad_norm": 9.887246131896973,
      "learning_rate": 4.824786324786325e-05,
      "loss": 4.1045,
      "step": 20500
    },
    {
      "epoch": 179.48717948717947,
      "grad_norm": 7.862114906311035,
      "learning_rate": 4.8205128205128205e-05,
      "loss": 4.0999,
      "step": 21000
    },
    {
      "epoch": 183.76068376068375,
      "grad_norm": 12.425511360168457,
      "learning_rate": 4.816239316239316e-05,
      "loss": 4.1091,
      "step": 21500
    },
    {
      "epoch": 188.03418803418805,
      "grad_norm": 6.507553577423096,
      "learning_rate": 4.8119658119658126e-05,
      "loss": 4.0822,
      "step": 22000
    },
    {
      "epoch": 192.30769230769232,
      "grad_norm": 8.65228271484375,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 4.0936,
      "step": 22500
    },
    {
      "epoch": 196.5811965811966,
      "grad_norm": 6.597097873687744,
      "learning_rate": 4.803418803418804e-05,
      "loss": 4.0898,
      "step": 23000
    },
    {
      "epoch": 200.85470085470087,
      "grad_norm": 9.715621948242188,
      "learning_rate": 4.7991452991453e-05,
      "loss": 4.0749,
      "step": 23500
    },
    {
      "epoch": 205.12820512820514,
      "grad_norm": 12.793821334838867,
      "learning_rate": 4.7948717948717955e-05,
      "loss": 4.0855,
      "step": 24000
    },
    {
      "epoch": 209.4017094017094,
      "grad_norm": 6.782865047454834,
      "learning_rate": 4.790598290598291e-05,
      "loss": 4.0723,
      "step": 24500
    },
    {
      "epoch": 213.67521367521368,
      "grad_norm": 7.109498500823975,
      "learning_rate": 4.786324786324787e-05,
      "loss": 4.0779,
      "step": 25000
    },
    {
      "epoch": 217.94871794871796,
      "grad_norm": 13.188231468200684,
      "learning_rate": 4.782051282051283e-05,
      "loss": 4.0752,
      "step": 25500
    },
    {
      "epoch": 222.22222222222223,
      "grad_norm": 4.958047866821289,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 4.0685,
      "step": 26000
    },
    {
      "epoch": 226.4957264957265,
      "grad_norm": 7.153029441833496,
      "learning_rate": 4.773504273504274e-05,
      "loss": 4.0625,
      "step": 26500
    },
    {
      "epoch": 230.76923076923077,
      "grad_norm": 6.8623046875,
      "learning_rate": 4.76923076923077e-05,
      "loss": 4.0617,
      "step": 27000
    },
    {
      "epoch": 235.04273504273505,
      "grad_norm": 5.7580485343933105,
      "learning_rate": 4.764957264957265e-05,
      "loss": 4.0614,
      "step": 27500
    },
    {
      "epoch": 239.31623931623932,
      "grad_norm": 9.779964447021484,
      "learning_rate": 4.7606837606837606e-05,
      "loss": 4.0556,
      "step": 28000
    },
    {
      "epoch": 243.5897435897436,
      "grad_norm": 11.769835472106934,
      "learning_rate": 4.7564102564102563e-05,
      "loss": 4.0585,
      "step": 28500
    },
    {
      "epoch": 247.86324786324786,
      "grad_norm": 6.6153154373168945,
      "learning_rate": 4.752136752136752e-05,
      "loss": 4.0474,
      "step": 29000
    },
    {
      "epoch": 252.13675213675214,
      "grad_norm": 4.998359203338623,
      "learning_rate": 4.747863247863248e-05,
      "loss": 4.0469,
      "step": 29500
    },
    {
      "epoch": 256.4102564102564,
      "grad_norm": 11.855521202087402,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 4.0443,
      "step": 30000
    },
    {
      "epoch": 260.6837606837607,
      "grad_norm": 5.53583288192749,
      "learning_rate": 4.739316239316239e-05,
      "loss": 4.0498,
      "step": 30500
    },
    {
      "epoch": 264.95726495726495,
      "grad_norm": 13.338637351989746,
      "learning_rate": 4.735042735042735e-05,
      "loss": 4.0511,
      "step": 31000
    },
    {
      "epoch": 269.2307692307692,
      "grad_norm": 5.888394832611084,
      "learning_rate": 4.730769230769231e-05,
      "loss": 4.0344,
      "step": 31500
    },
    {
      "epoch": 273.5042735042735,
      "grad_norm": 8.230168342590332,
      "learning_rate": 4.7264957264957264e-05,
      "loss": 4.0365,
      "step": 32000
    },
    {
      "epoch": 277.77777777777777,
      "grad_norm": 8.78708267211914,
      "learning_rate": 4.722222222222222e-05,
      "loss": 4.0326,
      "step": 32500
    },
    {
      "epoch": 282.05128205128204,
      "grad_norm": 9.223427772521973,
      "learning_rate": 4.717948717948718e-05,
      "loss": 4.0342,
      "step": 33000
    },
    {
      "epoch": 286.3247863247863,
      "grad_norm": 8.614336013793945,
      "learning_rate": 4.7136752136752136e-05,
      "loss": 4.0241,
      "step": 33500
    },
    {
      "epoch": 290.5982905982906,
      "grad_norm": 6.094795227050781,
      "learning_rate": 4.709401709401709e-05,
      "loss": 4.0278,
      "step": 34000
    },
    {
      "epoch": 294.87179487179486,
      "grad_norm": 7.887334823608398,
      "learning_rate": 4.705128205128205e-05,
      "loss": 4.0384,
      "step": 34500
    },
    {
      "epoch": 299.14529914529913,
      "grad_norm": 6.632718086242676,
      "learning_rate": 4.700854700854701e-05,
      "loss": 4.0288,
      "step": 35000
    },
    {
      "epoch": 303.4188034188034,
      "grad_norm": 7.052670001983643,
      "learning_rate": 4.6965811965811964e-05,
      "loss": 4.026,
      "step": 35500
    },
    {
      "epoch": 307.6923076923077,
      "grad_norm": 12.567428588867188,
      "learning_rate": 4.692307692307693e-05,
      "loss": 4.0186,
      "step": 36000
    },
    {
      "epoch": 311.96581196581195,
      "grad_norm": 8.308412551879883,
      "learning_rate": 4.6880341880341886e-05,
      "loss": 4.0384,
      "step": 36500
    },
    {
      "epoch": 316.2393162393162,
      "grad_norm": 6.915613651275635,
      "learning_rate": 4.683760683760684e-05,
      "loss": 4.0256,
      "step": 37000
    },
    {
      "epoch": 320.5128205128205,
      "grad_norm": 11.106752395629883,
      "learning_rate": 4.67948717948718e-05,
      "loss": 4.0234,
      "step": 37500
    },
    {
      "epoch": 324.78632478632477,
      "grad_norm": 9.530385971069336,
      "learning_rate": 4.675213675213676e-05,
      "loss": 4.0134,
      "step": 38000
    },
    {
      "epoch": 329.05982905982904,
      "grad_norm": 7.745054721832275,
      "learning_rate": 4.6709401709401714e-05,
      "loss": 4.0107,
      "step": 38500
    },
    {
      "epoch": 333.3333333333333,
      "grad_norm": 7.7903923988342285,
      "learning_rate": 4.666666666666667e-05,
      "loss": 4.0206,
      "step": 39000
    },
    {
      "epoch": 337.6068376068376,
      "grad_norm": 12.71082592010498,
      "learning_rate": 4.662393162393163e-05,
      "loss": 4.009,
      "step": 39500
    },
    {
      "epoch": 341.88034188034186,
      "grad_norm": 9.180649757385254,
      "learning_rate": 4.6581196581196586e-05,
      "loss": 4.0012,
      "step": 40000
    },
    {
      "epoch": 346.15384615384613,
      "grad_norm": 7.996723175048828,
      "learning_rate": 4.653846153846154e-05,
      "loss": 4.0107,
      "step": 40500
    },
    {
      "epoch": 350.4273504273504,
      "grad_norm": 6.678990840911865,
      "learning_rate": 4.64957264957265e-05,
      "loss": 4.0046,
      "step": 41000
    },
    {
      "epoch": 354.7008547008547,
      "grad_norm": 6.712183475494385,
      "learning_rate": 4.645299145299146e-05,
      "loss": 4.0028,
      "step": 41500
    },
    {
      "epoch": 358.97435897435895,
      "grad_norm": 6.708301067352295,
      "learning_rate": 4.6410256410256415e-05,
      "loss": 3.9945,
      "step": 42000
    },
    {
      "epoch": 363.2478632478632,
      "grad_norm": 7.1230268478393555,
      "learning_rate": 4.636752136752137e-05,
      "loss": 4.0007,
      "step": 42500
    },
    {
      "epoch": 367.5213675213675,
      "grad_norm": 10.119011878967285,
      "learning_rate": 4.632478632478633e-05,
      "loss": 4.007,
      "step": 43000
    },
    {
      "epoch": 371.79487179487177,
      "grad_norm": 7.079206466674805,
      "learning_rate": 4.6282051282051287e-05,
      "loss": 4.0029,
      "step": 43500
    },
    {
      "epoch": 376.0683760683761,
      "grad_norm": 12.11133861541748,
      "learning_rate": 4.6239316239316244e-05,
      "loss": 3.9995,
      "step": 44000
    },
    {
      "epoch": 380.34188034188037,
      "grad_norm": 11.6453857421875,
      "learning_rate": 4.61965811965812e-05,
      "loss": 3.9973,
      "step": 44500
    },
    {
      "epoch": 384.61538461538464,
      "grad_norm": 8.945377349853516,
      "learning_rate": 4.615384615384616e-05,
      "loss": 3.9993,
      "step": 45000
    },
    {
      "epoch": 388.8888888888889,
      "grad_norm": 7.565587997436523,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 4.002,
      "step": 45500
    },
    {
      "epoch": 393.1623931623932,
      "grad_norm": 9.409317016601562,
      "learning_rate": 4.6068376068376066e-05,
      "loss": 3.9949,
      "step": 46000
    },
    {
      "epoch": 397.43589743589746,
      "grad_norm": 9.163902282714844,
      "learning_rate": 4.602564102564102e-05,
      "loss": 3.9962,
      "step": 46500
    },
    {
      "epoch": 401.70940170940173,
      "grad_norm": 7.880346775054932,
      "learning_rate": 4.598290598290598e-05,
      "loss": 3.9968,
      "step": 47000
    },
    {
      "epoch": 405.982905982906,
      "grad_norm": 5.583776473999023,
      "learning_rate": 4.594017094017094e-05,
      "loss": 3.9935,
      "step": 47500
    },
    {
      "epoch": 410.2564102564103,
      "grad_norm": 6.784665584564209,
      "learning_rate": 4.5897435897435895e-05,
      "loss": 3.9866,
      "step": 48000
    },
    {
      "epoch": 414.52991452991455,
      "grad_norm": 6.990201473236084,
      "learning_rate": 4.585470085470085e-05,
      "loss": 3.9912,
      "step": 48500
    },
    {
      "epoch": 418.8034188034188,
      "grad_norm": 10.827975273132324,
      "learning_rate": 4.581196581196581e-05,
      "loss": 3.9956,
      "step": 49000
    },
    {
      "epoch": 423.0769230769231,
      "grad_norm": 6.200128078460693,
      "learning_rate": 4.576923076923077e-05,
      "loss": 4.0058,
      "step": 49500
    },
    {
      "epoch": 427.35042735042737,
      "grad_norm": 4.725303649902344,
      "learning_rate": 4.572649572649573e-05,
      "loss": 3.9786,
      "step": 50000
    },
    {
      "epoch": 431.62393162393164,
      "grad_norm": 8.688067436218262,
      "learning_rate": 4.568376068376069e-05,
      "loss": 3.9886,
      "step": 50500
    },
    {
      "epoch": 435.8974358974359,
      "grad_norm": 7.352275848388672,
      "learning_rate": 4.5641025641025645e-05,
      "loss": 3.9889,
      "step": 51000
    },
    {
      "epoch": 440.1709401709402,
      "grad_norm": 6.991694927215576,
      "learning_rate": 4.55982905982906e-05,
      "loss": 3.9875,
      "step": 51500
    },
    {
      "epoch": 444.44444444444446,
      "grad_norm": 8.359098434448242,
      "learning_rate": 4.555555555555556e-05,
      "loss": 3.978,
      "step": 52000
    },
    {
      "epoch": 448.71794871794873,
      "grad_norm": 8.430614471435547,
      "learning_rate": 4.5512820512820516e-05,
      "loss": 3.9929,
      "step": 52500
    },
    {
      "epoch": 452.991452991453,
      "grad_norm": 6.726223945617676,
      "learning_rate": 4.5470085470085474e-05,
      "loss": 3.9868,
      "step": 53000
    },
    {
      "epoch": 457.2649572649573,
      "grad_norm": 7.383115291595459,
      "learning_rate": 4.542735042735043e-05,
      "loss": 3.9775,
      "step": 53500
    },
    {
      "epoch": 461.53846153846155,
      "grad_norm": 18.195093154907227,
      "learning_rate": 4.538461538461539e-05,
      "loss": 3.9925,
      "step": 54000
    },
    {
      "epoch": 465.8119658119658,
      "grad_norm": 6.556818962097168,
      "learning_rate": 4.5341880341880345e-05,
      "loss": 3.9789,
      "step": 54500
    },
    {
      "epoch": 470.0854700854701,
      "grad_norm": 9.531962394714355,
      "learning_rate": 4.52991452991453e-05,
      "loss": 3.9807,
      "step": 55000
    },
    {
      "epoch": 474.35897435897436,
      "grad_norm": 8.319652557373047,
      "learning_rate": 4.525641025641026e-05,
      "loss": 3.9866,
      "step": 55500
    },
    {
      "epoch": 478.63247863247864,
      "grad_norm": 8.463211059570312,
      "learning_rate": 4.521367521367522e-05,
      "loss": 3.984,
      "step": 56000
    },
    {
      "epoch": 482.9059829059829,
      "grad_norm": 11.724905014038086,
      "learning_rate": 4.5170940170940174e-05,
      "loss": 3.9803,
      "step": 56500
    },
    {
      "epoch": 487.1794871794872,
      "grad_norm": 7.868284702301025,
      "learning_rate": 4.512820512820513e-05,
      "loss": 3.9747,
      "step": 57000
    },
    {
      "epoch": 491.45299145299145,
      "grad_norm": 9.71316909790039,
      "learning_rate": 4.508547008547009e-05,
      "loss": 3.9719,
      "step": 57500
    },
    {
      "epoch": 495.7264957264957,
      "grad_norm": 6.478739261627197,
      "learning_rate": 4.5042735042735046e-05,
      "loss": 3.9667,
      "step": 58000
    },
    {
      "epoch": 500.0,
      "grad_norm": 9.252631187438965,
      "learning_rate": 4.5e-05,
      "loss": 3.9815,
      "step": 58500
    },
    {
      "epoch": 504.2735042735043,
      "grad_norm": 8.657444953918457,
      "learning_rate": 4.495726495726496e-05,
      "loss": 3.9662,
      "step": 59000
    },
    {
      "epoch": 508.54700854700855,
      "grad_norm": 11.178494453430176,
      "learning_rate": 4.491452991452992e-05,
      "loss": 3.9685,
      "step": 59500
    },
    {
      "epoch": 512.8205128205128,
      "grad_norm": 6.415782928466797,
      "learning_rate": 4.4871794871794874e-05,
      "loss": 3.9812,
      "step": 60000
    },
    {
      "epoch": 517.0940170940171,
      "grad_norm": 6.879825592041016,
      "learning_rate": 4.482905982905983e-05,
      "loss": 3.9631,
      "step": 60500
    },
    {
      "epoch": 521.3675213675214,
      "grad_norm": 9.492097854614258,
      "learning_rate": 4.478632478632479e-05,
      "loss": 3.9694,
      "step": 61000
    },
    {
      "epoch": 525.6410256410256,
      "grad_norm": 8.633626937866211,
      "learning_rate": 4.4743589743589746e-05,
      "loss": 3.9754,
      "step": 61500
    },
    {
      "epoch": 529.9145299145299,
      "grad_norm": 7.7194719314575195,
      "learning_rate": 4.47008547008547e-05,
      "loss": 3.9779,
      "step": 62000
    },
    {
      "epoch": 534.1880341880342,
      "grad_norm": 7.292169570922852,
      "learning_rate": 4.465811965811966e-05,
      "loss": 3.9675,
      "step": 62500
    },
    {
      "epoch": 538.4615384615385,
      "grad_norm": 7.294814586639404,
      "learning_rate": 4.461538461538462e-05,
      "loss": 3.9811,
      "step": 63000
    },
    {
      "epoch": 542.7350427350427,
      "grad_norm": 12.58330249786377,
      "learning_rate": 4.4572649572649575e-05,
      "loss": 3.968,
      "step": 63500
    },
    {
      "epoch": 547.008547008547,
      "grad_norm": 7.1367926597595215,
      "learning_rate": 4.452991452991453e-05,
      "loss": 3.9733,
      "step": 64000
    },
    {
      "epoch": 551.2820512820513,
      "grad_norm": 8.501925468444824,
      "learning_rate": 4.448717948717949e-05,
      "loss": 3.9738,
      "step": 64500
    },
    {
      "epoch": 555.5555555555555,
      "grad_norm": 9.262921333312988,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 3.9545,
      "step": 65000
    },
    {
      "epoch": 559.8290598290598,
      "grad_norm": 8.666219711303711,
      "learning_rate": 4.4401709401709404e-05,
      "loss": 3.9604,
      "step": 65500
    },
    {
      "epoch": 564.1025641025641,
      "grad_norm": 9.485916137695312,
      "learning_rate": 4.435897435897436e-05,
      "loss": 3.9651,
      "step": 66000
    },
    {
      "epoch": 568.3760683760684,
      "grad_norm": 8.93874740600586,
      "learning_rate": 4.431623931623932e-05,
      "loss": 3.9588,
      "step": 66500
    },
    {
      "epoch": 572.6495726495726,
      "grad_norm": 7.005195140838623,
      "learning_rate": 4.4273504273504275e-05,
      "loss": 3.963,
      "step": 67000
    },
    {
      "epoch": 576.9230769230769,
      "grad_norm": 6.633715629577637,
      "learning_rate": 4.423076923076923e-05,
      "loss": 3.954,
      "step": 67500
    },
    {
      "epoch": 581.1965811965812,
      "grad_norm": 8.847023963928223,
      "learning_rate": 4.418803418803419e-05,
      "loss": 3.9619,
      "step": 68000
    },
    {
      "epoch": 585.4700854700855,
      "grad_norm": 7.92300271987915,
      "learning_rate": 4.414529914529915e-05,
      "loss": 3.966,
      "step": 68500
    },
    {
      "epoch": 589.7435897435897,
      "grad_norm": 6.521296501159668,
      "learning_rate": 4.4102564102564104e-05,
      "loss": 3.9593,
      "step": 69000
    },
    {
      "epoch": 594.017094017094,
      "grad_norm": 5.531820774078369,
      "learning_rate": 4.405982905982906e-05,
      "loss": 3.9644,
      "step": 69500
    },
    {
      "epoch": 598.2905982905983,
      "grad_norm": 5.972663402557373,
      "learning_rate": 4.401709401709402e-05,
      "loss": 3.9542,
      "step": 70000
    },
    {
      "epoch": 602.5641025641025,
      "grad_norm": 7.20762825012207,
      "learning_rate": 4.3974358974358976e-05,
      "loss": 3.9625,
      "step": 70500
    },
    {
      "epoch": 606.8376068376068,
      "grad_norm": 11.428756713867188,
      "learning_rate": 4.393162393162393e-05,
      "loss": 3.9613,
      "step": 71000
    },
    {
      "epoch": 611.1111111111111,
      "grad_norm": 8.51745319366455,
      "learning_rate": 4.388888888888889e-05,
      "loss": 3.9488,
      "step": 71500
    },
    {
      "epoch": 615.3846153846154,
      "grad_norm": 11.08763599395752,
      "learning_rate": 4.384615384615385e-05,
      "loss": 3.9551,
      "step": 72000
    },
    {
      "epoch": 619.6581196581196,
      "grad_norm": 6.370917797088623,
      "learning_rate": 4.3803418803418805e-05,
      "loss": 3.9657,
      "step": 72500
    },
    {
      "epoch": 623.9316239316239,
      "grad_norm": 10.158863067626953,
      "learning_rate": 4.376068376068376e-05,
      "loss": 3.9511,
      "step": 73000
    },
    {
      "epoch": 628.2051282051282,
      "grad_norm": 7.9173173904418945,
      "learning_rate": 4.371794871794872e-05,
      "loss": 3.9619,
      "step": 73500
    },
    {
      "epoch": 632.4786324786324,
      "grad_norm": 5.766732215881348,
      "learning_rate": 4.3675213675213676e-05,
      "loss": 3.9547,
      "step": 74000
    },
    {
      "epoch": 636.7521367521367,
      "grad_norm": 7.563880920410156,
      "learning_rate": 4.3632478632478634e-05,
      "loss": 3.9538,
      "step": 74500
    },
    {
      "epoch": 641.025641025641,
      "grad_norm": 10.54238224029541,
      "learning_rate": 4.358974358974359e-05,
      "loss": 3.9658,
      "step": 75000
    },
    {
      "epoch": 645.2991452991453,
      "grad_norm": 6.045254707336426,
      "learning_rate": 4.354700854700855e-05,
      "loss": 3.9482,
      "step": 75500
    },
    {
      "epoch": 649.5726495726495,
      "grad_norm": 6.508040904998779,
      "learning_rate": 4.3504273504273505e-05,
      "loss": 3.9636,
      "step": 76000
    },
    {
      "epoch": 653.8461538461538,
      "grad_norm": 8.1851167678833,
      "learning_rate": 4.346153846153846e-05,
      "loss": 3.952,
      "step": 76500
    },
    {
      "epoch": 658.1196581196581,
      "grad_norm": 8.655557632446289,
      "learning_rate": 4.341880341880342e-05,
      "loss": 3.954,
      "step": 77000
    },
    {
      "epoch": 662.3931623931624,
      "grad_norm": 5.89277458190918,
      "learning_rate": 4.337606837606838e-05,
      "loss": 3.9469,
      "step": 77500
    },
    {
      "epoch": 666.6666666666666,
      "grad_norm": 7.219918251037598,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 3.948,
      "step": 78000
    },
    {
      "epoch": 670.9401709401709,
      "grad_norm": 8.60533332824707,
      "learning_rate": 4.329059829059829e-05,
      "loss": 3.9442,
      "step": 78500
    },
    {
      "epoch": 675.2136752136752,
      "grad_norm": 6.112922191619873,
      "learning_rate": 4.324786324786325e-05,
      "loss": 3.9525,
      "step": 79000
    },
    {
      "epoch": 679.4871794871794,
      "grad_norm": 5.988474369049072,
      "learning_rate": 4.320512820512821e-05,
      "loss": 3.9515,
      "step": 79500
    },
    {
      "epoch": 683.7606837606837,
      "grad_norm": 10.35961627960205,
      "learning_rate": 4.316239316239317e-05,
      "loss": 3.945,
      "step": 80000
    },
    {
      "epoch": 688.034188034188,
      "grad_norm": 11.48615550994873,
      "learning_rate": 4.311965811965813e-05,
      "loss": 3.9461,
      "step": 80500
    },
    {
      "epoch": 692.3076923076923,
      "grad_norm": 6.490581512451172,
      "learning_rate": 4.3076923076923084e-05,
      "loss": 3.9405,
      "step": 81000
    },
    {
      "epoch": 696.5811965811965,
      "grad_norm": 8.728429794311523,
      "learning_rate": 4.303418803418804e-05,
      "loss": 3.9497,
      "step": 81500
    },
    {
      "epoch": 700.8547008547008,
      "grad_norm": 10.080997467041016,
      "learning_rate": 4.2991452991453e-05,
      "loss": 3.9505,
      "step": 82000
    },
    {
      "epoch": 705.1282051282051,
      "grad_norm": 6.6495890617370605,
      "learning_rate": 4.294871794871795e-05,
      "loss": 3.9414,
      "step": 82500
    },
    {
      "epoch": 709.4017094017094,
      "grad_norm": 7.373632431030273,
      "learning_rate": 4.2905982905982906e-05,
      "loss": 3.9425,
      "step": 83000
    },
    {
      "epoch": 713.6752136752136,
      "grad_norm": 6.070995330810547,
      "learning_rate": 4.286324786324786e-05,
      "loss": 3.9551,
      "step": 83500
    },
    {
      "epoch": 717.9487179487179,
      "grad_norm": 7.597427845001221,
      "learning_rate": 4.282051282051282e-05,
      "loss": 3.9492,
      "step": 84000
    },
    {
      "epoch": 722.2222222222222,
      "grad_norm": 9.22433090209961,
      "learning_rate": 4.277777777777778e-05,
      "loss": 3.9357,
      "step": 84500
    },
    {
      "epoch": 726.4957264957264,
      "grad_norm": 7.106647968292236,
      "learning_rate": 4.2735042735042735e-05,
      "loss": 3.9463,
      "step": 85000
    },
    {
      "epoch": 730.7692307692307,
      "grad_norm": 7.7301836013793945,
      "learning_rate": 4.269230769230769e-05,
      "loss": 3.9398,
      "step": 85500
    },
    {
      "epoch": 735.042735042735,
      "grad_norm": 9.358436584472656,
      "learning_rate": 4.264957264957265e-05,
      "loss": 3.9362,
      "step": 86000
    },
    {
      "epoch": 739.3162393162393,
      "grad_norm": 8.860991477966309,
      "learning_rate": 4.260683760683761e-05,
      "loss": 3.9381,
      "step": 86500
    },
    {
      "epoch": 743.5897435897435,
      "grad_norm": 7.590705394744873,
      "learning_rate": 4.2564102564102564e-05,
      "loss": 3.9449,
      "step": 87000
    },
    {
      "epoch": 747.8632478632478,
      "grad_norm": 15.584912300109863,
      "learning_rate": 4.252136752136752e-05,
      "loss": 3.9394,
      "step": 87500
    },
    {
      "epoch": 752.1367521367522,
      "grad_norm": 7.095953941345215,
      "learning_rate": 4.247863247863248e-05,
      "loss": 3.9406,
      "step": 88000
    },
    {
      "epoch": 756.4102564102565,
      "grad_norm": 10.528404235839844,
      "learning_rate": 4.2435897435897435e-05,
      "loss": 3.9372,
      "step": 88500
    },
    {
      "epoch": 760.6837606837607,
      "grad_norm": 7.935851573944092,
      "learning_rate": 4.239316239316239e-05,
      "loss": 3.9456,
      "step": 89000
    },
    {
      "epoch": 764.957264957265,
      "grad_norm": 8.081001281738281,
      "learning_rate": 4.235042735042735e-05,
      "loss": 3.937,
      "step": 89500
    },
    {
      "epoch": 769.2307692307693,
      "grad_norm": 5.476089000701904,
      "learning_rate": 4.230769230769231e-05,
      "loss": 3.9425,
      "step": 90000
    },
    {
      "epoch": 773.5042735042736,
      "grad_norm": 6.143497943878174,
      "learning_rate": 4.2264957264957264e-05,
      "loss": 3.9487,
      "step": 90500
    },
    {
      "epoch": 777.7777777777778,
      "grad_norm": 9.346363067626953,
      "learning_rate": 4.222222222222222e-05,
      "loss": 3.9459,
      "step": 91000
    },
    {
      "epoch": 782.0512820512821,
      "grad_norm": 9.733896255493164,
      "learning_rate": 4.217948717948718e-05,
      "loss": 3.9378,
      "step": 91500
    },
    {
      "epoch": 786.3247863247864,
      "grad_norm": 5.446666240692139,
      "learning_rate": 4.2136752136752136e-05,
      "loss": 3.9457,
      "step": 92000
    },
    {
      "epoch": 790.5982905982906,
      "grad_norm": 5.807028293609619,
      "learning_rate": 4.209401709401709e-05,
      "loss": 3.9275,
      "step": 92500
    },
    {
      "epoch": 794.8717948717949,
      "grad_norm": 7.36832857131958,
      "learning_rate": 4.205128205128206e-05,
      "loss": 3.9303,
      "step": 93000
    },
    {
      "epoch": 799.1452991452992,
      "grad_norm": 17.937969207763672,
      "learning_rate": 4.2008547008547014e-05,
      "loss": 3.9299,
      "step": 93500
    },
    {
      "epoch": 803.4188034188035,
      "grad_norm": 9.982585906982422,
      "learning_rate": 4.196581196581197e-05,
      "loss": 3.9394,
      "step": 94000
    },
    {
      "epoch": 807.6923076923077,
      "grad_norm": 8.930482864379883,
      "learning_rate": 4.192307692307693e-05,
      "loss": 3.9449,
      "step": 94500
    },
    {
      "epoch": 811.965811965812,
      "grad_norm": 9.467873573303223,
      "learning_rate": 4.1880341880341886e-05,
      "loss": 3.9318,
      "step": 95000
    },
    {
      "epoch": 816.2393162393163,
      "grad_norm": 6.778914451599121,
      "learning_rate": 4.183760683760684e-05,
      "loss": 3.9374,
      "step": 95500
    },
    {
      "epoch": 820.5128205128206,
      "grad_norm": 14.1344575881958,
      "learning_rate": 4.17948717948718e-05,
      "loss": 3.94,
      "step": 96000
    },
    {
      "epoch": 824.7863247863248,
      "grad_norm": 6.6074748039245605,
      "learning_rate": 4.175213675213676e-05,
      "loss": 3.9337,
      "step": 96500
    },
    {
      "epoch": 829.0598290598291,
      "grad_norm": 9.78016471862793,
      "learning_rate": 4.1709401709401715e-05,
      "loss": 3.942,
      "step": 97000
    },
    {
      "epoch": 833.3333333333334,
      "grad_norm": 9.650123596191406,
      "learning_rate": 4.166666666666667e-05,
      "loss": 3.9254,
      "step": 97500
    },
    {
      "epoch": 837.6068376068376,
      "grad_norm": 10.543852806091309,
      "learning_rate": 4.162393162393163e-05,
      "loss": 3.9298,
      "step": 98000
    },
    {
      "epoch": 841.8803418803419,
      "grad_norm": 5.64805269241333,
      "learning_rate": 4.1581196581196586e-05,
      "loss": 3.9352,
      "step": 98500
    },
    {
      "epoch": 846.1538461538462,
      "grad_norm": 6.631591796875,
      "learning_rate": 4.1538461538461544e-05,
      "loss": 3.9362,
      "step": 99000
    },
    {
      "epoch": 850.4273504273505,
      "grad_norm": 8.008435249328613,
      "learning_rate": 4.14957264957265e-05,
      "loss": 3.9416,
      "step": 99500
    },
    {
      "epoch": 854.7008547008547,
      "grad_norm": 6.436114311218262,
      "learning_rate": 4.145299145299146e-05,
      "loss": 3.9275,
      "step": 100000
    },
    {
      "epoch": 858.974358974359,
      "grad_norm": 8.495221138000488,
      "learning_rate": 4.1410256410256415e-05,
      "loss": 3.9391,
      "step": 100500
    },
    {
      "epoch": 863.2478632478633,
      "grad_norm": 6.968729019165039,
      "learning_rate": 4.1367521367521366e-05,
      "loss": 3.9393,
      "step": 101000
    },
    {
      "epoch": 867.5213675213676,
      "grad_norm": 6.70360803604126,
      "learning_rate": 4.132478632478632e-05,
      "loss": 3.9314,
      "step": 101500
    },
    {
      "epoch": 871.7948717948718,
      "grad_norm": 9.991844177246094,
      "learning_rate": 4.128205128205128e-05,
      "loss": 3.9144,
      "step": 102000
    },
    {
      "epoch": 876.0683760683761,
      "grad_norm": 9.202394485473633,
      "learning_rate": 4.123931623931624e-05,
      "loss": 3.9348,
      "step": 102500
    },
    {
      "epoch": 880.3418803418804,
      "grad_norm": 9.7063570022583,
      "learning_rate": 4.1196581196581195e-05,
      "loss": 3.9393,
      "step": 103000
    },
    {
      "epoch": 884.6153846153846,
      "grad_norm": 20.691463470458984,
      "learning_rate": 4.115384615384615e-05,
      "loss": 3.9284,
      "step": 103500
    },
    {
      "epoch": 888.8888888888889,
      "grad_norm": 7.045592784881592,
      "learning_rate": 4.111111111111111e-05,
      "loss": 3.9315,
      "step": 104000
    },
    {
      "epoch": 893.1623931623932,
      "grad_norm": 9.13902759552002,
      "learning_rate": 4.1068376068376066e-05,
      "loss": 3.9378,
      "step": 104500
    },
    {
      "epoch": 897.4358974358975,
      "grad_norm": 10.567039489746094,
      "learning_rate": 4.1025641025641023e-05,
      "loss": 3.933,
      "step": 105000
    },
    {
      "epoch": 901.7094017094017,
      "grad_norm": 8.040574073791504,
      "learning_rate": 4.098290598290598e-05,
      "loss": 3.9329,
      "step": 105500
    },
    {
      "epoch": 905.982905982906,
      "grad_norm": 9.906173706054688,
      "learning_rate": 4.094017094017094e-05,
      "loss": 3.9327,
      "step": 106000
    },
    {
      "epoch": 910.2564102564103,
      "grad_norm": 10.4525146484375,
      "learning_rate": 4.0897435897435895e-05,
      "loss": 3.9379,
      "step": 106500
    },
    {
      "epoch": 914.5299145299145,
      "grad_norm": 7.194275856018066,
      "learning_rate": 4.085470085470086e-05,
      "loss": 3.926,
      "step": 107000
    },
    {
      "epoch": 918.8034188034188,
      "grad_norm": 6.45619535446167,
      "learning_rate": 4.0811965811965816e-05,
      "loss": 3.9253,
      "step": 107500
    },
    {
      "epoch": 923.0769230769231,
      "grad_norm": 10.631048202514648,
      "learning_rate": 4.0769230769230773e-05,
      "loss": 3.9379,
      "step": 108000
    },
    {
      "epoch": 927.3504273504274,
      "grad_norm": 6.718042373657227,
      "learning_rate": 4.072649572649573e-05,
      "loss": 3.925,
      "step": 108500
    },
    {
      "epoch": 931.6239316239316,
      "grad_norm": 8.417428016662598,
      "learning_rate": 4.068376068376069e-05,
      "loss": 3.9305,
      "step": 109000
    },
    {
      "epoch": 935.8974358974359,
      "grad_norm": 5.9878411293029785,
      "learning_rate": 4.0641025641025645e-05,
      "loss": 3.9342,
      "step": 109500
    },
    {
      "epoch": 940.1709401709402,
      "grad_norm": 7.957812786102295,
      "learning_rate": 4.05982905982906e-05,
      "loss": 3.9112,
      "step": 110000
    },
    {
      "epoch": 944.4444444444445,
      "grad_norm": 8.152853965759277,
      "learning_rate": 4.055555555555556e-05,
      "loss": 3.9275,
      "step": 110500
    },
    {
      "epoch": 948.7179487179487,
      "grad_norm": 6.989166736602783,
      "learning_rate": 4.051282051282052e-05,
      "loss": 3.928,
      "step": 111000
    },
    {
      "epoch": 952.991452991453,
      "grad_norm": 7.953254222869873,
      "learning_rate": 4.0470085470085474e-05,
      "loss": 3.9237,
      "step": 111500
    },
    {
      "epoch": 957.2649572649573,
      "grad_norm": 8.593807220458984,
      "learning_rate": 4.042735042735043e-05,
      "loss": 3.9194,
      "step": 112000
    },
    {
      "epoch": 961.5384615384615,
      "grad_norm": 7.274503707885742,
      "learning_rate": 4.038461538461539e-05,
      "loss": 3.9266,
      "step": 112500
    },
    {
      "epoch": 965.8119658119658,
      "grad_norm": 8.488576889038086,
      "learning_rate": 4.0341880341880346e-05,
      "loss": 3.9132,
      "step": 113000
    },
    {
      "epoch": 970.0854700854701,
      "grad_norm": 7.243028163909912,
      "learning_rate": 4.02991452991453e-05,
      "loss": 3.9279,
      "step": 113500
    },
    {
      "epoch": 974.3589743589744,
      "grad_norm": 6.3631391525268555,
      "learning_rate": 4.025641025641026e-05,
      "loss": 3.9279,
      "step": 114000
    },
    {
      "epoch": 978.6324786324786,
      "grad_norm": 7.6876397132873535,
      "learning_rate": 4.021367521367522e-05,
      "loss": 3.9233,
      "step": 114500
    },
    {
      "epoch": 982.9059829059829,
      "grad_norm": 27.16080093383789,
      "learning_rate": 4.0170940170940174e-05,
      "loss": 3.9221,
      "step": 115000
    },
    {
      "epoch": 987.1794871794872,
      "grad_norm": 7.697185516357422,
      "learning_rate": 4.012820512820513e-05,
      "loss": 3.9254,
      "step": 115500
    },
    {
      "epoch": 991.4529914529915,
      "grad_norm": 13.04598331451416,
      "learning_rate": 4.008547008547009e-05,
      "loss": 3.9139,
      "step": 116000
    },
    {
      "epoch": 995.7264957264957,
      "grad_norm": 7.719341278076172,
      "learning_rate": 4.0042735042735046e-05,
      "loss": 3.9205,
      "step": 116500
    },
    {
      "epoch": 1000.0,
      "grad_norm": 15.483312606811523,
      "learning_rate": 4e-05,
      "loss": 3.9254,
      "step": 117000
    },
    {
      "epoch": 1004.2735042735043,
      "grad_norm": 7.559781074523926,
      "learning_rate": 3.995726495726496e-05,
      "loss": 3.9132,
      "step": 117500
    },
    {
      "epoch": 1008.5470085470085,
      "grad_norm": 6.561147689819336,
      "learning_rate": 3.991452991452992e-05,
      "loss": 3.9305,
      "step": 118000
    }
  ],
  "logging_steps": 500,
  "max_steps": 585000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5000,
  "save_steps": 14800,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5144248350601856e+16,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
