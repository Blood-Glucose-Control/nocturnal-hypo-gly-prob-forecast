data:
  data_split:
  - 0.95
  - 0.05
  dataloader_num_workers: 2
  fewshot_percent: 100
  resolution_min: 5
  source_name: aleppo
  timestamp_column: datetime
  x_features:
  - cob
  - carb_availability
  - insulin_availability
  - iob
  y_feature:
  - bg_mM
evaluation:
  eval_batch_size: 2048
  eval_frequency: 5000
  metrics:
  - mse
experiment:
  description: High performance TTM training with varying batch sizes with Aleppo
    dataset
  name: ttm_aleppo_gpu_optimization
  tags:
  - 100p_count1000_test
  - batch_optimization
  - L40S
  version: '1.0'
hardware:
  expected_gpu_type: L40S
  expected_memory_gb: 48
  mixed_precision: true
model:
  context_length: 512
  forecast_length: 96
  path: ibm-granite/granite-timeseries-ttm-r2
optimization:
  optimizer: adamw
  optimizer_params:
    weight_decay: 0.01
  scheduler: cosine
  scheduler_params:
    T_max: 5
    eta_min: 0.0001
output:
  checkpoint_frequency: 10000
  log_frequency: 2000
  save_best_model: true
  save_checkpoints: true
training:
  batch_size: 64
  learning_rate: 0.001
  loss: mse
  num_epochs: 3
  resume_dir: null
  use_cpu: false
