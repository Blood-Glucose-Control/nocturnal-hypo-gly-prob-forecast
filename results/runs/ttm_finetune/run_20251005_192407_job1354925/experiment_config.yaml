data:
  data_split:
  - 0.9
  - 0.1
  dataloader_num_workers: 2
  fewshot_percent: 100
  resolution_min: 5
  source_name: kaggle_brisT1D
  timestamp_column: datetime
  x_features:
  - steps
  - cob
  - carb_availability
  - insulin_availability
  - iob
  y_feature:
  - bg_mM
evaluation:
  eval_batch_size: 256
  eval_frequency: 1000
  metrics:
  - mse
experiment:
  description: Assessing optimial batch sizes for TTM on L40S GPU with kaggle_brisT1D
    dataset
  name: ttm_batch_size_performance_limits_experiment
  tags:
  - batch_size
  - mixed_precision
  version: '1.0'
hardware:
  expected_gpu_type: L40S
  expected_memory_gb: 48
  mixed_precision: true
model:
  context_length: 512
  forecast_length: 96
  path: ibm-granite/granite-timeseries-ttm-r2
optimization:
  optimizer: adamw
  optimizer_params:
    weight_decay: 0.01
  scheduler: exponential
  scheduler_params:
    gamma: 0.99
output:
  checkpoint_frequency: 1000
  log_frequency: 50
  save_best_model: true
  save_checkpoints: true
training:
  batch_size: 512
  learning_rate: 0.002
  loss: mse
  num_epochs: 10
  resume_dir: null
  use_cpu: false
