{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3df612ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from src.utils.os_helper import get_project_root\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6705542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = get_project_root()\n",
    "\n",
    "CACHE_DIR = repo / \"cache\" / \"data\" / \"awesome_cgm\" / \"aleppo\"\n",
    "data_tables = CACHE_DIR / \"raw\" / \"Data Tables\"\n",
    "db_path = CACHE_DIR / \"awesome_cgm.db\"\n",
    "\n",
    "# SQLite param cap (commonly 999). Keep a margin.\n",
    "SQLITE_MAX_VARS = 999\n",
    "MARGIN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ef861",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(db_path)\n",
    "cur = con.cursor()\n",
    "cur.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "cur.execute(\"PRAGMA synchronous=OFF;\")\n",
    "cur.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "\n",
    "for f in sorted(data_tables.glob(\"*.txt\")):\n",
    "    table = f.stem\n",
    "    print(f\"Importing {f.name} -> {table}\")\n",
    "\n",
    "    # Stream in chunks to control memory and SQL variables\n",
    "    first = True\n",
    "    for df in pd.read_csv(f, sep=\"|\", dtype=str, low_memory=False, chunksize=50_000):\n",
    "        ncols = len(df.columns)\n",
    "        # rows per insert so (rows * cols) <= SQLITE_MAX_VARS - MARGIN\n",
    "        safe_rows = max(1, (SQLITE_MAX_VARS - MARGIN) // max(1, ncols))\n",
    "\n",
    "        # method=None avoids multi-row SQL text construction; Pandas will executemany\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con,\n",
    "            if_exists=\"replace\" if first else \"append\",\n",
    "            index=False,\n",
    "            chunksize=safe_rows,\n",
    "            method=None,\n",
    "        )\n",
    "        first = False\n",
    "\n",
    "\n",
    "indexes = [\n",
    "    # Bolus Indexes\n",
    "    \"CREATE INDEX idx_hdevicebolus_ptid ON HDeviceBolus(PtID);\",\n",
    "    \"CREATE INDEX idx_hdevicebolus_parentid ON HDeviceBolus(ParentHDeviceUploadsID);\",\n",
    "    \"CREATE INDEX idx_hdevicebolus_ptid_days ON HDeviceBolus(PtID, DeviceDtTmDaysFromEnroll);\",\n",
    "    \"CREATE INDEX idx_hdevicebolus_normal ON HDeviceBolus(Normal) WHERE Normal IS NOT NULL;\",\n",
    "    \"CREATE INDEX idx_hdevicebolus_order ON HDeviceBolus(PtID, DeviceDtTmDaysFromEnroll, DeviceTm);\",\n",
    "    # Uploads Indexes\n",
    "    \"CREATE INDEX idx_hdeviceuploads_recid ON HDeviceUploads(RecID);\",\n",
    "    \"CREATE INDEX idx_hdeviceuploads_ptid ON HDeviceUploads(PtID);\",\n",
    "    \"CREATE INDEX idx_hdeviceuploads_device ON HDeviceUploads(DeviceModel, DeviceType);\",\n",
    "    # CGM Indexes\n",
    "    \"CREATE INDEX idx_hdevicecgm_recid ON HDeviceCGM(RecID);\",\n",
    "    \"CREATE INDEX idx_hdevicecgm_recordtype ON HDeviceCGM(RecordType);\",\n",
    "    \"CREATE INDEX idx_hdevicecgm_ptid_days ON HDeviceCGM(PtID, DeviceDtTmDaysFromEnroll);\",\n",
    "    \"CREATE INDEX idx_hdevicecgm_ptid_time ON HDeviceCGM(PtID, DeviceTm);\",\n",
    "    # Wizard Indexes\n",
    "    \"CREATE INDEX idx_hdevicewizard_recid ON HDeviceWizard(RecID);\",\n",
    "    \"CREATE INDEX idx_hdevicewizard_ptid ON HDeviceWizard(PtID);\",\n",
    "    \"CREATE INDEX idx_hdevicewizard_ptid_days ON HDeviceWizard(PtID, DeviceDtTmDaysFromEnroll);\",\n",
    "    \"CREATE INDEX idx_hdevicewizard_ptid_time ON HDeviceWizard(PtID, DeviceTm);\",\n",
    "]\n",
    "\n",
    "for i, index_sql in enumerate(indexes):\n",
    "    try:\n",
    "        cur.execute(index_sql)\n",
    "        print(\n",
    "            f\"Created index {i+1}/{len(indexes)}: {index_sql.split('ON ')[1].split('(')[0]}\"\n",
    "        )\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error creating index: {e}\")\n",
    "\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(query: str):\n",
    "    # Open the connection first\n",
    "    con = sqlite3.connect(db_path)\n",
    "    # cur = con.cursor()\n",
    "    df = pd.read_sql_query(query, con)\n",
    "    con.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37ac29",
   "metadata": {},
   "source": [
    "### Convert data to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30c080",
   "metadata": {},
   "source": [
    "- pid\n",
    "- date\n",
    "- tableType\n",
    "- bolusType\n",
    "- normalBolus\n",
    "- expectedNormalBolus\n",
    "- extendedBolus\n",
    "- expectedExtendedBolus\n",
    "- bgInput\n",
    "- foodG\n",
    "- iob\n",
    "- cr\n",
    "- isf\n",
    "- bgMgdl\n",
    "- rate\n",
    "- suprBasalType\n",
    "- suprRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f649c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH\n",
    "  params AS (\n",
    "    SELECT '2020-01-01' AS base_date\n",
    "  )\n",
    "SELECT\n",
    "    pid, date, tableType,\n",
    "    bolusType, normalBolus, expectedNormalBolus, extendedBolus, expectedExtendedBolus,\n",
    "    bgInput, foodG, iob, cr, isf,\n",
    "    bgMgdl,\n",
    "    rate, suprBasalType, suprRate\n",
    "FROM (\n",
    "    -- Bolus data\n",
    "    SELECT\n",
    "        HDeviceBolus.PtID as pid,\n",
    "        datetime(julianday((SELECT base_date FROM params)) + CAST(HDeviceBolus.DeviceDtTmDaysFromEnroll AS INTEGER) + (julianday(HDeviceBolus.DeviceTm) - julianday('00:00:00'))) AS date,\n",
    "        'bolus' as tableType,\n",
    "        HDeviceBolus.BolusType as bolusType,\n",
    "        Normal as normalBolus,\n",
    "        ExpectedNormal as expectedNormalBolus,\n",
    "        Extended as extendedBolus,\n",
    "        ExpectedExtended as expectedExtendedBolus,\n",
    "        NULL as bgInput,\n",
    "        NULL as foodG,\n",
    "        NULL as iob,\n",
    "        NULL as cr,\n",
    "        NULL as isf,\n",
    "        NULL as bgMgdl,\n",
    "        NULL as rate,\n",
    "        NULL as suprBasalType,\n",
    "        NULL as suprRate\n",
    "    FROM HDeviceBolus\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Wizard data\n",
    "    SELECT\n",
    "        HDeviceWizard.PtID as pid,\n",
    "        datetime(julianday((SELECT base_date FROM params)) + CAST(HDeviceWizard.DeviceDtTmDaysFromEnroll AS INTEGER) + (julianday(HDeviceWizard.DeviceTm) - julianday('00:00:00'))) AS date,\n",
    "        'wizard' as tableType,\n",
    "        NULL as bolusType,\n",
    "        NULL as normalBolus,\n",
    "        NULL as expectedNormalBolus,\n",
    "        NULL as extendedBolus,\n",
    "        NULL as expectedExtendedBolus,\n",
    "        HDeviceWizard.BgInput as bgInput,\n",
    "        HDeviceWizard.CarbInput / 1000 as foodG,\n",
    "        HDeviceWizard.InsulinOnBoard as iob,\n",
    "        HDeviceWizard.InsulinCarbRatio as cr,\n",
    "        HDeviceWizard.InsulinSensitivity as isf,\n",
    "        NULL as bgMgdl,\n",
    "        NULL as rate,\n",
    "        NULL as suprBasalType,\n",
    "        NULL as suprRate\n",
    "    FROM HDeviceWizard\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- CGM data\n",
    "    SELECT\n",
    "        HDeviceCGM.PtID as pid,\n",
    "        datetime(julianday((SELECT base_date FROM params)) + CAST(HDeviceCGM.DeviceDtTmDaysFromEnroll AS INTEGER) + (julianday(HDeviceCGM.DeviceTm) - julianday('00:00:00'))) AS date,\n",
    "        'cgm' as tableType,\n",
    "        NULL as bolusType,\n",
    "        NULL as normalBolus,\n",
    "        NULL as expectedNormalBolus,\n",
    "        NULL as extendedBolus,\n",
    "        NULL as expectedExtendedBolus,\n",
    "        NULL as bgInput,\n",
    "        NULL as foodG,\n",
    "        NULL as iob,\n",
    "        NULL as cr,\n",
    "        NULL as isf,\n",
    "        HDeviceCGM.GlucoseValue as bgMgdl,\n",
    "        NULL as rate,\n",
    "        NULL as suprBasalType,\n",
    "        NULL as suprRate\n",
    "    FROM HDeviceCGM\n",
    "    WHERE HDeviceCGM.GlucoseValue IS NOT NULL\n",
    "    AND HDeviceCGM.RecordType = 'CGM'\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Basal data\n",
    "    SELECT\n",
    "        HDeviceBasal.PtID as pid,\n",
    "        datetime(julianday((SELECT base_date FROM params)) + CAST(HDeviceBasal.DeviceDtTmDaysFromEnroll AS INTEGER) + (julianday(HDeviceBasal.DeviceTm) - julianday('00:00:00'))) AS date,\n",
    "        'basal' as tableType,\n",
    "        NULL as bolusType,\n",
    "        NULL as normalBolus,\n",
    "        NULL as expectedNormalBolus,\n",
    "        NULL as extendedBolus,\n",
    "        NULL as expectedExtendedBolus,\n",
    "        NULL as bgInput,\n",
    "        NULL as foodG,\n",
    "        NULL as iob,\n",
    "        NULL as cr,\n",
    "        NULL as isf,\n",
    "        NULL as bgMgdl,\n",
    "        HDeviceBasal.Rate as rate,\n",
    "        HDeviceBasal.SuprBasalType as suprBasalType,\n",
    "        HDeviceBasal.SuprRate as suprRate\n",
    "    FROM HDeviceBasal\n",
    ")\n",
    "ORDER BY pid, date ASC;\n",
    "\"\"\"\n",
    "df_all = query_db(query)\n",
    "df_all.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27057ae",
   "metadata": {},
   "source": [
    "### Raw -> Interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c73035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing pid 10\n",
      "Done processing pid 101\n",
      "Done processing pid 102\n",
      "Done processing pid 103\n",
      "Done processing pid 105\n",
      "Done processing pid 106\n",
      "Done processing pid 108\n",
      "Done processing pid 109\n",
      "Done processing pid 11\n",
      "Done processing pid 110\n",
      "Done processing pid 111\n",
      "Done processing pid 112\n",
      "Done processing pid 113\n",
      "Done processing pid 115\n",
      "Done processing pid 116\n",
      "Done processing pid 118\n",
      "Done processing pid 119\n",
      "Done processing pid 121\n",
      "Done processing pid 123\n",
      "Done processing pid 124\n",
      "Done processing pid 127\n",
      "Done processing pid 128\n",
      "Done processing pid 129\n",
      "Done processing pid 130\n",
      "Done processing pid 131\n",
      "Done processing pid 132\n",
      "Done processing pid 134\n",
      "Done processing pid 135\n",
      "Done processing pid 136\n",
      "Done processing pid 137\n",
      "Done processing pid 138\n",
      "Done processing pid 139\n",
      "Done processing pid 14\n",
      "Done processing pid 140\n",
      "Done processing pid 141\n",
      "Done processing pid 143\n",
      "Done processing pid 145\n",
      "Done processing pid 146\n",
      "Done processing pid 147\n",
      "Done processing pid 148\n",
      "Done processing pid 149\n",
      "Done processing pid 15\n",
      "Done processing pid 152\n",
      "Done processing pid 155\n",
      "Done processing pid 156\n",
      "Done processing pid 157\n",
      "Done processing pid 158\n",
      "Done processing pid 16\n",
      "Done processing pid 160\n",
      "Done processing pid 162\n",
      "Done processing pid 163\n",
      "Done processing pid 164\n",
      "Done processing pid 165\n",
      "Done processing pid 166\n",
      "Done processing pid 167\n",
      "Done processing pid 168\n",
      "Done processing pid 169\n",
      "Done processing pid 17\n",
      "Done processing pid 170\n",
      "Done processing pid 171\n",
      "Done processing pid 172\n",
      "Done processing pid 173\n",
      "Done processing pid 174\n",
      "Done processing pid 175\n",
      "Done processing pid 176\n",
      "Done processing pid 177\n",
      "Done processing pid 179\n",
      "Done processing pid 18\n",
      "Done processing pid 181\n",
      "Done processing pid 183\n",
      "Done processing pid 184\n",
      "Done processing pid 185\n",
      "Done processing pid 186\n",
      "Done processing pid 187\n",
      "Done processing pid 188\n",
      "Done processing pid 189\n",
      "Done processing pid 19\n",
      "Done processing pid 190\n",
      "Done processing pid 193\n",
      "Done processing pid 197\n",
      "Done processing pid 198\n",
      "Done processing pid 2\n",
      "Done processing pid 20\n",
      "Done processing pid 200\n",
      "Done processing pid 201\n",
      "Done processing pid 203\n",
      "Done processing pid 204\n",
      "Done processing pid 205\n",
      "Done processing pid 206\n",
      "Done processing pid 209\n",
      "Done processing pid 21\n",
      "Done processing pid 210\n",
      "Done processing pid 211\n",
      "Done processing pid 213\n",
      "Done processing pid 214\n",
      "Done processing pid 215\n",
      "Done processing pid 216\n",
      "Done processing pid 217\n",
      "Done processing pid 218\n",
      "Done processing pid 219\n",
      "Done processing pid 22\n",
      "Done processing pid 220\n",
      "Done processing pid 221\n",
      "Done processing pid 222\n",
      "Done processing pid 223\n",
      "Done processing pid 224\n",
      "Done processing pid 226\n",
      "Done processing pid 227\n",
      "Done processing pid 228\n",
      "Done processing pid 229\n",
      "Done processing pid 23\n",
      "Done processing pid 231\n",
      "Done processing pid 232\n",
      "Done processing pid 233\n",
      "Done processing pid 234\n",
      "Done processing pid 235\n",
      "Done processing pid 236\n",
      "Done processing pid 239\n",
      "Done processing pid 24\n",
      "Done processing pid 240\n",
      "Done processing pid 241\n",
      "Done processing pid 243\n",
      "Done processing pid 244\n",
      "Done processing pid 245\n",
      "Done processing pid 246\n",
      "Done processing pid 247\n",
      "Done processing pid 248\n",
      "Done processing pid 249\n",
      "Done processing pid 250\n",
      "Done processing pid 251\n",
      "Done processing pid 252\n",
      "Done processing pid 253\n",
      "Done processing pid 254\n",
      "Done processing pid 256\n",
      "Done processing pid 257\n",
      "Done processing pid 258\n",
      "Done processing pid 26\n",
      "Done processing pid 260\n",
      "Done processing pid 263\n",
      "Done processing pid 264\n",
      "Done processing pid 265\n",
      "Done processing pid 266\n",
      "Done processing pid 267\n",
      "Done processing pid 269\n",
      "Done processing pid 27\n",
      "Done processing pid 271\n",
      "Done processing pid 272\n",
      "Done processing pid 273\n",
      "Done processing pid 274\n",
      "Done processing pid 275\n",
      "Done processing pid 276\n",
      "Done processing pid 277\n",
      "Done processing pid 278\n",
      "Done processing pid 280\n",
      "Done processing pid 281\n",
      "Done processing pid 283\n",
      "Done processing pid 284\n",
      "Done processing pid 285\n",
      "Done processing pid 287\n",
      "Done processing pid 288\n",
      "Done processing pid 289\n",
      "Done processing pid 29\n",
      "Done processing pid 290\n",
      "Done processing pid 291\n",
      "Done processing pid 292\n",
      "Done processing pid 293\n",
      "Done processing pid 3\n",
      "Done processing pid 30\n",
      "Done processing pid 31\n",
      "Done processing pid 32\n",
      "Done processing pid 33\n",
      "Done processing pid 35\n",
      "Done processing pid 36\n",
      "Done processing pid 37\n",
      "Done processing pid 38\n",
      "Done processing pid 39\n",
      "Done processing pid 40\n",
      "Done processing pid 41\n",
      "Done processing pid 42\n",
      "Done processing pid 43\n",
      "Done processing pid 45\n",
      "Done processing pid 46\n",
      "Done processing pid 47\n",
      "Done processing pid 48\n",
      "Done processing pid 49\n",
      "Done processing pid 5\n",
      "Done processing pid 50\n",
      "Done processing pid 52\n",
      "Done processing pid 53\n",
      "Done processing pid 54\n",
      "Done processing pid 55\n",
      "Done processing pid 57\n",
      "Done processing pid 58\n",
      "Done processing pid 60\n",
      "Done processing pid 61\n",
      "Done processing pid 62\n",
      "Done processing pid 64\n",
      "Done processing pid 65\n",
      "Done processing pid 67\n",
      "Done processing pid 68\n",
      "Done processing pid 69\n",
      "Done processing pid 7\n",
      "Done processing pid 70\n",
      "Done processing pid 71\n",
      "Done processing pid 72\n",
      "Done processing pid 73\n",
      "Done processing pid 74\n",
      "Done processing pid 76\n",
      "Done processing pid 77\n",
      "Done processing pid 78\n",
      "Done processing pid 79\n",
      "Done processing pid 8\n",
      "Done processing pid 80\n",
      "Done processing pid 81\n",
      "Done processing pid 82\n",
      "Done processing pid 86\n",
      "Done processing pid 87\n",
      "Done processing pid 89\n",
      "Done processing pid 9\n",
      "Done processing pid 90\n",
      "Done processing pid 91\n",
      "Done processing pid 93\n",
      "Done processing pid 95\n",
      "Done processing pid 96\n",
      "Done processing pid 97\n",
      "Done processing pid 98\n"
     ]
    }
   ],
   "source": [
    "def convert_to_csv(df):\n",
    "    project_root = get_project_root()\n",
    "    data_dir = project_root / \"cache\" / \"data\" / \"awesome_cgm\" / \"aleppo\" / \"interim\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    for pid in df[\"pid\"].unique():\n",
    "        df_pid = df[df[\"pid\"] == pid]\n",
    "        df_pid.to_csv(data_dir / f\"p{pid}_full.csv\", index=False)\n",
    "        print(f\"Done processing pid {pid}\")\n",
    "\n",
    "\n",
    "convert_to_csv(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5dfaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.models import ColumnNames\n",
    "from src.data.preprocessing.pipeline import preprocessing_pipeline\n",
    "from src.utils.unit import mg_dl_to_mmol_l\n",
    "\n",
    "\n",
    "# \tpid\tdate\ttableType\tbolusType\tnormalBolus\texpectedNormalBolus\textendedBolus\texpectedExtendedBolus\tbgInput\tfoodG\tiob\tcr\tisf\tbgMgdl\trate\tsuprBasalType\tsuprRate\n",
    "def data_translation(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    'pid' -> p_num\n",
    "    'date' -> datetime\n",
    "    'tableType' -> msg_type (bolus, wizard, cgm). wizard contains information like carbs intake\n",
    "    'eventType': This is bolus type\n",
    "    'normal': Number of units of normal bolus\n",
    "    'expectedNormal'\n",
    "    'extended': Number of units for extended delivery\n",
    "    'expectedExtended'\n",
    "    'bgInput' -> Blood glucose as inputted into wizard in mg\n",
    "    'carbInput' -> Carbohydrates as inputted into wizard in mg\n",
    "    'iob': Units of insulin on board\n",
    "    'cr': Number of mg carbs covered by unit of insulin. Not used yet but we can find a way to give model a hint about the slope of the glucose curve\n",
    "    'isf': Number of bgs covered by unit of insulin. Same as above\n",
    "    'recordType': CGM | CALIBRATION\n",
    "    'glucoseValue' -> bg_mM\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    # TODO: Rename to the correct column names\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"pid\": ColumnNames.P_NUM.value,\n",
    "            \"date\": ColumnNames.DATETIME.value,\n",
    "            \"tableType\": ColumnNames.MSG_TYPE.value,\n",
    "            \"normalBolus\": ColumnNames.DOSE_UNITS.value,\n",
    "            # \"expectedNormalBolus\": ColumnNames.EXPECTED_NORMAL.value,\n",
    "            # \"extendedBolus\": ColumnNames.EXTENDED.value,\n",
    "            # \"expectedExtendedBolus\": ColumnNames.EXPECTED_EXTENDED.value,\n",
    "            # \"bgInput\": ColumnNames.BG.value, todo: Maybe tag the record type as bgInput\n",
    "            \"foodG\": ColumnNames.FOOD_G.value,\n",
    "            \"iob\": ColumnNames.IOB.value,\n",
    "            # \"cr\": ColumnNames.INSULIN_CARB_RATIO.value,\n",
    "            # \"isf\": ColumnNames.ISF.value,\n",
    "            \"recordType\": ColumnNames.RECORD_TYPE.value,\n",
    "            \"bgMgdl\": ColumnNames.BG.value,\n",
    "            \"rate\": ColumnNames.RATE.value,\n",
    "            \"suprBasalType\": ColumnNames.SUPR_BASAL_TYPE.value,\n",
    "            \"suprRate\": ColumnNames.SUPR_RATE.value,\n",
    "        }\n",
    "    )\n",
    "    df.drop(columns=[\"expectedNormalBolus\", \"expectedExtendedBolus\"], inplace=True)\n",
    "    df.set_index(ColumnNames.DATETIME.value, inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Convert blood glucose from mg/dL to mmol/L\n",
    "    # TODO: Add this back in\n",
    "    df[ColumnNames.BG.value] = mg_dl_to_mmol_l(df, bgl_col=ColumnNames.BG.value)\n",
    "\n",
    "    # Convert carbs from mg to g\n",
    "    df[ColumnNames.FOOD_G.value] = df[ColumnNames.FOOD_G.value].astype(float) / 1000\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def keep_overlapping_data(patient_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        patient_df: A dataframe that has been through data_translation (datetime is the index)\n",
    "    Keep data that has overlapping time windows for all table types.\n",
    "    Note that not all patients have data for all table types so we only consider the table types that are present.\n",
    "    \"\"\"\n",
    "    table_types = patient_df[ColumnNames.MSG_TYPE.value].unique()\n",
    "    start_datetime = None  # This should be the max of all the min datetimes\n",
    "    end_datetime = None  # This should be the min of all the max datetimes\n",
    "\n",
    "    for table_type in table_types:\n",
    "        table_data = patient_df[patient_df[ColumnNames.MSG_TYPE.value] == table_type]\n",
    "        if table_data.empty:\n",
    "            continue\n",
    "\n",
    "        min_datetime = table_data.index.min()\n",
    "        max_datetime = table_data.index.max()\n",
    "\n",
    "        if start_datetime is None or min_datetime > start_datetime:\n",
    "            start_datetime = min_datetime\n",
    "        if end_datetime is None or max_datetime < end_datetime:\n",
    "            end_datetime = max_datetime\n",
    "\n",
    "    if start_datetime is None or end_datetime is None:\n",
    "        return None\n",
    "\n",
    "    has_overlap = start_datetime < end_datetime\n",
    "    if not has_overlap:\n",
    "        return None\n",
    "\n",
    "    # Filter using the index (datetime)\n",
    "    return patient_df[\n",
    "        (patient_df.index >= start_datetime) & (patient_df.index <= end_datetime)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_patient(\n",
    "    df_raw: pd.DataFrame,\n",
    "    to_csv: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the raw data for one patient:\n",
    "    1. Translate the data (columns and units)\n",
    "    2. Keep overlapping data (for bolus, wizard, cgm and basal)\n",
    "    3. TODO: Convert basal rate to 5-min interval\n",
    "    4. preprocessing_pipeline (This include resampling and deriving cob and iob)\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Translate the data\n",
    "    df = data_translation(df)\n",
    "    pid = df[ColumnNames.P_NUM.value].iloc[0]\n",
    "\n",
    "    # Keep overlapping data\n",
    "    df = keep_overlapping_data(df)\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    # Print data meta: span of datetime index and number of rows\n",
    "    start_dt = df.index.min()\n",
    "    end_dt = df.index.max()\n",
    "    print(\n",
    "        f\"Patient {pid} processed data spans from {start_dt} to {end_dt} ({(end_dt - start_dt)})\"\n",
    "    )\n",
    "\n",
    "    food_g = df[df[ColumnNames.FOOD_G.value].notna()]\n",
    "    print(f\"Number of rows with food intake: {len(food_g)}\")\n",
    "\n",
    "    bolus = df[df[ColumnNames.DOSE_UNITS.value].notna()]\n",
    "    print(f\"Number of rows with bolus: {len(bolus)}\")\n",
    "\n",
    "    # Let the pipeline handle the rest\n",
    "    # This derives iob and cob which we is information we already have\n",
    "    df = preprocessing_pipeline(pid, df)\n",
    "    # Debug only\n",
    "    if to_csv:\n",
    "        debug_dir = repo / \"cache\" / \"data\" / \"awesome_cgm\" / \"aleppo\" / \"debug\"\n",
    "        os.makedirs(debug_dir, exist_ok=True)\n",
    "        df.to_csv(debug_dir / f\"p{pid}.csv\", index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_all_patients(\n",
    "    interim_path: Path,\n",
    "    processed_path: Path,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean all patients' data in the interim path and save the processed data to the processed path.\n",
    "    \"\"\"\n",
    "    for pid in os.listdir(interim_path):\n",
    "        df = pd.read_csv(interim_path / pid)\n",
    "        df = process_one_patient(df)\n",
    "        df.to_csv(processed_path / pid, index=True)\n",
    "        print(f\"{\"-\"*10}Done processing pid {pid} {\"-\"*10}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418bb74",
   "metadata": {},
   "source": [
    "### Interim -> Processed for one patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0781bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 7 original data spans from 2019-08-17 14:32:00 to 2020-09-30 10:14:30\n",
      "Patient 7 processed data spans from 2019-11-02 09:19:27 to 2020-09-30 08:58:17 (332 days 23:38:50)\n",
      "Number of rows with food intake: 1651\n",
      "Number of rows with bolus: 1648\n"
     ]
    }
   ],
   "source": [
    "pid_check = 7\n",
    "root = get_project_root()\n",
    "patient_df = pd.read_csv(\n",
    "    f\"{root}/cache/data/awesome_cgm/aleppo/interim/p{pid_check}_full.csv\"\n",
    ")\n",
    "\n",
    "start_time = patient_df.date.min()\n",
    "end_time = patient_df.date.max()\n",
    "print(f\"Patient {pid_check} original data spans from {start_time} to {end_time}\")\n",
    "processed_df = process_one_patient(patient_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0440df3",
   "metadata": {},
   "source": [
    "### Process all patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_path = repo / \"cache\" / \"data\" / \"awesome_cgm\" / \"aleppo\" / \"interim\"\n",
    "processed_path = repo / \"cache\" / \"data\" / \"awesome_cgm\" / \"aleppo\" / \"processed\"\n",
    "\n",
    "\n",
    "# TODO: Double check this. This is too fast to be true.\n",
    "process_all_patients(interim_path, processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0eeee6",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- [x] Figure out why we are missing some patients like p04 - There is no reason\n",
    "- [x] Verify db is created correctly in the right directory\n",
    "- [x] Figure out the columns of the tables and make sure the unit is actually correct. Is CarbInput in g and \"normal\" in units at all?\n",
    "- [ ] Plot a histogram to see if there are significantly less data before the enrollment day.\n",
    "- [ ] Figure out how to handle missing data (Some patients have missing cgm data). - Need to filter out calibaration. That seems to remove some of the large gap\n",
    "- [ ] Union with Basal table too (the rate should be roll to the next few rows. For example, a patient with 5-min interval has a rate of 0.1 units/hour, then we should have a 12 rows of 0.1/12 unit each). Also need to do a addition with whatever bolus is already there. So if a row already has 8 units of insulin, then it will be 8 + 0.1/12 units. Also need to keep rate as 0 if null.\n",
    "- [ ] Take a look at some patients' raw data and see if it makes sense at all first\n",
    "\n",
    "\n",
    "## Loader\n",
    "- [ ] Add a test split for the loader (5% of the entire dataset is enough)\n",
    "- [ ] Verify the `clean_all_patients` is working\n",
    "- [ ] Verify that `data_loader` works at all\n",
    "- [ ] For the loader class, cross check with Kaggle to do a similar data check\n",
    "- [ ] Update `README.md`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1aedc9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".noctprob-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
