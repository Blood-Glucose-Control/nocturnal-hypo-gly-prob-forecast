{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_loader import load_data\n",
    "from src.data.data_cleaner import clean_data\n",
    "from src.data.data_transforms import create_time_diff_cols\n",
    "\n",
    "df = clean_data(load_data())\n",
    "df = create_time_diff_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just tune the hyperparams on the first 100 timepoints for patient 1 since we're just testing\n",
    "y = df.iloc[:100][\n",
    "    [\"bg-0:00\", \"insulin-0:00\"]\n",
    "]  # Only want to test the endogenous data right now\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "# Impute missing values\n",
    "transformer = Imputer(method=\"nearest\")\n",
    "transformer.fit(y)\n",
    "y = transformer.transform(y)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should return none\n",
    "y[y.isna()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tuning.load_forecasters import load_all_forecasters\n",
    "\n",
    "forecasters = load_all_forecasters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV\n",
    "from sktime.split import ExpandingSlidingWindowSplitter\n",
    "from sktime.performance_metrics.forecasting import MeanSquaredError\n",
    "\n",
    "fh = [1, 2, 3, 4, 5, 6]\n",
    "cv = ExpandingSlidingWindowSplitter(\n",
    "    fh=fh, initial_window=12, step_length=12, max_expanding_window_length=24 * 12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config_loader import load_yaml_config\n",
    "\n",
    "# Change below to the path of your config file\n",
    "config = load_yaml_config(\"../../src/tuning/configs/modset1.yaml\")\n",
    "\n",
    "# Specify the model names you want to test params for as labelled in the config\n",
    "# The names should match their sktime class names\n",
    "models = [\"ARIMA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from src.tuning.param_grid import generate_param_grid\n",
    "from src.tuning.load_forecasters import get_estimator\n",
    "\n",
    "# Keep track of all models tested and log any errors\n",
    "tested_models = []\n",
    "\n",
    "# Loop through all listed models and tune params for the first day of data\n",
    "for model in models:\n",
    "    forecaster = get_estimator(forecasters, model)\n",
    "    params = generate_param_grid(model, config)\n",
    "\n",
    "    param_grid = params\n",
    "    gscv = ForecastingGridSearchCV(\n",
    "        forecaster=forecaster(),\n",
    "        # Simplify the dictionary so only one set of values are tested\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=MeanSquaredError(square_root=True),\n",
    "        # Raise errors so we can see what params are causing errors\n",
    "        error_score=\"raise\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        gscv.fit(y[[\"bg-0:00\"]], X=y[[\"insulin-0:00\"]])\n",
    "        tested_models.append(\n",
    "            {\n",
    "                \"model_name\": model,\n",
    "                \"parameters\": str(gscv.best_params_),\n",
    "                \"status\": \"Pass\",\n",
    "                \"datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error\": \"\",\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        tested_models.append(\n",
    "            {\n",
    "                \"model_name\": model,\n",
    "                \"parameters\": str(params),\n",
    "                \"status\": \"Fail\",\n",
    "                \"datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "\n",
    "pd.DataFrame(tested_models).to_csv(\"tuning_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".noctprob-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
