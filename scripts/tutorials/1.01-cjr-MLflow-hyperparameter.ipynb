{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094cb6f8",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning & Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2c8d3",
   "metadata": {},
   "source": [
    "Run this in your terminal (change the port number to whatever you're using):\n",
    "\n",
    "```export MLFLOW_TRACKING_URI=http://localhost:5556```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47338fd5",
   "metadata": {},
   "source": [
    "The Challenge: Wine Quality Prediction\n",
    "\n",
    "We'll optimize a neural network that predicts wine quality from chemical properties. Our goal is to minimize Root Mean Square Error (RMSE) by finding the optimal combination of:\n",
    "\n",
    "    - Learning Rate: How aggressively the model learns\n",
    "    - Momentum: How much the optimizer considers previous updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867362c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5556\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3238c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 16:10:57.030959: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-17 16:10:57.045784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758125457.060015 2244647 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758125457.064313 2244647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758125457.076341 2244647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758125457.076362 2244647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758125457.076365 2244647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758125457.076366 2244647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-17 16:10:57.081008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# Load the wine quality dataset\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "\n",
    "# Create train/validation/test splits\n",
    "train, test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_x = train.drop([\"quality\"], axis=1).values\n",
    "train_y = train[[\"quality\"]].values.ravel()\n",
    "test_x = test.drop([\"quality\"], axis=1).values\n",
    "test_y = test[[\"quality\"]].values.ravel()\n",
    "\n",
    "# Further split training data for validation\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create model signature for deployment\n",
    "signature = infer_signature(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66fa7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(learning_rate, momentum, epochs=10):\n",
    "    \"\"\"\n",
    "    Create and train a neural network with specified hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training results including model and metrics\n",
    "    \"\"\"\n",
    "    # Normalize input features for better training stability\n",
    "    mean = np.mean(train_x, axis=0)\n",
    "    var = np.var(train_x, axis=0)\n",
    "\n",
    "    # Define model architecture\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input([train_x.shape[1]]),\n",
    "            keras.layers.Normalization(mean=mean, variance=var),\n",
    "            keras.layers.Dense(64, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.2),  # Add regularization\n",
    "            keras.layers.Dense(32, activation=\"relu\"),\n",
    "            keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile with specified hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "\n",
    "    # Train with early stopping for efficiency\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        patience=3, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        validation_data=(valid_x, valid_y),\n",
    "        epochs=epochs,\n",
    "        batch_size=64,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0,  # Reduce output for cleaner logs\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"val_rmse\": val_rmse,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"history\": history,\n",
    "        \"epochs_trained\": len(history.history[\"loss\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970ce1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space defined:\n",
      "- Learning rate: 1e-5 to 1e-1 (log-uniform)\n",
      "- Momentum: 0.0 to 0.9 (uniform)\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization.\n",
    "    This function will be called by Hyperopt for each trial.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Log hyperparameters being tested\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"learning_rate\": params[\"learning_rate\"],\n",
    "                \"momentum\": params[\"momentum\"],\n",
    "                \"optimizer\": \"SGD\",\n",
    "                \"architecture\": \"64-32-1\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Train model with current hyperparameters\n",
    "        result = create_and_train_model(\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            momentum=params[\"momentum\"],\n",
    "            epochs=15,\n",
    "        )\n",
    "\n",
    "        # Log training results\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                \"val_rmse\": result[\"val_rmse\"],\n",
    "                \"val_loss\": result[\"val_loss\"],\n",
    "                \"epochs_trained\": result[\"epochs_trained\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Log the trained model\n",
    "        mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n",
    "\n",
    "        # Log training curves as artifacts\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n",
    "        plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n",
    "        plt.title(\"Model Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(\n",
    "            result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n",
    "        )\n",
    "        plt.plot(\n",
    "            result[\"history\"].history[\"val_root_mean_squared_error\"],\n",
    "            label=\"Validation RMSE\",\n",
    "        )\n",
    "        plt.title(\"Model RMSE\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"training_curves.png\")\n",
    "        mlflow.log_artifact(\"training_curves.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Return loss for Hyperopt (it minimizes)\n",
    "        return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "# Define search space for hyperparameters\n",
    "search_space = {\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(1e-1)),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.0, 0.9),\n",
    "}\n",
    "\n",
    "print(\"Search space defined:\")\n",
    "print(\"- Learning rate: 1e-5 to 1e-1 (log-uniform)\")\n",
    "print(\"- Momentum: 0.0 to 0.9 (uniform)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf814c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/17 16:11:00 INFO mlflow.tracking.fluent: Experiment with name 'wine-quality-optimization' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization experiment: wine-quality-optimization\n",
      "This will run 15 trials to find optimal hyperparameters...\n",
      "  0%|          | 0/15 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 16:11:00.694634: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run bemused-croc-327 at: http://127.0.0.1:5556/#/experiments/3/runs/00c0753809064b19a76e948fd684a532\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3\n",
      "\n",
      "🏃 View run bedecked-swan-648 at: http://127.0.0.1:5556/#/experiments/3/runs/9df57be8dd22444991ea30cb9da6e828\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                   \n",
      "\n",
      "🏃 View run nosy-slug-901 at: http://127.0.0.1:5556/#/experiments/3/runs/5f57b51cd83c4879bca468b02099f606\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                   \n",
      "\n",
      "🏃 View run blushing-fly-771 at: http://127.0.0.1:5556/#/experiments/3/runs/e8ac72552e574556ab34bfd679abe8c6\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                   \n",
      "\n",
      "🏃 View run marvelous-bat-558 at: http://127.0.0.1:5556/#/experiments/3/runs/c3db78304feb4a57b9eb5d2df91db42a\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                   \n",
      "\n",
      "🏃 View run suave-cat-969 at: http://127.0.0.1:5556/#/experiments/3/runs/749561e1ffb7411490d5ca9ed29ffb5a\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                    \n",
      "\n",
      "🏃 View run spiffy-rook-795 at: http://127.0.0.1:5556/#/experiments/3/runs/5a93b968e9674d5e9a8371af4a076c6e\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                    \n",
      "\n",
      "🏃 View run burly-penguin-830 at: http://127.0.0.1:5556/#/experiments/3/runs/9cda8b3da1214344b4292fe1c30dc993\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                    \n",
      "\n",
      "🏃 View run salty-cow-554 at: http://127.0.0.1:5556/#/experiments/3/runs/db4e182c11ee48b2a17be81ac6ee2b4c\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                    \n",
      "\n",
      "🏃 View run flawless-colt-960 at: http://127.0.0.1:5556/#/experiments/3/runs/e89e65e50bec4457879c3dc810269ede\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                    \n",
      "\n",
      "🏃 View run powerful-hound-704 at: http://127.0.0.1:5556/#/experiments/3/runs/69f73be48fec4842911c730ce49f3ec2\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                     \n",
      "\n",
      "🏃 View run upset-ant-43 at: http://127.0.0.1:5556/#/experiments/3/runs/1136613cb0df4016b33e7ea405d9a6a5\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                     \n",
      "\n",
      "🏃 View run charming-squid-359 at: http://127.0.0.1:5556/#/experiments/3/runs/d6399ace600e4eef8afca1161e1631c4\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                     \n",
      "\n",
      "🏃 View run beautiful-fowl-889 at: http://127.0.0.1:5556/#/experiments/3/runs/af9f0b3fb19442a18658a49a53aa6dfe\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                     \n",
      "\n",
      "🏃 View run bedecked-shark-91 at: http://127.0.0.1:5556/#/experiments/3/runs/4f4fb0bdbc8a440abf58e1d807d1650f\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3                     \n",
      "\n",
      "100%|██████████| 15/15 [02:20<00:00,  9.33s/trial, best loss: 0.6971480250358582]\n",
      "🏃 View run hyperparameter-sweep at: http://127.0.0.1:5556/#/experiments/3/runs/c3e8dd8d89c548f085580cbefa6de788\n",
      "🧪 View experiment at: http://127.0.0.1:5556/#/experiments/3\n"
     ]
    }
   ],
   "source": [
    "# Create or set experiment\n",
    "experiment_name = \"wine-quality-optimization\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"Starting hyperparameter optimization experiment: {experiment_name}\")\n",
    "print(\"This will run 15 trials to find optimal hyperparameters...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"hyperparameter-sweep\"):\n",
    "    # Log experiment metadata\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"optimization_method\": \"Tree-structured Parzen Estimator (TPE)\",\n",
    "            \"max_evaluations\": 15,\n",
    "            \"objective_metric\": \"validation_rmse\",\n",
    "            \"dataset\": \"wine-quality\",\n",
    "            \"model_type\": \"neural_network\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Run optimization\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=15,\n",
    "        trials=trials,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Find and log best results\n",
    "    best_trial = min(trials.results, key=lambda x: x[\"loss\"])\n",
    "    best_rmse = best_trial[\"loss\"]\n",
    "\n",
    "    # Log optimization results\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"best_learning_rate\": best_params[\"learning_rate\"],\n",
    "            \"best_momentum\": best_params[\"momentum\"],\n",
    "        }\n",
    "    )\n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            \"best_val_rmse\": best_rmse,\n",
    "            \"total_trials\": len(trials.trials),\n",
    "            \"optimization_completed\": 1,\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".noctprob-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
