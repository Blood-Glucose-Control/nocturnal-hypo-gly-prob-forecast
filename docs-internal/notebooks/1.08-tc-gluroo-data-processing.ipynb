{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process the raw data\n",
        "\n",
        "TODO: Need to figure out how to load the iterable dataset directly into the Trainer. It needs to know there are different patients in there too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from_path = '/data/shared/cache/data/gluroo_2026/groups_backups/groups.csv.gz'\n",
        "groups_df = pd.read_csv(from_path, compression='gzip')\n",
        "\n",
        "# Get a summary of which gid values appear more than once\n",
        "gid_counts = groups_df['gid'].value_counts()\n",
        "duplicate_gid_values = gid_counts[gid_counts > 1]\n",
        "\n",
        "print(f\"Number of gid values that appear more than once: {len(duplicate_gid_values)}\")\n",
        "print(duplicate_gid_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (102875, 22)\n",
            "Original unique gid count: 102844\n"
          ]
        }
      ],
      "source": [
        "# There is a bug with the data export script where if a group onboarded multiple times, we would have duplicated gid.\n",
        "# Deduplicate by keeping the row with the highest index for each gid\n",
        "print(f\"Original shape: {groups_df.shape}\")\n",
        "print(f\"Original unique gid count: {groups_df['gid'].nunique()}\")\n",
        "\n",
        "# Sort by index to ensure we keep the highest index, then drop duplicates keeping the last occurrence\n",
        "groups_df_dedup = groups_df.sort_index().drop_duplicates(subset=['gid'], keep='last')\n",
        "to_path = '/data/shared/cache/data/gluroo_2026/raw/groups.csv.gz'\n",
        "groups_df_dedup.to_csv(to_path, compression='gzip', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from_path = '/data/shared/cache/data/gluroo_2026/raw/groups.csv.gz'\n",
        "# Read only the header to get column names without loading the entire file\n",
        "groups_df = pd.read_csv(from_path, nrows=5)\n",
        "groups_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "updated_groups_path = '/data/shared/cache/data/gluroo_2026/raw/groups.csv.gz'\n",
        "updated_groups_df = pd.read_csv(updated_groups_path, compression='gzip')\n",
        "updated_groups_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data.diabetes_datasets.data_loader import get_loader\n",
        "\n",
        "# add dependency: \n",
        "# 1. pip install psycopg2-binary\n",
        "\n",
        "loader = get_loader(\n",
        "    data_source_name=\"gluroo_2026\",\n",
        "    keep_columns=None,\n",
        "    use_cached=False,\n",
        "\n",
        "    # Testing for now\n",
        "    patients_per_batch=2,\n",
        "    patients_per_file=6,\n",
        "    number_of_patients_to_process=4,\n",
        "    min_date_span_days=30,\n",
        "    load_all=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader.processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from src.data.models import ColumnNames\n",
        "\n",
        "# Combine all patients' data into a single DataFrame, add a column for patient_id if not present\n",
        "all_data = []\n",
        "for p_num, df in loader.processed_data.items():\n",
        "    # note that newly filled rows won't have p_num\n",
        "    df[ColumnNames.P_NUM.value] = p_num\n",
        "    all_data.append(df)\n",
        "if all_data:\n",
        "    df_all = pd.concat(all_data, ignore_index=True)\n",
        "    df_all.to_csv(\"gluroo_processed_data.csv\", index=False)\n",
        "else:\n",
        "    print(\"No processed data found in loader.processed_data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterable Dataset - WIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stream_ds = loader.get_hf_streaming_dataset(\n",
        "    columns=[\"datetime\", \"p_num\", \"bg_mM\", \"food_g\", \"dose_units\", \"cob\", \"iob\"],\n",
        "    # patient_ids=[\"gluroo_1\", \"gluroo_2\"],  # drop to None to check all data\n",
        "    batch_size=1024,\n",
        "    validate_non_empty=True,  # default; set False if you donâ€™t want the peek\n",
        ")\n",
        "# first_batch = next(iter(stream_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "from src.models.ttm.model import create_ttm_model\n",
        "from src.models.ttm.ttm import get_model\n",
        "\n",
        "# Build model and training args\n",
        "ttm = create_ttm_model(model_path=\"ibm-granite/granite-timeseries-ttm-r2\")\n",
        "model = ttm.model\n",
        "out_dir = \"./out\"\n",
        "batch_size = 1024\n",
        "finetune_forecast_args = TrainingArguments(\n",
        "    output_dir=out_dir,\n",
        "    overwrite_output_dir=False,\n",
        "    # learning_rate=learning_rate,\n",
        "    # num_train_epochs=num_epochs,\n",
        "    do_eval=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=1000,  # Evaluate every 1000 steps (less frequent = faster training)\n",
        "    fp16=False,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    dataloader_num_workers=1,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "    logging_first_step=True,  # Log the first step\n",
        "    save_steps=2000,  # Save checkpoints every 2000 steps\n",
        "    save_total_limit=100,\n",
        "    max_steps=5000,  # Required for streaming datasets (no __len__); LR scheduler needs known total steps\n",
        "    logging_dir=os.path.join(out_dir, \"logs\"),\n",
        "    load_best_model_at_end=True,  # Load the best model when training ends\n",
        "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
        "    greater_is_better=False,  # For loss\n",
        "    use_cpu=False,\n",
        "    # Additional logging control\n",
        "    log_level=\"info\",  # Control log verbosity\n",
        "    disable_tqdm=False,  # Keep progress bars\n",
        ")\n",
        "# fp16 is only supported on CUDA; disable on MPS/CPU to avoid \"fp16 requires a GPU (not 'mps')\"\n",
        "if not torch.cuda.is_available():\n",
        "    finetune_forecast_args.fp16 = False\n",
        "    finetune_forecast_args.bf16 = False\n",
        "    os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
        "\n",
        "# # Build train dataset from notebook's ts_df (from stream_ds). TTM expects datetime, bg_mM, p_num.\n",
        "# train_df = ts_df.reset_index().rename(\n",
        "#     columns={\"timestamp\": \"datetime\", \"target\": \"bg_mM\", \"item_id\": \"p_num\"}\n",
        "# )\n",
        "# train_loader, val_loader, _ = ttm._prepare_data(train_data=train_df)\n",
        "# train_dataset = train_loader.dataset if train_loader else None\n",
        "# eval_dataset = val_loader.dataset if val_loader else Non\n",
        "#\n",
        "#\n",
        "finetune_forecast_model = get_model(\n",
        "    \"ibm-granite/granite-timeseries-ttm-r2\",\n",
        "    context_length=512,\n",
        "    prediction_length=96,\n",
        "    freq_prefix_tuning=False,\n",
        "    prefer_l1_loss=False,\n",
        "    prefer_longer_context=True,\n",
        "    # Can also provide TTM Config args. A param?\n",
        "    loss=\"mse\",\n",
        "    quantile=0.5,\n",
        ")\n",
        "\n",
        "# Import the pickle-safe data collator from the gluroo module\n",
        "\n",
        "# Use Trainer directly\n",
        "# trainer = Trainer(\n",
        "#     model=finetune_forecast_model,\n",
        "#     args=finetune_forecast_args,\n",
        "#     train_dataset=stream_ds,\n",
        "#     eval_dataset=stream_ds,  # For testing\n",
        "#     data_collator=gluroo_data_collator,\n",
        "# )\n",
        "# trainer.train()\n",
        "# trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read groups.csv.gz using pandas\n",
        "groups_df = pd.read_csv('/data/shared/cache/data/gluroo_2026/raw/groups.csv.gz', compression='gzip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "noctprob",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
